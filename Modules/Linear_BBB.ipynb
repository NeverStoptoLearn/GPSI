{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BNN层，类似于BP网络的Linear层，与BP网络类似，一层BNN层由weight和bias组成，weight和bias都具有均值和方差\n",
    "class Linear_BBB(nn.Module):\n",
    "    def __init__(self, input_features, output_features, prior_var=1.):\n",
    "        \"\"\"\n",
    "            初始化 : 先验是一个以0为中心，方差为20的正态分布\n",
    "        \"\"\"\n",
    "        # 初始化神经网络各层\n",
    "        super().__init__()\n",
    "        # 设置输入输出维度\n",
    "        self.input_features = input_features\n",
    "        self.output_features = output_features\n",
    "\n",
    "        # initialize mu and rho parameters for the weights of the layer\n",
    "        self.w_mu = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "        self.w_rho = nn.Parameter(torch.zeros(output_features, input_features))\n",
    "\n",
    "        #initialize mu and rho parameters for the layer's bias\n",
    "        self.b_mu =  nn.Parameter(torch.zeros(output_features))\n",
    "        self.b_rho = nn.Parameter(torch.zeros(output_features))\n",
    "\n",
    "        #initialize weight samples (these will be calculated whenever the layer makes a prediction)\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "        # initialize prior distribution for all of the weights and biases\n",
    "        self.prior = torch.distributions.Normal(0,prior_var)\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "          Optimization process\n",
    "        \"\"\"\n",
    "        # sample weights\n",
    "        # 从标准正态分布中采样权重\n",
    "        w_epsilon = Normal(0,1).sample(self.w_mu.shape)\n",
    "        # 获得服从均值为mu，方差为delta的正态分布的样本\n",
    "        self.w = self.w_mu + torch.log(1+torch.exp(self.w_rho)) * w_epsilon\n",
    "\n",
    "        # sample bias\n",
    "        # 与sample weights同理\n",
    "        b_epsilon = Normal(0,1).sample(self.b_mu.shape)\n",
    "        self.b = self.b_mu + torch.log(1+torch.exp(self.b_rho)) * b_epsilon\n",
    "\n",
    "        # record log prior by evaluating log pdf of prior at sampled weight and bias\n",
    "        # 计算log p(w)，用于后续计算loss\n",
    "        w_log_prior = self.prior.log_prob(self.w)\n",
    "        b_log_prior = self.prior.log_prob(self.b)\n",
    "        self.log_prior = torch.sum(w_log_prior) + torch.sum(b_log_prior)\n",
    "\n",
    "        # record log variational posterior by evaluating log pdf of normal distribution defined by parameters with respect at the sampled values\n",
    "        # 计算 log p(w|\\theta)，用于后续计算loss\n",
    "        self.w_post = Normal(self.w_mu.data, torch.log(1+torch.exp(self.w_rho)))\n",
    "        self.b_post = Normal(self.b_mu.data, torch.log(1+torch.exp(self.b_rho)))\n",
    "        self.log_post = self.w_post.log_prob(self.w).sum() + self.b_post.log_prob(self.b).sum()\n",
    "\n",
    "        # 权重确定后，和BP网络层一样使用\n",
    "        return F.linear(input, self.w, self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_BBB(nn.Module):\n",
    "    def __init__(self, hidden_units, noise_tol=.1,  prior_var=1.):\n",
    "\n",
    "        # initialize the network like you would with a standard multilayer perceptron, but using the BBB layer\n",
    "        super().__init__()\n",
    "        # 输入为1，输出为1，只含有一层隐藏层的BNN\n",
    "        self.hidden = Linear_BBB(1,hidden_units, prior_var=prior_var)\n",
    "        self.out = Linear_BBB(hidden_units, 1, prior_var=prior_var)\n",
    "        self.noise_tol = noise_tol # we will use the noise tolerance to calculate our likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        # again, this is equivalent to a standard multilayer perceptron\n",
    "        # 激活函数选用sigmoid\n",
    "        x = torch.sigmoid(self.hidden(x))\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "\n",
    "    def log_prior(self):\n",
    "        # calculate the log prior over all the layers\n",
    "        return self.hidden.log_prior + self.out.log_prior\n",
    "\n",
    "    def log_post(self):\n",
    "        # calculate the log posterior over all the layers\n",
    "        return self.hidden.log_post + self.out.log_post\n",
    "\n",
    "    # 计算loss\n",
    "    def sample_elbo(self, input, target, samples):\n",
    "        # we calculate the negative elbo, which will be our loss function\n",
    "        #initialize tensors\n",
    "        outputs = torch.zeros(samples, target.shape[0])\n",
    "        log_priors = torch.zeros(samples)\n",
    "        log_posts = torch.zeros(samples)\n",
    "        log_likes = torch.zeros(samples)\n",
    "        # make predictions and calculate prior, posterior, and likelihood for a given number of samples\n",
    "\n",
    "        # 蒙特卡罗近似\n",
    "        for i in range(samples):\n",
    "            outputs[i] = self(input).reshape(-1) # make predictions\n",
    "            log_priors[i] = self.log_prior() # get log prior\n",
    "            log_posts[i] = self.log_post() # get log variational posterior\n",
    "            log_likes[i] = Normal(outputs[i], self.noise_tol).log_prob(target.reshape(-1)).sum() # calculate the log likelihood\n",
    "        # calculate monte carlo estimate of prior posterior and likelihood\n",
    "        log_prior = log_priors.mean()\n",
    "        log_post = log_posts.mean()\n",
    "        log_like = log_likes.mean()\n",
    "        # calculate the negative elbo (which is our loss function)\n",
    "        loss = log_post - log_prior - log_like\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toy_function(x):\n",
    "    return -x**4 + 3*x**2 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0000],\n",
      "        [-1.8000],\n",
      "        [-1.0000],\n",
      "        [ 1.0000],\n",
      "        [ 1.8000],\n",
      "        [ 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# toy dataset we can start with\n",
    "x = torch.tensor([-2, -1.8, -1, 1, 1.8, 2]).reshape(-1,1)\n",
    "print(x)\n",
    "y = toy_function(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1])\n",
      "(500, 100)\n"
     ]
    }
   ],
   "source": [
    "samples = 500\n",
    "x_tmp = torch.linspace(-2,2,100).reshape(-1,1)\n",
    "y_samp = np.zeros((samples,100))\n",
    "print(x_tmp.shape)\n",
    "print(y_samp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1/2000\n",
      "Loss: 2168.072265625\n",
      "epoch: 11/2000\n",
      "Loss: 2648.31494140625\n",
      "epoch: 21/2000\n",
      "Loss: 3481.820556640625\n",
      "epoch: 31/2000\n",
      "Loss: 3500.812744140625\n",
      "epoch: 41/2000\n",
      "Loss: 2767.3134765625\n",
      "epoch: 51/2000\n",
      "Loss: 2900.45654296875\n",
      "epoch: 61/2000\n",
      "Loss: 9668.7900390625\n",
      "epoch: 71/2000\n",
      "Loss: 2263.67724609375\n",
      "epoch: 81/2000\n",
      "Loss: 2259.599853515625\n",
      "epoch: 91/2000\n",
      "Loss: 2051.795166015625\n",
      "epoch: 101/2000\n",
      "Loss: 2169.40673828125\n",
      "epoch: 111/2000\n",
      "Loss: 1948.0478515625\n",
      "epoch: 121/2000\n",
      "Loss: 1738.3046875\n",
      "epoch: 131/2000\n",
      "Loss: 1786.688720703125\n",
      "epoch: 141/2000\n",
      "Loss: 1605.324951171875\n",
      "epoch: 151/2000\n",
      "Loss: 1666.7508544921875\n",
      "epoch: 161/2000\n",
      "Loss: 1609.324462890625\n",
      "epoch: 171/2000\n",
      "Loss: 1625.7210693359375\n",
      "epoch: 181/2000\n",
      "Loss: 1114.5604248046875\n",
      "epoch: 191/2000\n",
      "Loss: 1207.525390625\n",
      "epoch: 201/2000\n",
      "Loss: 1026.197509765625\n",
      "epoch: 211/2000\n",
      "Loss: 868.3079223632812\n",
      "epoch: 221/2000\n",
      "Loss: 884.6836547851562\n",
      "epoch: 231/2000\n",
      "Loss: 920.8575439453125\n",
      "epoch: 241/2000\n",
      "Loss: 734.8651123046875\n",
      "epoch: 251/2000\n",
      "Loss: 1097.7230224609375\n",
      "epoch: 261/2000\n",
      "Loss: 900.6905517578125\n",
      "epoch: 271/2000\n",
      "Loss: 632.6608276367188\n",
      "epoch: 281/2000\n",
      "Loss: 614.8218383789062\n",
      "epoch: 291/2000\n",
      "Loss: 562.81787109375\n",
      "epoch: 301/2000\n",
      "Loss: 609.8336181640625\n",
      "epoch: 311/2000\n",
      "Loss: 589.101806640625\n",
      "epoch: 321/2000\n",
      "Loss: 569.3611450195312\n",
      "epoch: 331/2000\n",
      "Loss: 533.06396484375\n",
      "epoch: 341/2000\n",
      "Loss: 490.3621826171875\n",
      "epoch: 351/2000\n",
      "Loss: 490.98138427734375\n",
      "epoch: 361/2000\n",
      "Loss: 498.72686767578125\n",
      "epoch: 371/2000\n",
      "Loss: 590.2171630859375\n",
      "epoch: 381/2000\n",
      "Loss: 579.6346435546875\n",
      "epoch: 391/2000\n",
      "Loss: 507.9752197265625\n",
      "epoch: 401/2000\n",
      "Loss: 507.5284423828125\n",
      "epoch: 411/2000\n",
      "Loss: 504.5888671875\n",
      "epoch: 421/2000\n",
      "Loss: 427.8103942871094\n",
      "epoch: 431/2000\n",
      "Loss: 466.2176818847656\n",
      "epoch: 441/2000\n",
      "Loss: 489.5626220703125\n",
      "epoch: 451/2000\n",
      "Loss: 442.45758056640625\n",
      "epoch: 461/2000\n",
      "Loss: 460.53662109375\n",
      "epoch: 471/2000\n",
      "Loss: 448.0845947265625\n",
      "epoch: 481/2000\n",
      "Loss: 481.9274597167969\n",
      "epoch: 491/2000\n",
      "Loss: 424.3948974609375\n",
      "epoch: 501/2000\n",
      "Loss: 461.69293212890625\n",
      "epoch: 511/2000\n",
      "Loss: 405.7461853027344\n",
      "epoch: 521/2000\n",
      "Loss: 541.658203125\n",
      "epoch: 531/2000\n",
      "Loss: 459.244873046875\n",
      "epoch: 541/2000\n",
      "Loss: 400.5156555175781\n",
      "epoch: 551/2000\n",
      "Loss: 443.2228088378906\n",
      "epoch: 561/2000\n",
      "Loss: 384.4255065917969\n",
      "epoch: 571/2000\n",
      "Loss: 472.23565673828125\n",
      "epoch: 581/2000\n",
      "Loss: 426.75\n",
      "epoch: 591/2000\n",
      "Loss: 438.35302734375\n",
      "epoch: 601/2000\n",
      "Loss: 395.44622802734375\n",
      "epoch: 611/2000\n",
      "Loss: 444.02044677734375\n",
      "epoch: 621/2000\n",
      "Loss: 425.2484436035156\n",
      "epoch: 631/2000\n",
      "Loss: 375.65704345703125\n",
      "epoch: 641/2000\n",
      "Loss: 405.25665283203125\n",
      "epoch: 651/2000\n",
      "Loss: 439.50140380859375\n",
      "epoch: 661/2000\n",
      "Loss: 352.7305603027344\n",
      "epoch: 671/2000\n",
      "Loss: 352.2041015625\n",
      "epoch: 681/2000\n",
      "Loss: 373.61981201171875\n",
      "epoch: 691/2000\n",
      "Loss: 350.18499755859375\n",
      "epoch: 701/2000\n",
      "Loss: 378.19476318359375\n",
      "epoch: 711/2000\n",
      "Loss: 419.96307373046875\n",
      "epoch: 721/2000\n",
      "Loss: 378.4838562011719\n",
      "epoch: 731/2000\n",
      "Loss: 355.1728515625\n",
      "epoch: 741/2000\n",
      "Loss: 382.3729248046875\n",
      "epoch: 751/2000\n",
      "Loss: 385.3912658691406\n",
      "epoch: 761/2000\n",
      "Loss: 365.30908203125\n",
      "epoch: 771/2000\n",
      "Loss: 365.4666442871094\n",
      "epoch: 781/2000\n",
      "Loss: 358.0937194824219\n",
      "epoch: 791/2000\n",
      "Loss: 323.4865417480469\n",
      "epoch: 801/2000\n",
      "Loss: 377.06036376953125\n",
      "epoch: 811/2000\n",
      "Loss: 331.9712829589844\n",
      "epoch: 821/2000\n",
      "Loss: 391.1688537597656\n",
      "epoch: 831/2000\n",
      "Loss: 318.56634521484375\n",
      "epoch: 841/2000\n",
      "Loss: 425.384765625\n",
      "epoch: 851/2000\n",
      "Loss: 315.2923889160156\n",
      "epoch: 861/2000\n",
      "Loss: 370.6808166503906\n",
      "epoch: 871/2000\n",
      "Loss: 319.52581787109375\n",
      "epoch: 881/2000\n",
      "Loss: 349.7903137207031\n",
      "epoch: 891/2000\n",
      "Loss: 315.0722961425781\n",
      "epoch: 901/2000\n",
      "Loss: 337.6758117675781\n",
      "epoch: 911/2000\n",
      "Loss: 379.1668701171875\n",
      "epoch: 921/2000\n",
      "Loss: 316.1570739746094\n",
      "epoch: 931/2000\n",
      "Loss: 317.63372802734375\n",
      "epoch: 941/2000\n",
      "Loss: 303.784423828125\n",
      "epoch: 951/2000\n",
      "Loss: 550.7343139648438\n",
      "epoch: 961/2000\n",
      "Loss: 354.2315368652344\n",
      "epoch: 971/2000\n",
      "Loss: 381.0716857910156\n",
      "epoch: 981/2000\n",
      "Loss: 345.70166015625\n",
      "epoch: 991/2000\n",
      "Loss: 340.7320556640625\n",
      "epoch: 1001/2000\n",
      "Loss: 365.8602600097656\n",
      "epoch: 1011/2000\n",
      "Loss: 381.964599609375\n",
      "epoch: 1021/2000\n",
      "Loss: 307.3406982421875\n",
      "epoch: 1031/2000\n",
      "Loss: 329.4811706542969\n",
      "epoch: 1041/2000\n",
      "Loss: 425.1270751953125\n",
      "epoch: 1051/2000\n",
      "Loss: 304.8632507324219\n",
      "epoch: 1061/2000\n",
      "Loss: 299.88299560546875\n",
      "epoch: 1071/2000\n",
      "Loss: 348.79339599609375\n",
      "epoch: 1081/2000\n",
      "Loss: 295.0287170410156\n",
      "epoch: 1091/2000\n",
      "Loss: 296.82183837890625\n",
      "epoch: 1101/2000\n",
      "Loss: 302.06707763671875\n",
      "epoch: 1111/2000\n",
      "Loss: 314.62152099609375\n",
      "epoch: 1121/2000\n",
      "Loss: 320.5986328125\n",
      "epoch: 1131/2000\n",
      "Loss: 278.98883056640625\n",
      "epoch: 1141/2000\n",
      "Loss: 498.2762145996094\n",
      "epoch: 1151/2000\n",
      "Loss: 343.86016845703125\n",
      "epoch: 1161/2000\n",
      "Loss: 279.2850646972656\n",
      "epoch: 1171/2000\n",
      "Loss: 307.1152648925781\n",
      "epoch: 1181/2000\n",
      "Loss: 366.99774169921875\n",
      "epoch: 1191/2000\n",
      "Loss: 305.7867126464844\n",
      "epoch: 1201/2000\n",
      "Loss: 281.9560241699219\n",
      "epoch: 1211/2000\n",
      "Loss: 337.9833068847656\n",
      "epoch: 1221/2000\n",
      "Loss: 274.5859680175781\n",
      "epoch: 1231/2000\n",
      "Loss: 513.870361328125\n",
      "epoch: 1241/2000\n",
      "Loss: 365.6127624511719\n",
      "epoch: 1251/2000\n",
      "Loss: 287.4327697753906\n",
      "epoch: 1261/2000\n",
      "Loss: 300.0927734375\n",
      "epoch: 1271/2000\n",
      "Loss: 375.8740539550781\n",
      "epoch: 1281/2000\n",
      "Loss: 286.20068359375\n",
      "epoch: 1291/2000\n",
      "Loss: 349.1085205078125\n",
      "epoch: 1301/2000\n",
      "Loss: 282.04681396484375\n",
      "epoch: 1311/2000\n",
      "Loss: 292.02093505859375\n",
      "epoch: 1321/2000\n",
      "Loss: 278.8290100097656\n",
      "epoch: 1331/2000\n",
      "Loss: 310.37725830078125\n",
      "epoch: 1341/2000\n",
      "Loss: 295.82623291015625\n",
      "epoch: 1351/2000\n",
      "Loss: 269.7403869628906\n",
      "epoch: 1361/2000\n",
      "Loss: 307.3327331542969\n",
      "epoch: 1371/2000\n",
      "Loss: 283.13824462890625\n",
      "epoch: 1381/2000\n",
      "Loss: 296.8330993652344\n",
      "epoch: 1391/2000\n",
      "Loss: 278.1852111816406\n",
      "epoch: 1401/2000\n",
      "Loss: 276.6961975097656\n",
      "epoch: 1411/2000\n",
      "Loss: 350.2853698730469\n",
      "epoch: 1421/2000\n",
      "Loss: 260.7762145996094\n",
      "epoch: 1431/2000\n",
      "Loss: 274.6707763671875\n",
      "epoch: 1441/2000\n",
      "Loss: 367.20135498046875\n",
      "epoch: 1451/2000\n",
      "Loss: 256.138427734375\n",
      "epoch: 1461/2000\n",
      "Loss: 329.24945068359375\n",
      "epoch: 1471/2000\n",
      "Loss: 306.4365539550781\n",
      "epoch: 1481/2000\n",
      "Loss: 325.82403564453125\n",
      "epoch: 1491/2000\n",
      "Loss: 266.5502014160156\n",
      "epoch: 1501/2000\n",
      "Loss: 315.1822204589844\n",
      "epoch: 1511/2000\n",
      "Loss: 307.84942626953125\n",
      "epoch: 1521/2000\n",
      "Loss: 282.2591247558594\n",
      "epoch: 1531/2000\n",
      "Loss: 296.4093017578125\n",
      "epoch: 1541/2000\n",
      "Loss: 298.3422546386719\n",
      "epoch: 1551/2000\n",
      "Loss: 449.079833984375\n",
      "epoch: 1561/2000\n",
      "Loss: 254.25198364257812\n",
      "epoch: 1571/2000\n",
      "Loss: 353.92706298828125\n",
      "epoch: 1581/2000\n",
      "Loss: 328.42669677734375\n",
      "epoch: 1591/2000\n",
      "Loss: 271.4715270996094\n",
      "epoch: 1601/2000\n",
      "Loss: 255.42498779296875\n",
      "epoch: 1611/2000\n",
      "Loss: 258.93768310546875\n",
      "epoch: 1621/2000\n",
      "Loss: 254.74221801757812\n",
      "epoch: 1631/2000\n",
      "Loss: 318.1536865234375\n",
      "epoch: 1641/2000\n",
      "Loss: 249.4576416015625\n",
      "epoch: 1651/2000\n",
      "Loss: 251.86045837402344\n",
      "epoch: 1661/2000\n",
      "Loss: 258.4066162109375\n",
      "epoch: 1671/2000\n",
      "Loss: 290.2068176269531\n",
      "epoch: 1681/2000\n",
      "Loss: 258.1444091796875\n",
      "epoch: 1691/2000\n",
      "Loss: 246.84886169433594\n",
      "epoch: 1701/2000\n",
      "Loss: 290.45294189453125\n",
      "epoch: 1711/2000\n",
      "Loss: 294.9754943847656\n",
      "epoch: 1721/2000\n",
      "Loss: 265.0420227050781\n",
      "epoch: 1731/2000\n",
      "Loss: 277.2896423339844\n",
      "epoch: 1741/2000\n",
      "Loss: 275.4974060058594\n",
      "epoch: 1751/2000\n",
      "Loss: 250.12294006347656\n",
      "epoch: 1761/2000\n",
      "Loss: 242.5916748046875\n",
      "epoch: 1771/2000\n",
      "Loss: 268.82489013671875\n",
      "epoch: 1781/2000\n",
      "Loss: 277.8280334472656\n",
      "epoch: 1791/2000\n",
      "Loss: 260.2958679199219\n",
      "epoch: 1801/2000\n",
      "Loss: 258.239013671875\n",
      "epoch: 1811/2000\n",
      "Loss: 255.70932006835938\n",
      "epoch: 1821/2000\n",
      "Loss: 247.2823486328125\n",
      "epoch: 1831/2000\n",
      "Loss: 270.70416259765625\n",
      "epoch: 1841/2000\n",
      "Loss: 240.0027618408203\n",
      "epoch: 1851/2000\n",
      "Loss: 247.98110961914062\n",
      "epoch: 1861/2000\n",
      "Loss: 262.774658203125\n",
      "epoch: 1871/2000\n",
      "Loss: 268.50653076171875\n",
      "epoch: 1881/2000\n",
      "Loss: 258.45318603515625\n",
      "epoch: 1891/2000\n",
      "Loss: 266.9819030761719\n",
      "epoch: 1901/2000\n",
      "Loss: 265.7019348144531\n",
      "epoch: 1911/2000\n",
      "Loss: 248.7916259765625\n",
      "epoch: 1921/2000\n",
      "Loss: 241.9488525390625\n",
      "epoch: 1931/2000\n",
      "Loss: 258.34564208984375\n",
      "epoch: 1941/2000\n",
      "Loss: 238.10348510742188\n",
      "epoch: 1951/2000\n",
      "Loss: 244.14816284179688\n",
      "epoch: 1961/2000\n",
      "Loss: 240.56248474121094\n",
      "epoch: 1971/2000\n",
      "Loss: 415.49932861328125\n",
      "epoch: 1981/2000\n",
      "Loss: 247.6923828125\n",
      "epoch: 1991/2000\n",
      "Loss: 240.99755859375\n",
      "Finished Training\n",
      "test result: [-10.1616605  -10.15571793 -10.14988706 -10.14418722 -10.13863885\n",
      " -10.13326431 -10.12808733 -10.12313151 -10.11841809 -10.11395881\n",
      " -10.10975054 -10.10576183 -10.10191789 -10.09807506 -10.09398169\n",
      " -10.08920808 -10.08303472 -10.07425619 -10.06085354 -10.03944637\n",
      " -10.00438563  -9.94628049  -9.84968563  -9.68971032  -9.42766717\n",
      "  -9.00718318  -8.35518752  -7.39674757  -6.09318789  -4.49567645\n",
      "  -2.76984805  -1.14173618   0.20888043   1.21444901   1.90424741\n",
      "   2.3512351    2.63036035   2.80077334   2.90350661   2.96507643\n",
      "   3.00195163   3.02414891   3.03769957   3.04620217   3.05174976\n",
      "   3.05549038   3.05801007   3.05958041   3.06026771   3.05999755\n",
      "   3.05863417   3.05612375   3.05252523   3.047884     3.04218961\n",
      "   3.03539068   3.02730318   3.01743716   3.00495703   2.98839445\n",
      "   2.96472794   2.9276368    2.86447886   2.75091904   2.54194952\n",
      "   2.16030374   1.49285038   0.4267064   -1.04053584  -2.68933635\n",
      "  -4.16973238  -5.25938945  -5.95846952  -6.37952175  -6.63638116\n",
      "  -6.80706789  -6.93695469  -7.05026901  -7.15916776  -7.26926449\n",
      "  -7.38271224  -7.49987524  -7.6202519   -7.74297981  -7.86710654\n",
      "  -7.99172002  -8.11599371  -8.23919429  -8.36066979  -8.47984459\n",
      "  -8.59621078  -8.70933013  -8.8188305   -8.92441556  -9.02585488\n",
      "  -9.12299206  -9.2157342   -9.3040516   -9.38796526  -9.46754189]\n"
     ]
    }
   ],
   "source": [
    "net = MLP_BBB(32, prior_var=10)\n",
    "optimizer = optim.Adam(net.parameters(), lr=.1)\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "    optimizer.zero_grad()\n",
    "    # forward + backward + optimize\n",
    "    loss = net.sample_elbo(x, y, 1)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch: {}/{}'.format(epoch+1,epochs))\n",
    "        print('Loss:', loss.item())\n",
    "print('Finished Training')\n",
    "\n",
    "samples = 100\n",
    "x_tmp = torch.linspace(-5,5,100).reshape(-1,1)\n",
    "y_samp = np.zeros((samples,100))\n",
    "for s in range(samples):\n",
    "    y_tmp = net(x_tmp).detach().numpy()\n",
    "    y_samp[s] = y_tmp.reshape(-1)\n",
    "\n",
    "print(\"test result:\",np.mean(y_samp, axis = 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch171",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
