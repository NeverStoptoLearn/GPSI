{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gc\n",
    "\n",
    "import Modules.UNet as un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomSeedInitial(seed=256, cudnnDeterministic=True, cudnnBenchmark=False):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    # 为了保证可复现性, defaul True False; False True 可能可以提升gpu运行效率\n",
    "    torch.backends.cudnn.deterministic = cudnnDeterministic\n",
    "    torch.backends.cudnn.benchmark = cudnnBenchmark\n",
    "    \n",
    "begin_time = time.time()\n",
    "randomSeedInitial(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 单污染源\n",
    "# 13x8\n",
    "# train = np.load('./13x8/all/allconc.npy', allow_pickle=True)\n",
    "# 15x25\n",
    "# train = np.load('./15x25/all/allconc.npy', allow_pickle=True)\n",
    "# 14x18\n",
    "# train = np.load('./high/all/allconc.npy', allow_pickle=True)\n",
    "# 多污染源\n",
    "# 13x8\n",
    "# train = np.load('./13x8d/all/allconc.npy', allow_pickle=True)\n",
    "# 15x25\n",
    "# train = np.load('./new15x25d/all/allconc.npy', allow_pickle=True)\n",
    "train = np.load('./15x25d/all/allconc.npy', allow_pickle=True)\n",
    "\n",
    "# 14x18\n",
    "# train = np.load('./14x18d/all/allconc.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x: np.numarray, y: np.numarray):\n",
    "        \"\"\"\n",
    "        :param x: shape like (n_sample, t, n_well)\n",
    "        :param y: shape like (n_sample, t, h, w)\n",
    "        :param z: shape like (n, h, w)\n",
    "        \"\"\"\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.x = torch.from_numpy(x).to(torch.float32)\n",
    "        self.y = torch.from_numpy(y).to(torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item], self.y[item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "train_input = []\n",
    "\n",
    "for k in range(len(train)):\n",
    "    label.append(train[k][1])\n",
    "    train_input.append(train[k][-1])\n",
    "# label = (np.array(label)).reshape(100, 1, 18, 14)*100000\n",
    "# train = (np.array(train_input)).reshape(100, 1, 18, 14)*100000\n",
    "# label = (np.array(label)).reshape(100, 1, 18, 14)*1\n",
    "# train = (np.array(train_input)).reshape(100,1,18,14)*1\n",
    "# label = (np.array(label)).reshape(len(label), 1, 8, 13)*1000\n",
    "# train = (np.array(train_input)).reshape(len(label),1,8,13)*1000\n",
    "label = (np.array(label)).reshape(len(label), 1, 15, 25)*1000\n",
    "train = (np.array(train_input)).reshape(len(train),1,15,25)*1000\n",
    "# print(label.shape)\n",
    "# print(train.shape)\n",
    "\n",
    "# label = (np.array(label)).reshape(len(label), 15, 25)*100\n",
    "# train = (np.array(train_input)).reshape(len(train),15,25)*100\n",
    "\n",
    "mid = np.ones((1, 1, 4, 4))\n",
    "# mid.dtype = 'float32'\n",
    "label = np.kron(label, mid)\n",
    "train = np.kron(train, mid)\n",
    "\n",
    "# print(label.shape)\n",
    "# print(train.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, label, test_size=0.2, random_state=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.125, random_state=256)\n",
    "train_data = MyDataset(X_train, y_train)\n",
    "val_data = MyDataset(X_valid, y_valid)\n",
    "test_data = MyDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=7, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(dataset=val_data, batch_size=5, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=10, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Conv2(img, kernel, n, stride,p):\n",
    "    #img：输入图片；kernel：卷积核值；n：卷积核大小为n*n；stride:步长。\n",
    "    #return：feature map\n",
    "    h, w = img.shape\n",
    "    res_h = ((h+2*p-n)//stride)+1 #卷积边长计算公式：((n+2*p-k)/stride)+1\n",
    "    res_w = ((w+2*p -n)//stride)+1\n",
    "    res = np.zeros([res_h, res_w])\n",
    "    for i in range(res_h):\n",
    "        for j in range(res_w):\n",
    "            temp = img[i*stride:i*stride+n , j*stride:j*stride+n]\n",
    "            temp = np.multiply(kernel, temp)\n",
    "            res[i][j] = temp.sum()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = un.UNet()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-3)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, [10, 20], 0.1)\n",
    "loss_func = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 79396.142920, Acc: 672406.196\n",
      "Val Loss: 29617.350586, Acc: 480585.850\n",
      "480585.85\n",
      "480585.85 480585.85\n",
      "Epoch: 1 Train Loss: 13147.189819, Acc: 283979.059\n",
      "Val Loss: 1346.896362, Acc: 99877.825\n",
      "99877.825\n",
      "99877.825 99877.825\n",
      "Epoch: 2 Train Loss: 3455.118051, Acc: 129414.839\n",
      "Val Loss: 900.065430, Acc: 78370.100\n",
      "78370.1\n",
      "78370.1 78370.1\n",
      "Epoch: 3 Train Loss: 1630.656046, Acc: 90222.666\n",
      "Val Loss: 1172.080383, Acc: 82557.163\n",
      "82557.1625\n",
      "Epoch: 4 Train Loss: 766.020425, Acc: 60509.657\n",
      "Val Loss: 693.102142, Acc: 70039.531\n",
      "70039.53125\n",
      "70039.53125 70039.53125\n",
      "Epoch: 5 Train Loss: 458.914658, Acc: 45089.199\n",
      "Val Loss: 337.501343, Acc: 37551.622\n",
      "37551.621875\n",
      "37551.621875 37551.621875\n",
      "Epoch: 6 Train Loss: 310.206741, Acc: 36949.503\n",
      "Val Loss: 246.261757, Acc: 27302.394\n",
      "27302.39375\n",
      "27302.39375 27302.39375\n",
      "Epoch: 7 Train Loss: 272.432970, Acc: 31130.674\n",
      "Val Loss: 262.680069, Acc: 35089.772\n",
      "35089.771875\n",
      "Epoch: 8 Train Loss: 238.896596, Acc: 28995.724\n",
      "Val Loss: 223.011078, Acc: 25996.328\n",
      "25996.328125\n",
      "25996.328125 25996.328125\n",
      "Epoch: 9 Train Loss: 213.538979, Acc: 26128.795\n",
      "Val Loss: 207.787910, Acc: 24602.905\n",
      "24602.9046875\n",
      "24602.9046875 24602.9046875\n",
      "Epoch: 10 Train Loss: 198.880646, Acc: 24117.982\n",
      "Val Loss: 188.297447, Acc: 23658.885\n",
      "23658.88515625\n",
      "23658.88515625 23658.88515625\n",
      "Epoch: 11 Train Loss: 184.449046, Acc: 22983.634\n",
      "Val Loss: 170.645493, Acc: 22656.835\n",
      "22656.83515625\n",
      "22656.83515625 22656.83515625\n",
      "Epoch: 12 Train Loss: 165.603589, Acc: 22202.098\n",
      "Val Loss: 150.291435, Acc: 22098.565\n",
      "22098.56484375\n",
      "22098.56484375 22098.56484375\n",
      "Epoch: 13 Train Loss: 149.022949, Acc: 21454.283\n",
      "Val Loss: 131.235283, Acc: 21062.491\n",
      "21062.49140625\n",
      "21062.49140625 21062.49140625\n",
      "Epoch: 14 Train Loss: 139.926482, Acc: 21061.245\n",
      "Val Loss: 120.710152, Acc: 20957.320\n",
      "20957.3203125\n",
      "20957.3203125 20957.3203125\n",
      "Epoch: 15 Train Loss: 124.765010, Acc: 20357.737\n",
      "Val Loss: 104.836994, Acc: 18656.641\n",
      "18656.640625\n",
      "18656.640625 18656.640625\n",
      "Epoch: 16 Train Loss: 111.789258, Acc: 18783.481\n",
      "Val Loss: 96.939800, Acc: 18170.266\n",
      "18170.26640625\n",
      "18170.26640625 18170.26640625\n",
      "Epoch: 17 Train Loss: 100.269332, Acc: 17504.354\n",
      "Val Loss: 89.595680, Acc: 17331.610\n",
      "17331.61015625\n",
      "17331.61015625 17331.61015625\n",
      "Epoch: 18 Train Loss: 93.856953, Acc: 16958.319\n",
      "Val Loss: 86.182751, Acc: 16112.686\n",
      "16112.6859375\n",
      "16112.6859375 16112.6859375\n",
      "Epoch: 19 Train Loss: 90.833503, Acc: 16532.711\n",
      "Val Loss: 84.658554, Acc: 15976.685\n",
      "15976.68515625\n",
      "15976.68515625 15976.68515625\n",
      "Epoch: 20 Train Loss: 86.677668, Acc: 15870.965\n",
      "Val Loss: 76.263256, Acc: 15424.902\n",
      "15424.9015625\n",
      "15424.9015625 15424.9015625\n",
      "Epoch: 21 Train Loss: 82.813893, Acc: 15375.641\n",
      "Val Loss: 78.998093, Acc: 15348.098\n",
      "15348.0984375\n",
      "15348.0984375 15348.0984375\n",
      "Epoch: 22 Train Loss: 80.695665, Acc: 15078.918\n",
      "Val Loss: 72.645061, Acc: 14089.355\n",
      "14089.3546875\n",
      "14089.3546875 14089.3546875\n",
      "Epoch: 23 Train Loss: 76.740514, Acc: 15256.455\n",
      "Val Loss: 77.582737, Acc: 14440.891\n",
      "14440.89140625\n",
      "Epoch: 24 Train Loss: 75.763651, Acc: 14904.318\n",
      "Val Loss: 66.094442, Acc: 14969.491\n",
      "14969.490625\n",
      "Epoch: 25 Train Loss: 69.235054, Acc: 13829.156\n",
      "Val Loss: 69.844286, Acc: 13747.497\n",
      "13747.496875\n",
      "13747.496875 13747.496875\n",
      "Epoch: 26 Train Loss: 65.867901, Acc: 13194.375\n",
      "Val Loss: 65.066507, Acc: 12891.345\n",
      "12891.3453125\n",
      "12891.3453125 12891.3453125\n",
      "Epoch: 27 Train Loss: 61.095255, Acc: 12446.448\n",
      "Val Loss: 55.329935, Acc: 12627.237\n",
      "12627.2375\n",
      "12627.2375 12627.2375\n",
      "Epoch: 28 Train Loss: 60.005724, Acc: 12176.644\n",
      "Val Loss: 53.249838, Acc: 11668.963\n",
      "11668.96328125\n",
      "11668.96328125 11668.96328125\n",
      "Epoch: 29 Train Loss: 60.496649, Acc: 12202.815\n",
      "Val Loss: 62.821316, Acc: 13015.321\n",
      "13015.321484375\n",
      "Epoch: 30 Train Loss: 58.057939, Acc: 12425.749\n",
      "Val Loss: 49.885824, Acc: 11704.429\n",
      "11704.429296875\n",
      "Epoch: 31 Train Loss: 54.907057, Acc: 11269.364\n",
      "Val Loss: 49.247051, Acc: 11500.196\n",
      "11500.196484375\n",
      "11500.196484375 11500.196484375\n",
      "Epoch: 32 Train Loss: 52.545080, Acc: 11363.025\n",
      "Val Loss: 49.718126, Acc: 10728.626\n",
      "10728.626171875\n",
      "10728.626171875 10728.626171875\n",
      "Epoch: 33 Train Loss: 50.213825, Acc: 10878.606\n",
      "Val Loss: 49.064947, Acc: 11742.648\n",
      "11742.6484375\n",
      "Epoch: 34 Train Loss: 49.851443, Acc: 10749.618\n",
      "Val Loss: 47.710163, Acc: 10308.089\n",
      "10308.088671875\n",
      "10308.088671875 10308.088671875\n",
      "Epoch: 35 Train Loss: 47.351928, Acc: 10180.445\n",
      "Val Loss: 49.617670, Acc: 10615.773\n",
      "10615.77265625\n",
      "Epoch: 36 Train Loss: 45.791770, Acc: 10336.342\n",
      "Val Loss: 49.787460, Acc: 10556.145\n",
      "10556.1453125\n",
      "Epoch: 37 Train Loss: 45.277411, Acc: 10215.075\n",
      "Val Loss: 45.145908, Acc: 10332.348\n",
      "10332.348046875\n",
      "Epoch: 38 Train Loss: 43.826074, Acc: 9714.591\n",
      "Val Loss: 41.736847, Acc: 9996.775\n",
      "9996.775390625\n",
      "9996.775390625 9996.775390625\n",
      "Epoch: 39 Train Loss: 48.033450, Acc: 10509.344\n",
      "Val Loss: 41.847193, Acc: 10070.336\n",
      "10070.336328125\n",
      "Epoch: 40 Train Loss: 41.971575, Acc: 9874.845\n",
      "Val Loss: 41.091208, Acc: 9601.656\n",
      "9601.655859375\n",
      "9601.655859375 9601.655859375\n",
      "Epoch: 41 Train Loss: 40.894423, Acc: 9484.969\n",
      "Val Loss: 44.125315, Acc: 9724.656\n",
      "9724.655859375\n",
      "Epoch: 42 Train Loss: 40.487191, Acc: 9258.092\n",
      "Val Loss: 39.313612, Acc: 9535.212\n",
      "9535.2125\n",
      "9535.2125 9535.2125\n",
      "Epoch: 43 Train Loss: 39.722691, Acc: 9312.222\n",
      "Val Loss: 46.629051, Acc: 9841.143\n",
      "9841.14296875\n",
      "Epoch: 44 Train Loss: 38.852446, Acc: 9230.392\n",
      "Val Loss: 38.331820, Acc: 9395.575\n",
      "9395.575\n",
      "9395.575 9395.575\n",
      "Epoch: 45 Train Loss: 37.716859, Acc: 9010.008\n",
      "Val Loss: 39.177507, Acc: 9093.968\n",
      "9093.968359375\n",
      "9093.968359375 9093.968359375\n",
      "Epoch: 46 Train Loss: 36.580637, Acc: 8805.669\n",
      "Val Loss: 43.491943, Acc: 9052.664\n",
      "9052.663671875\n",
      "9052.663671875 9052.663671875\n",
      "Epoch: 47 Train Loss: 35.540807, Acc: 8738.993\n",
      "Val Loss: 37.775900, Acc: 9409.101\n",
      "9409.101171875\n",
      "Epoch: 48 Train Loss: 39.161528, Acc: 9107.427\n",
      "Val Loss: 60.328072, Acc: 11210.851\n",
      "11210.851171875\n",
      "Epoch: 49 Train Loss: 38.762761, Acc: 9465.922\n",
      "Val Loss: 40.016123, Acc: 8756.401\n",
      "8756.40078125\n",
      "8756.40078125 8756.40078125\n",
      "Epoch: 50 Train Loss: 34.545593, Acc: 8560.722\n",
      "Val Loss: 37.135124, Acc: 8677.774\n",
      "8677.773828125\n",
      "8677.773828125 8677.773828125\n",
      "Epoch: 51 Train Loss: 33.449663, Acc: 8345.560\n",
      "Val Loss: 44.223091, Acc: 9322.204\n",
      "9322.20390625\n",
      "Epoch: 52 Train Loss: 34.341451, Acc: 8415.499\n",
      "Val Loss: 36.217922, Acc: 8572.111\n",
      "8572.111328125\n",
      "8572.111328125 8572.111328125\n",
      "Epoch: 53 Train Loss: 32.690226, Acc: 8314.222\n",
      "Val Loss: 46.520193, Acc: 9034.374\n",
      "9034.37421875\n",
      "Epoch: 54 Train Loss: 31.775238, Acc: 8314.286\n",
      "Val Loss: 37.702751, Acc: 8601.706\n",
      "8601.70625\n",
      "Epoch: 55 Train Loss: 30.927938, Acc: 8012.027\n",
      "Val Loss: 40.652102, Acc: 8712.070\n",
      "8712.06953125\n",
      "Epoch: 56 Train Loss: 31.099082, Acc: 8003.411\n",
      "Val Loss: 36.110172, Acc: 8421.980\n",
      "8421.980078125\n",
      "8421.980078125 8421.980078125\n",
      "Epoch: 57 Train Loss: 30.469878, Acc: 7941.675\n",
      "Val Loss: 40.628057, Acc: 8779.924\n",
      "8779.92421875\n",
      "Epoch: 58 Train Loss: 29.695733, Acc: 7855.934\n",
      "Val Loss: 35.878447, Acc: 8264.877\n",
      "8264.8765625\n",
      "8264.8765625 8264.8765625\n",
      "Epoch: 59 Train Loss: 29.484714, Acc: 7832.657\n",
      "Val Loss: 34.284239, Acc: 8208.330\n",
      "8208.33046875\n",
      "8208.33046875 8208.33046875\n",
      "Epoch: 60 Train Loss: 31.876379, Acc: 8711.234\n",
      "Val Loss: 40.312750, Acc: 8811.760\n",
      "8811.759765625\n",
      "Epoch: 61 Train Loss: 31.253948, Acc: 8535.217\n",
      "Val Loss: 36.226522, Acc: 8206.325\n",
      "8206.325\n",
      "8206.325 8206.325\n",
      "Epoch: 62 Train Loss: 30.490188, Acc: 7834.864\n",
      "Val Loss: 41.944386, Acc: 8189.450\n",
      "8189.45\n",
      "8189.45 8189.45\n",
      "Epoch: 63 Train Loss: 29.045064, Acc: 7781.963\n",
      "Val Loss: 41.834164, Acc: 8511.053\n",
      "8511.052734375\n",
      "Epoch: 64 Train Loss: 29.391724, Acc: 7798.250\n",
      "Val Loss: 38.281286, Acc: 8917.518\n",
      "8917.51796875\n",
      "Epoch: 65 Train Loss: 28.510502, Acc: 7803.526\n",
      "Val Loss: 36.934835, Acc: 7980.113\n",
      "7980.112890625\n",
      "7980.112890625 7980.112890625\n",
      "Epoch: 66 Train Loss: 26.451678, Acc: 7481.632\n",
      "Val Loss: 35.490311, Acc: 8262.141\n",
      "8262.14140625\n",
      "Epoch: 67 Train Loss: 26.622988, Acc: 7575.346\n",
      "Val Loss: 45.420395, Acc: 8344.112\n",
      "8344.11171875\n",
      "Epoch: 68 Train Loss: 29.658985, Acc: 7878.377\n",
      "Val Loss: 34.651083, Acc: 8086.948\n",
      "8086.948046875\n",
      "Epoch: 69 Train Loss: 28.250087, Acc: 7644.767\n",
      "Val Loss: 36.734320, Acc: 7956.830\n",
      "7956.8296875\n",
      "7956.8296875 7956.8296875\n",
      "Epoch: 70 Train Loss: 33.546026, Acc: 8698.156\n",
      "Val Loss: 51.122395, Acc: 10165.514\n",
      "10165.5140625\n",
      "Epoch: 71 Train Loss: 30.236985, Acc: 8589.256\n",
      "Val Loss: 33.322708, Acc: 7877.900\n",
      "7877.9\n",
      "7877.9 7877.9\n",
      "Epoch: 72 Train Loss: 25.741927, Acc: 7271.633\n",
      "Val Loss: 36.682470, Acc: 8013.163\n",
      "8013.1625\n",
      "Epoch: 73 Train Loss: 24.412406, Acc: 7128.534\n",
      "Val Loss: 34.505537, Acc: 8065.619\n",
      "8065.619140625\n",
      "Epoch: 74 Train Loss: 24.474589, Acc: 7207.423\n",
      "Val Loss: 32.332006, Acc: 7434.927\n",
      "7434.9265625\n",
      "7434.9265625 7434.9265625\n",
      "Epoch: 75 Train Loss: 25.059768, Acc: 7291.639\n",
      "Val Loss: 41.679171, Acc: 7937.900\n",
      "7937.9\n",
      "Epoch: 76 Train Loss: 25.310942, Acc: 7403.025\n",
      "Val Loss: 34.756642, Acc: 7580.606\n",
      "7580.605859375\n",
      "Epoch: 77 Train Loss: 24.996713, Acc: 7419.345\n",
      "Val Loss: 35.233722, Acc: 7645.784\n",
      "7645.783984375\n",
      "Epoch: 78 Train Loss: 27.182111, Acc: 7429.522\n",
      "Val Loss: 37.981586, Acc: 9419.288\n",
      "9419.287890625\n",
      "Epoch: 79 Train Loss: 34.398117, Acc: 9624.413\n",
      "Val Loss: 51.790083, Acc: 12407.189\n",
      "12407.189453125\n",
      "Epoch: 80 Train Loss: 29.471901, Acc: 8976.047\n",
      "Val Loss: 40.522345, Acc: 10690.466\n",
      "10690.465625\n",
      "Epoch: 81 Train Loss: 31.999640, Acc: 8962.661\n",
      "Val Loss: 45.638889, Acc: 10451.088\n",
      "10451.08828125\n",
      "Epoch: 82 Train Loss: 25.986034, Acc: 7969.704\n",
      "Val Loss: 31.798189, Acc: 7260.482\n",
      "7260.482421875\n",
      "7260.482421875 7260.482421875\n",
      "Epoch: 83 Train Loss: 24.808350, Acc: 7305.471\n",
      "Val Loss: 33.840988, Acc: 7100.198\n",
      "7100.1984375\n",
      "7100.1984375 7100.1984375\n",
      "Epoch: 84 Train Loss: 24.652353, Acc: 7680.487\n",
      "Val Loss: 35.235602, Acc: 7337.804\n",
      "7337.803515625\n",
      "Epoch: 85 Train Loss: 22.334271, Acc: 6927.410\n",
      "Val Loss: 33.403867, Acc: 7140.794\n",
      "7140.79375\n",
      "Epoch: 86 Train Loss: 25.485187, Acc: 7237.532\n",
      "Val Loss: 33.762255, Acc: 8895.469\n",
      "8895.46875\n",
      "Epoch: 87 Train Loss: 26.725818, Acc: 7735.714\n",
      "Val Loss: 35.298767, Acc: 7083.718\n",
      "7083.71796875\n",
      "7083.71796875 7083.71796875\n",
      "Epoch: 88 Train Loss: 24.290697, Acc: 7389.377\n",
      "Val Loss: 43.457407, Acc: 8241.457\n",
      "8241.45703125\n",
      "Epoch: 89 Train Loss: 30.022148, Acc: 8848.000\n",
      "Val Loss: 38.164825, Acc: 10436.890\n",
      "10436.890234375\n",
      "Epoch: 90 Train Loss: 27.582784, Acc: 7647.497\n",
      "Val Loss: 31.815475, Acc: 6961.392\n",
      "6961.3921875\n",
      "6961.3921875 6961.3921875\n",
      "Epoch: 91 Train Loss: 21.668736, Acc: 6660.133\n",
      "Val Loss: 32.427618, Acc: 6955.670\n",
      "6955.66953125\n",
      "6955.66953125 6955.66953125\n",
      "Epoch: 92 Train Loss: 22.195939, Acc: 6624.778\n",
      "Val Loss: 31.480684, Acc: 7966.733\n",
      "7966.7328125\n",
      "Epoch: 93 Train Loss: 22.588521, Acc: 6973.869\n",
      "Val Loss: 32.153139, Acc: 8233.459\n",
      "8233.458984375\n",
      "Epoch: 94 Train Loss: 21.961989, Acc: 7101.787\n",
      "Val Loss: 29.785586, Acc: 6886.107\n",
      "6886.10703125\n",
      "6886.10703125 6886.10703125\n",
      "Epoch: 95 Train Loss: 21.141392, Acc: 6423.392\n",
      "Val Loss: 30.946884, Acc: 6972.827\n",
      "6972.82734375\n",
      "Epoch: 96 Train Loss: 21.984644, Acc: 6811.618\n",
      "Val Loss: 32.792498, Acc: 6661.425\n",
      "6661.425\n",
      "6661.425 6661.425\n",
      "Epoch: 97 Train Loss: 25.009832, Acc: 7426.045\n",
      "Val Loss: 48.812771, Acc: 10390.782\n",
      "10390.782421875\n",
      "Epoch: 98 Train Loss: 26.125191, Acc: 8573.358\n",
      "Val Loss: 34.145384, Acc: 9602.346\n",
      "9602.345703125\n",
      "Epoch: 99 Train Loss: 24.846390, Acc: 7463.017\n",
      "Val Loss: 42.931814, Acc: 7591.838\n",
      "7591.83828125\n",
      "Epoch: 100 Train Loss: 24.849062, Acc: 7690.128\n",
      "Val Loss: 42.063143, Acc: 7074.523\n",
      "7074.52265625\n",
      "Epoch: 101 Train Loss: 24.613499, Acc: 7380.831\n",
      "Val Loss: 36.810864, Acc: 7523.038\n",
      "7523.0375\n",
      "Epoch: 102 Train Loss: 21.141385, Acc: 6547.006\n",
      "Val Loss: 32.716637, Acc: 6843.438\n",
      "6843.43828125\n",
      "Epoch: 103 Train Loss: 23.302066, Acc: 7590.696\n",
      "Val Loss: 42.759645, Acc: 7949.236\n",
      "7949.236328125\n",
      "Epoch: 104 Train Loss: 24.965272, Acc: 7453.059\n",
      "Val Loss: 32.144617, Acc: 6861.365\n",
      "6861.36484375\n",
      "Epoch: 105 Train Loss: 23.078685, Acc: 7375.493\n",
      "Val Loss: 34.659131, Acc: 6809.424\n",
      "6809.42421875\n",
      "Epoch: 106 Train Loss: 21.484401, Acc: 6534.313\n",
      "Val Loss: 30.020228, Acc: 6898.937\n",
      "6898.93671875\n",
      "Epoch: 107 Train Loss: 19.965989, Acc: 6312.852\n",
      "Val Loss: 32.197920, Acc: 6524.977\n",
      "6524.976953125\n",
      "6524.976953125 6524.976953125\n",
      "Epoch: 108 Train Loss: 19.474048, Acc: 6083.079\n",
      "Val Loss: 31.202082, Acc: 6509.037\n",
      "6509.037109375\n",
      "6509.037109375 6509.037109375\n",
      "Epoch: 109 Train Loss: 19.228022, Acc: 6220.398\n",
      "Val Loss: 36.406969, Acc: 6560.348\n",
      "6560.34765625\n",
      "Epoch: 110 Train Loss: 20.701859, Acc: 6511.244\n",
      "Val Loss: 33.780380, Acc: 8272.462\n",
      "8272.462109375\n",
      "Epoch: 111 Train Loss: 20.436434, Acc: 6738.891\n",
      "Val Loss: 31.283291, Acc: 8162.785\n",
      "8162.784765625\n",
      "Epoch: 112 Train Loss: 25.592178, Acc: 8798.256\n",
      "Val Loss: 43.241905, Acc: 11130.526\n",
      "11130.526171875\n",
      "Epoch: 113 Train Loss: 22.836979, Acc: 7292.270\n",
      "Val Loss: 28.793806, Acc: 6245.652\n",
      "6245.6521484375\n",
      "6245.6521484375 6245.6521484375\n",
      "Epoch: 114 Train Loss: 22.136458, Acc: 7845.707\n",
      "Val Loss: 29.076917, Acc: 6681.545\n",
      "6681.54453125\n",
      "Epoch: 115 Train Loss: 21.808295, Acc: 6888.321\n",
      "Val Loss: 37.236725, Acc: 9708.724\n",
      "9708.72421875\n",
      "Epoch: 116 Train Loss: 20.717748, Acc: 7077.693\n",
      "Val Loss: 31.754356, Acc: 6561.581\n",
      "6561.5806640625\n",
      "Epoch: 117 Train Loss: 18.879794, Acc: 6037.488\n",
      "Val Loss: 30.977879, Acc: 6869.633\n",
      "6869.633203125\n",
      "Epoch: 118 Train Loss: 19.422130, Acc: 6308.711\n",
      "Val Loss: 28.941413, Acc: 6190.701\n",
      "6190.70078125\n",
      "6190.70078125 6190.70078125\n",
      "Epoch: 119 Train Loss: 19.781906, Acc: 6434.517\n",
      "Val Loss: 32.570180, Acc: 6210.257\n",
      "6210.25703125\n",
      "Epoch: 120 Train Loss: 21.019578, Acc: 6676.335\n",
      "Val Loss: 40.443429, Acc: 8216.002\n",
      "8216.001953125\n",
      "Epoch: 121 Train Loss: 21.574785, Acc: 7562.028\n",
      "Val Loss: 32.897721, Acc: 7111.846\n",
      "7111.846484375\n",
      "Epoch: 122 Train Loss: 20.105594, Acc: 7158.368\n",
      "Val Loss: 36.308065, Acc: 9696.170\n",
      "9696.1703125\n",
      "Epoch: 123 Train Loss: 22.221801, Acc: 7283.663\n",
      "Val Loss: 35.512316, Acc: 6830.466\n",
      "6830.466015625\n",
      "Epoch: 124 Train Loss: 24.292375, Acc: 7736.267\n",
      "Val Loss: 31.463408, Acc: 7093.411\n",
      "7093.4109375\n",
      "Epoch: 125 Train Loss: 24.412196, Acc: 7755.076\n",
      "Val Loss: 32.865911, Acc: 6714.455\n",
      "6714.455078125\n",
      "Epoch: 126 Train Loss: 20.299656, Acc: 6536.658\n",
      "Val Loss: 28.923015, Acc: 6231.907\n",
      "6231.9068359375\n",
      "Epoch: 127 Train Loss: 17.471845, Acc: 5656.183\n",
      "Val Loss: 29.622222, Acc: 6043.416\n",
      "6043.415625\n",
      "6043.415625 6043.415625\n",
      "Epoch: 128 Train Loss: 18.055058, Acc: 5873.773\n",
      "Val Loss: 28.937353, Acc: 6202.340\n",
      "6202.3400390625\n",
      "Epoch: 129 Train Loss: 20.016999, Acc: 6569.865\n",
      "Val Loss: 41.377821, Acc: 10972.478\n",
      "10972.478125\n",
      "Epoch: 130 Train Loss: 22.932469, Acc: 7466.702\n",
      "Val Loss: 60.203463, Acc: 15332.494\n",
      "15332.49375\n",
      "Epoch: 131 Train Loss: 32.476956, Acc: 10585.216\n",
      "Val Loss: 32.064814, Acc: 6063.283\n",
      "6063.2833984375\n",
      "Epoch: 132 Train Loss: 20.561173, Acc: 7142.459\n",
      "Val Loss: 36.740793, Acc: 11172.447\n",
      "11172.446875\n",
      "Epoch: 133 Train Loss: 21.950029, Acc: 7431.921\n",
      "Val Loss: 35.733821, Acc: 8551.089\n",
      "8551.0890625\n",
      "Epoch: 134 Train Loss: 20.443101, Acc: 7136.512\n",
      "Val Loss: 36.709322, Acc: 7033.695\n",
      "7033.69453125\n",
      "Epoch: 135 Train Loss: 18.312982, Acc: 6177.222\n",
      "Val Loss: 38.212008, Acc: 6783.834\n",
      "6783.834375\n",
      "Epoch: 136 Train Loss: 24.158650, Acc: 7885.161\n",
      "Val Loss: 29.614265, Acc: 6609.014\n",
      "6609.0140625\n",
      "Epoch: 137 Train Loss: 21.214083, Acc: 7388.860\n",
      "Val Loss: 32.577666, Acc: 7880.324\n",
      "7880.32421875\n",
      "Epoch: 138 Train Loss: 18.859453, Acc: 6757.414\n",
      "Val Loss: 34.259818, Acc: 6589.052\n",
      "6589.0515625\n",
      "Epoch: 139 Train Loss: 17.177946, Acc: 5896.104\n",
      "Val Loss: 32.454886, Acc: 6470.788\n",
      "6470.7884765625\n",
      "Epoch: 140 Train Loss: 17.219371, Acc: 5702.087\n",
      "Val Loss: 29.997652, Acc: 6003.747\n",
      "6003.746875\n",
      "6003.746875 6003.746875\n",
      "Epoch: 141 Train Loss: 17.304406, Acc: 5705.926\n",
      "Val Loss: 28.897153, Acc: 6257.033\n",
      "6257.0328125\n",
      "Epoch: 142 Train Loss: 18.892754, Acc: 6169.025\n",
      "Val Loss: 42.983658, Acc: 13265.968\n",
      "13265.96796875\n",
      "Epoch: 143 Train Loss: 34.392646, Acc: 11226.226\n",
      "Val Loss: 32.671132, Acc: 6834.636\n",
      "6834.636328125\n",
      "Epoch: 144 Train Loss: 30.558089, Acc: 10236.626\n",
      "Val Loss: 38.147450, Acc: 7255.942\n",
      "7255.941796875\n",
      "Epoch: 145 Train Loss: 26.492563, Acc: 9539.487\n",
      "Val Loss: 44.160509, Acc: 9405.630\n",
      "9405.63046875\n",
      "Epoch: 146 Train Loss: 18.602840, Acc: 6705.359\n",
      "Val Loss: 29.822758, Acc: 6674.023\n",
      "6674.0234375\n",
      "Epoch: 147 Train Loss: 19.125109, Acc: 7094.558\n",
      "Val Loss: 30.475214, Acc: 8005.170\n",
      "8005.1703125\n",
      "Epoch: 148 Train Loss: 16.041038, Acc: 5750.466\n",
      "Val Loss: 29.095053, Acc: 5671.760\n",
      "5671.7599609375\n",
      "5671.7599609375 5671.7599609375\n",
      "Epoch: 149 Train Loss: 15.013757, Acc: 5041.141\n",
      "Val Loss: 30.318707, Acc: 5863.138\n",
      "5863.13828125\n",
      "Epoch: 150 Train Loss: 15.729801, Acc: 5383.656\n",
      "Val Loss: 29.623939, Acc: 5847.361\n",
      "5847.3611328125\n",
      "Epoch: 151 Train Loss: 17.450508, Acc: 6250.509\n",
      "Val Loss: 37.883897, Acc: 9525.671\n",
      "9525.67109375\n",
      "Epoch: 152 Train Loss: 17.665221, Acc: 6325.254\n",
      "Val Loss: 32.234056, Acc: 6272.864\n",
      "6272.8642578125\n",
      "Epoch: 153 Train Loss: 15.386729, Acc: 5315.492\n",
      "Val Loss: 31.734236, Acc: 5710.147\n",
      "5710.1466796875\n",
      "Epoch: 154 Train Loss: 15.679607, Acc: 5390.603\n",
      "Val Loss: 35.490539, Acc: 6548.616\n",
      "6548.615625\n",
      "Epoch: 155 Train Loss: 17.693032, Acc: 6449.803\n",
      "Val Loss: 31.165311, Acc: 5654.781\n",
      "5654.780859375\n",
      "5654.780859375 5654.780859375\n",
      "Epoch: 156 Train Loss: 15.999238, Acc: 5570.403\n",
      "Val Loss: 35.019865, Acc: 7751.193\n",
      "7751.193359375\n",
      "Epoch: 157 Train Loss: 19.151566, Acc: 7043.937\n",
      "Val Loss: 30.610917, Acc: 7496.452\n",
      "7496.45234375\n",
      "Epoch: 158 Train Loss: 19.988839, Acc: 6227.732\n",
      "Val Loss: 39.585924, Acc: 11780.866\n",
      "11780.86640625\n",
      "Epoch: 159 Train Loss: 22.417298, Acc: 7564.908\n",
      "Val Loss: 57.795620, Acc: 13998.974\n",
      "13998.97421875\n",
      "Epoch: 160 Train Loss: 24.247685, Acc: 8893.015\n",
      "Val Loss: 48.015579, Acc: 10045.950\n",
      "10045.949609375\n",
      "Epoch: 161 Train Loss: 25.777010, Acc: 9171.032\n",
      "Val Loss: 45.370720, Acc: 10649.316\n",
      "10649.31640625\n",
      "Epoch: 162 Train Loss: 24.287351, Acc: 8784.731\n",
      "Val Loss: 36.793315, Acc: 6672.307\n",
      "6672.306640625\n",
      "Epoch: 163 Train Loss: 22.413307, Acc: 8214.413\n",
      "Val Loss: 42.127251, Acc: 10906.733\n",
      "10906.733203125\n",
      "Epoch: 164 Train Loss: 22.083087, Acc: 7811.356\n",
      "Val Loss: 31.439976, Acc: 7152.014\n",
      "7152.014453125\n",
      "Epoch: 165 Train Loss: 18.869885, Acc: 6503.803\n",
      "Val Loss: 37.728295, Acc: 9462.948\n",
      "9462.94765625\n",
      "Epoch: 166 Train Loss: 21.533603, Acc: 8470.886\n",
      "Val Loss: 35.179337, Acc: 8172.558\n",
      "8172.558203125\n",
      "Epoch: 167 Train Loss: 15.151943, Acc: 5154.556\n",
      "Val Loss: 37.790819, Acc: 7497.153\n",
      "7497.153125\n",
      "Epoch: 168 Train Loss: 17.728193, Acc: 6076.119\n",
      "Val Loss: 37.107183, Acc: 6369.210\n",
      "6369.21015625\n",
      "Epoch: 169 Train Loss: 17.036635, Acc: 5726.306\n",
      "Val Loss: 35.861444, Acc: 8551.824\n",
      "8551.823828125\n",
      "Epoch: 170 Train Loss: 15.317226, Acc: 5439.158\n",
      "Val Loss: 32.557855, Acc: 5731.635\n",
      "5731.63515625\n",
      "Epoch: 171 Train Loss: 14.880939, Acc: 5331.798\n",
      "Val Loss: 28.461468, Acc: 5746.148\n",
      "5746.14765625\n",
      "Epoch: 172 Train Loss: 15.681741, Acc: 5892.470\n",
      "Val Loss: 27.497291, Acc: 5578.043\n",
      "5578.043359375\n",
      "5578.043359375 5578.043359375\n",
      "Epoch: 173 Train Loss: 16.575566, Acc: 6120.450\n",
      "Val Loss: 29.872998, Acc: 5888.936\n",
      "5888.936328125\n",
      "Epoch: 174 Train Loss: 17.956688, Acc: 6157.626\n",
      "Val Loss: 34.206430, Acc: 5714.148\n",
      "5714.1484375\n",
      "Epoch: 175 Train Loss: 17.733892, Acc: 5918.473\n",
      "Val Loss: 34.670281, Acc: 9077.580\n",
      "9077.580078125\n",
      "Epoch: 176 Train Loss: 15.680304, Acc: 5580.843\n",
      "Val Loss: 30.757233, Acc: 5544.520\n",
      "5544.5197265625\n",
      "5544.5197265625 5544.5197265625\n",
      "Epoch: 177 Train Loss: 14.842395, Acc: 5384.029\n",
      "Val Loss: 31.809387, Acc: 8707.182\n",
      "8707.18203125\n",
      "Epoch: 178 Train Loss: 18.955903, Acc: 7658.173\n",
      "Val Loss: 32.896573, Acc: 9426.410\n",
      "9426.41015625\n",
      "Epoch: 179 Train Loss: 17.452453, Acc: 6555.721\n",
      "Val Loss: 34.181369, Acc: 9790.170\n",
      "9790.16953125\n",
      "Epoch: 180 Train Loss: 34.832591, Acc: 10811.186\n",
      "Val Loss: 49.245895, Acc: 15343.531\n",
      "15343.53125\n",
      "Epoch: 181 Train Loss: 53.400560, Acc: 15147.486\n",
      "Val Loss: 147.519104, Acc: 30323.581\n",
      "30323.58125\n",
      "Epoch: 182 Train Loss: 91.497603, Acc: 19786.661\n",
      "Val Loss: 62.933672, Acc: 17218.351\n",
      "17218.35078125\n",
      "Epoch: 183 Train Loss: 45.014556, Acc: 13624.795\n",
      "Val Loss: 31.454147, Acc: 7105.624\n",
      "7105.62421875\n",
      "Epoch: 184 Train Loss: 20.438149, Acc: 7542.832\n",
      "Val Loss: 30.983937, Acc: 5869.384\n",
      "5869.3841796875\n",
      "Epoch: 185 Train Loss: 17.159919, Acc: 6219.418\n",
      "Val Loss: 30.192680, Acc: 5476.638\n",
      "5476.6376953125\n",
      "5476.6376953125 5476.6376953125\n",
      "Epoch: 186 Train Loss: 15.926452, Acc: 5723.383\n",
      "Val Loss: 32.359467, Acc: 7657.702\n",
      "7657.7015625\n",
      "Epoch: 187 Train Loss: 15.881164, Acc: 6229.803\n",
      "Val Loss: 40.546435, Acc: 9899.335\n",
      "9899.334765625\n",
      "Epoch: 188 Train Loss: 22.862311, Acc: 8151.918\n",
      "Val Loss: 34.376024, Acc: 5828.464\n",
      "5828.4638671875\n",
      "Epoch: 189 Train Loss: 19.705772, Acc: 6959.185\n",
      "Val Loss: 38.884274, Acc: 5910.095\n",
      "5910.0951171875\n",
      "Epoch: 190 Train Loss: 18.994422, Acc: 6190.310\n",
      "Val Loss: 30.201163, Acc: 5793.333\n",
      "5793.3326171875\n",
      "Epoch: 191 Train Loss: 17.015970, Acc: 6500.689\n",
      "Val Loss: 48.575369, Acc: 10777.361\n",
      "10777.361328125\n",
      "Epoch: 192 Train Loss: 18.766263, Acc: 6538.026\n",
      "Val Loss: 31.702198, Acc: 8161.704\n",
      "8161.704296875\n",
      "Epoch: 193 Train Loss: 15.196657, Acc: 5963.767\n",
      "Val Loss: 27.989863, Acc: 6169.699\n",
      "6169.6994140625\n",
      "Epoch: 194 Train Loss: 13.666478, Acc: 5079.187\n",
      "Val Loss: 29.606436, Acc: 7425.458\n",
      "7425.458203125\n",
      "Epoch: 195 Train Loss: 16.312855, Acc: 5969.607\n",
      "Val Loss: 28.757137, Acc: 5769.627\n",
      "5769.6271484375\n",
      "Epoch: 196 Train Loss: 14.813195, Acc: 5736.541\n",
      "Val Loss: 28.548332, Acc: 5227.106\n",
      "5227.105859375\n",
      "5227.105859375 5227.105859375\n",
      "Epoch: 197 Train Loss: 13.267844, Acc: 4735.859\n",
      "Val Loss: 28.083395, Acc: 6185.127\n",
      "6185.12734375\n",
      "Epoch: 198 Train Loss: 15.246140, Acc: 5632.380\n",
      "Val Loss: 27.878202, Acc: 5967.633\n",
      "5967.6333984375\n",
      "Epoch: 199 Train Loss: 14.908825, Acc: 5606.229\n",
      "Val Loss: 30.469127, Acc: 6052.358\n",
      "6052.3578125\n",
      "Epoch: 200 Train Loss: 13.775771, Acc: 5348.510\n",
      "Val Loss: 30.022088, Acc: 6516.661\n",
      "6516.6609375\n",
      "Epoch: 201 Train Loss: 13.488064, Acc: 5227.611\n",
      "Val Loss: 30.695229, Acc: 7046.755\n",
      "7046.755078125\n",
      "Epoch: 202 Train Loss: 13.452109, Acc: 5466.977\n",
      "Val Loss: 31.999737, Acc: 6068.993\n",
      "6068.993359375\n",
      "Epoch: 203 Train Loss: 14.762740, Acc: 5728.351\n",
      "Val Loss: 35.815452, Acc: 5505.101\n",
      "5505.10078125\n",
      "Epoch: 204 Train Loss: 18.536457, Acc: 6622.167\n",
      "Val Loss: 31.296970, Acc: 7307.928\n",
      "7307.928125\n",
      "Epoch: 205 Train Loss: 14.089267, Acc: 5158.628\n",
      "Val Loss: 30.116544, Acc: 6494.780\n",
      "6494.78046875\n",
      "Epoch: 206 Train Loss: 12.627516, Acc: 4698.437\n",
      "Val Loss: 30.101751, Acc: 5193.725\n",
      "5193.725390625\n",
      "5193.725390625 5193.725390625\n",
      "Epoch: 207 Train Loss: 15.650519, Acc: 6065.804\n",
      "Val Loss: 31.814521, Acc: 6883.173\n",
      "6883.1734375\n",
      "Epoch: 208 Train Loss: 18.109926, Acc: 6296.897\n",
      "Val Loss: 32.805640, Acc: 7138.715\n",
      "7138.71484375\n",
      "Epoch: 209 Train Loss: 17.481667, Acc: 6278.581\n",
      "Val Loss: 30.822543, Acc: 5685.302\n",
      "5685.30234375\n",
      "Epoch: 210 Train Loss: 16.204264, Acc: 5794.210\n",
      "Val Loss: 31.719633, Acc: 5790.320\n",
      "5790.319921875\n",
      "Epoch: 211 Train Loss: 13.525894, Acc: 5170.653\n",
      "Val Loss: 27.692384, Acc: 5207.860\n",
      "5207.8599609375\n",
      "Epoch: 212 Train Loss: 12.546574, Acc: 4599.633\n",
      "Val Loss: 31.731279, Acc: 6110.469\n",
      "6110.469140625\n",
      "Epoch: 213 Train Loss: 13.228028, Acc: 5243.917\n",
      "Val Loss: 31.135823, Acc: 5288.800\n",
      "5288.8001953125\n",
      "Epoch: 214 Train Loss: 15.804156, Acc: 6261.802\n",
      "Val Loss: 30.335138, Acc: 7954.377\n",
      "7954.3765625\n",
      "Epoch: 215 Train Loss: 15.489851, Acc: 5897.068\n",
      "Val Loss: 28.627517, Acc: 6054.899\n",
      "6054.8986328125\n",
      "Epoch: 216 Train Loss: 14.077034, Acc: 5402.762\n",
      "Val Loss: 33.467129, Acc: 8486.427\n",
      "8486.426953125\n",
      "Epoch: 217 Train Loss: 15.182120, Acc: 6111.703\n",
      "Val Loss: 28.161266, Acc: 5359.810\n",
      "5359.81015625\n",
      "Epoch: 218 Train Loss: 13.353635, Acc: 5089.037\n",
      "Val Loss: 27.579197, Acc: 5210.776\n",
      "5210.7763671875\n",
      "Epoch: 219 Train Loss: 18.353032, Acc: 6276.092\n",
      "Val Loss: 32.652757, Acc: 7786.427\n",
      "7786.42734375\n",
      "Epoch: 220 Train Loss: 19.535245, Acc: 7704.974\n",
      "Val Loss: 38.710028, Acc: 12041.201\n",
      "12041.20078125\n",
      "Epoch: 221 Train Loss: 20.996440, Acc: 8528.471\n",
      "Val Loss: 57.719379, Acc: 14799.235\n",
      "14799.23515625\n",
      "Epoch: 222 Train Loss: 45.746514, Acc: 14213.457\n",
      "Val Loss: 40.763733, Acc: 13200.780\n",
      "13200.7796875\n",
      "Epoch: 223 Train Loss: 505.010021, Acc: 50952.683\n",
      "Val Loss: 229.416687, Acc: 39306.634\n",
      "39306.634375\n",
      "Epoch: 224 Train Loss: 404.377509, Acc: 46021.281\n",
      "Val Loss: 42.494783, Acc: 10430.175\n",
      "10430.175\n",
      "Epoch: 225 Train Loss: 183.650360, Acc: 28117.870\n",
      "Val Loss: 118.938389, Acc: 26639.116\n",
      "26639.11640625\n",
      "Epoch: 226 Train Loss: 141.116259, Acc: 25467.450\n",
      "Val Loss: 82.551376, Acc: 20702.567\n",
      "20702.5671875\n",
      "Epoch: 227 Train Loss: 79.429116, Acc: 19717.247\n",
      "Val Loss: 32.297323, Acc: 8238.996\n",
      "8238.995703125\n",
      "Epoch: 228 Train Loss: 42.912651, Acc: 13324.424\n",
      "Val Loss: 36.462626, Acc: 7482.443\n",
      "7482.44296875\n",
      "Epoch: 229 Train Loss: 22.325361, Acc: 8333.868\n",
      "Val Loss: 27.680412, Acc: 7488.582\n",
      "7488.581640625\n",
      "Epoch: 230 Train Loss: 20.339721, Acc: 7868.151\n",
      "Val Loss: 30.441398, Acc: 9484.699\n",
      "9484.69921875\n",
      "Epoch: 231 Train Loss: 18.302363, Acc: 7279.043\n",
      "Val Loss: 28.567838, Acc: 7075.386\n",
      "7075.386328125\n",
      "Epoch: 232 Train Loss: 15.118471, Acc: 5455.398\n",
      "Val Loss: 26.398680, Acc: 6617.184\n",
      "6617.184375\n",
      "Epoch: 233 Train Loss: 15.423639, Acc: 5656.099\n",
      "Val Loss: 27.556952, Acc: 5519.253\n",
      "5519.2533203125\n",
      "Epoch: 234 Train Loss: 14.778710, Acc: 5078.982\n",
      "Val Loss: 31.547180, Acc: 7035.514\n",
      "7035.5140625\n",
      "Epoch: 235 Train Loss: 15.102156, Acc: 5508.072\n",
      "Val Loss: 27.871238, Acc: 5891.777\n",
      "5891.7771484375\n",
      "Epoch: 236 Train Loss: 14.476411, Acc: 5253.157\n",
      "Val Loss: 29.195326, Acc: 5896.085\n",
      "5896.084765625\n",
      "Epoch: 237 Train Loss: 14.408240, Acc: 5419.050\n",
      "Val Loss: 26.797702, Acc: 5844.073\n",
      "5844.073046875\n",
      "Epoch: 238 Train Loss: 14.485837, Acc: 5639.246\n",
      "Val Loss: 27.899089, Acc: 6623.025\n",
      "6623.024609375\n",
      "Epoch: 239 Train Loss: 13.355602, Acc: 4940.490\n",
      "Val Loss: 26.532580, Acc: 5452.128\n",
      "5452.1275390625\n",
      "Epoch: 240 Train Loss: 13.555986, Acc: 5020.742\n",
      "Val Loss: 26.068551, Acc: 5453.643\n",
      "5453.6431640625\n",
      "Epoch: 241 Train Loss: 13.567520, Acc: 5212.381\n",
      "Val Loss: 26.130075, Acc: 5481.513\n",
      "5481.5134765625\n",
      "Epoch: 242 Train Loss: 13.779427, Acc: 5322.218\n",
      "Val Loss: 27.336360, Acc: 5260.637\n",
      "5260.6373046875\n",
      "Epoch: 243 Train Loss: 13.433556, Acc: 4896.288\n",
      "Val Loss: 27.913061, Acc: 5992.274\n",
      "5992.27421875\n",
      "Epoch: 244 Train Loss: 12.850765, Acc: 4720.991\n",
      "Val Loss: 26.619409, Acc: 5255.564\n",
      "5255.5640625\n",
      "Epoch: 245 Train Loss: 13.178665, Acc: 4970.796\n",
      "Val Loss: 29.091065, Acc: 5558.368\n",
      "5558.3681640625\n",
      "Epoch: 246 Train Loss: 12.977415, Acc: 4799.951\n",
      "Val Loss: 28.779459, Acc: 5424.693\n",
      "5424.692578125\n",
      "Epoch: 247 Train Loss: 13.138639, Acc: 4855.545\n",
      "Val Loss: 35.401249, Acc: 5577.408\n",
      "5577.4078125\n",
      "Epoch: 248 Train Loss: 16.610496, Acc: 5441.084\n",
      "Val Loss: 34.422456, Acc: 6735.502\n",
      "6735.501953125\n",
      "Epoch: 249 Train Loss: 14.444482, Acc: 5085.226\n",
      "Val Loss: 29.128635, Acc: 5728.606\n",
      "5728.605859375\n",
      "Epoch: 250 Train Loss: 13.708388, Acc: 5353.936\n",
      "Val Loss: 26.692370, Acc: 5094.281\n",
      "5094.28125\n",
      "5094.28125 5094.28125\n",
      "Epoch: 251 Train Loss: 12.704957, Acc: 4652.374\n",
      "Val Loss: 26.417284, Acc: 5027.837\n",
      "5027.8365234375\n",
      "5027.8365234375 5027.8365234375\n",
      "Epoch: 252 Train Loss: 12.957472, Acc: 4885.726\n",
      "Val Loss: 27.262691, Acc: 5194.301\n",
      "5194.3005859375\n",
      "Epoch: 253 Train Loss: 15.961831, Acc: 6487.615\n",
      "Val Loss: 27.666004, Acc: 5602.133\n",
      "5602.1328125\n",
      "Epoch: 254 Train Loss: 18.827198, Acc: 7471.951\n",
      "Val Loss: 34.878193, Acc: 10491.305\n",
      "10491.3046875\n",
      "Epoch: 255 Train Loss: 18.191955, Acc: 7319.597\n",
      "Val Loss: 27.360605, Acc: 6116.047\n",
      "6116.046875\n",
      "Epoch: 256 Train Loss: 13.911435, Acc: 5662.948\n",
      "Val Loss: 26.311896, Acc: 4998.372\n",
      "4998.3724609375\n",
      "4998.3724609375 4998.3724609375\n",
      "Epoch: 257 Train Loss: 12.263169, Acc: 4628.757\n",
      "Val Loss: 26.096377, Acc: 5317.059\n",
      "5317.059375\n",
      "Epoch: 258 Train Loss: 13.647316, Acc: 5239.023\n",
      "Val Loss: 29.297345, Acc: 7654.695\n",
      "7654.694921875\n",
      "Epoch: 259 Train Loss: 15.685004, Acc: 6398.741\n",
      "Val Loss: 30.000035, Acc: 7735.437\n",
      "7735.437109375\n",
      "Epoch: 260 Train Loss: 17.455715, Acc: 6997.945\n",
      "Val Loss: 60.250593, Acc: 17404.559\n",
      "17404.55859375\n",
      "Epoch: 261 Train Loss: 28.413977, Acc: 11347.478\n",
      "Val Loss: 34.653086, Acc: 7790.776\n",
      "7790.776171875\n",
      "Epoch: 262 Train Loss: 16.785021, Acc: 7520.858\n",
      "Val Loss: 27.081107, Acc: 4997.172\n",
      "4997.1720703125\n",
      "4997.1720703125 4997.1720703125\n",
      "Epoch: 263 Train Loss: 13.771723, Acc: 4970.869\n",
      "Val Loss: 27.604388, Acc: 5215.684\n",
      "5215.68359375\n",
      "Epoch: 264 Train Loss: 16.177005, Acc: 6100.544\n",
      "Val Loss: 29.277237, Acc: 7681.184\n",
      "7681.184375\n",
      "Epoch: 265 Train Loss: 15.317570, Acc: 5820.281\n",
      "Val Loss: 26.093422, Acc: 5165.046\n",
      "5165.0462890625\n",
      "Epoch: 266 Train Loss: 13.128077, Acc: 5115.572\n",
      "Val Loss: 26.989122, Acc: 6393.027\n",
      "6393.0267578125\n",
      "Epoch: 267 Train Loss: 15.245515, Acc: 6012.503\n",
      "Val Loss: 26.416467, Acc: 5797.761\n",
      "5797.7609375\n",
      "Epoch: 268 Train Loss: 13.507592, Acc: 5971.174\n",
      "Val Loss: 33.020424, Acc: 8444.474\n",
      "8444.47421875\n",
      "Epoch: 269 Train Loss: 13.252563, Acc: 5279.603\n",
      "Val Loss: 26.914795, Acc: 5164.958\n",
      "5164.9580078125\n",
      "Epoch: 270 Train Loss: 11.284802, Acc: 4145.430\n",
      "Val Loss: 26.421037, Acc: 4985.613\n",
      "4985.612890625\n",
      "4985.612890625 4985.612890625\n",
      "Epoch: 271 Train Loss: 11.992914, Acc: 4423.431\n",
      "Val Loss: 26.568631, Acc: 5697.470\n",
      "5697.4703125\n",
      "Epoch: 272 Train Loss: 11.879861, Acc: 4578.181\n",
      "Val Loss: 30.585664, Acc: 6714.839\n",
      "6714.838671875\n",
      "Epoch: 273 Train Loss: 13.178725, Acc: 4969.748\n",
      "Val Loss: 31.974614, Acc: 8440.766\n",
      "8440.766015625\n",
      "Epoch: 274 Train Loss: 12.684972, Acc: 5200.506\n",
      "Val Loss: 26.907262, Acc: 5901.836\n",
      "5901.835546875\n",
      "Epoch: 275 Train Loss: 13.628671, Acc: 5671.305\n",
      "Val Loss: 31.786663, Acc: 7531.229\n",
      "7531.229296875\n",
      "Epoch: 276 Train Loss: 15.303551, Acc: 6271.768\n",
      "Val Loss: 27.230077, Acc: 5848.989\n",
      "5848.988671875\n",
      "Epoch: 277 Train Loss: 13.947665, Acc: 5524.031\n",
      "Val Loss: 28.018009, Acc: 6442.606\n",
      "6442.6056640625\n",
      "Epoch: 278 Train Loss: 11.988420, Acc: 4872.384\n",
      "Val Loss: 25.972852, Acc: 5415.232\n",
      "5415.23203125\n",
      "Epoch: 279 Train Loss: 14.706768, Acc: 5944.634\n",
      "Val Loss: 33.456284, Acc: 6633.634\n",
      "6633.63359375\n",
      "Epoch: 280 Train Loss: 30.013815, Acc: 11080.080\n",
      "Val Loss: 33.914205, Acc: 10585.912\n",
      "10585.91171875\n",
      "Epoch: 281 Train Loss: 45.141296, Acc: 14601.675\n",
      "Val Loss: 77.699253, Acc: 21775.142\n",
      "21775.1421875\n",
      "Epoch: 282 Train Loss: 25.308448, Acc: 9860.668\n",
      "Val Loss: 32.310167, Acc: 6652.262\n",
      "6652.2619140625\n",
      "Epoch: 283 Train Loss: 19.321351, Acc: 8445.470\n",
      "Val Loss: 40.470009, Acc: 11162.807\n",
      "11162.806640625\n",
      "Epoch: 284 Train Loss: 17.986929, Acc: 7457.456\n",
      "Val Loss: 45.985136, Acc: 14983.494\n",
      "14983.49375\n",
      "Epoch: 285 Train Loss: 31.256079, Acc: 11557.521\n",
      "Val Loss: 29.333030, Acc: 6147.196\n",
      "6147.19609375\n",
      "Epoch: 286 Train Loss: 24.922660, Acc: 9500.732\n",
      "Val Loss: 27.660828, Acc: 5591.594\n",
      "5591.5935546875\n",
      "Epoch: 287 Train Loss: 13.369559, Acc: 5871.207\n",
      "Val Loss: 26.109709, Acc: 4925.084\n",
      "4925.084375\n",
      "4925.084375 4925.084375\n",
      "Epoch: 288 Train Loss: 11.167156, Acc: 4192.028\n",
      "Val Loss: 28.631371, Acc: 5301.401\n",
      "5301.40078125\n",
      "Epoch: 289 Train Loss: 11.402929, Acc: 4359.504\n",
      "Val Loss: 27.586291, Acc: 5184.825\n",
      "5184.8248046875\n",
      "Epoch: 290 Train Loss: 14.124208, Acc: 5710.546\n",
      "Val Loss: 32.548771, Acc: 9316.125\n",
      "9316.124609375\n",
      "Epoch: 291 Train Loss: 13.780363, Acc: 5332.424\n",
      "Val Loss: 29.418336, Acc: 5678.112\n",
      "5678.1123046875\n",
      "Epoch: 292 Train Loss: 11.869048, Acc: 4926.312\n",
      "Val Loss: 28.094316, Acc: 6979.574\n",
      "6979.57421875\n",
      "Epoch: 293 Train Loss: 13.251241, Acc: 5673.422\n",
      "Val Loss: 31.265897, Acc: 8063.937\n",
      "8063.937109375\n",
      "Epoch: 294 Train Loss: 16.662598, Acc: 6926.368\n",
      "Val Loss: 32.598371, Acc: 9775.741\n",
      "9775.74140625\n",
      "Epoch: 295 Train Loss: 17.207320, Acc: 7437.199\n",
      "Val Loss: 32.390409, Acc: 8632.942\n",
      "8632.9421875\n",
      "Epoch: 296 Train Loss: 12.831906, Acc: 5358.909\n",
      "Val Loss: 27.544328, Acc: 4758.064\n",
      "4758.064453125\n",
      "4758.064453125 4758.064453125\n",
      "Epoch: 297 Train Loss: 12.362265, Acc: 5105.885\n",
      "Val Loss: 31.911948, Acc: 9496.360\n",
      "9496.359765625\n",
      "Epoch: 298 Train Loss: 25.469599, Acc: 10290.782\n",
      "Val Loss: 66.791134, Acc: 17144.236\n",
      "17144.2359375\n",
      "Epoch: 299 Train Loss: 27.488222, Acc: 10500.470\n",
      "Val Loss: 33.487282, Acc: 9767.614\n",
      "9767.613671875\n",
      "Epoch: 300 Train Loss: 14.808839, Acc: 6304.292\n",
      "Val Loss: 29.806582, Acc: 7327.789\n",
      "7327.789453125\n",
      "Epoch: 301 Train Loss: 14.544321, Acc: 6404.999\n",
      "Val Loss: 28.314373, Acc: 7064.346\n",
      "7064.345703125\n",
      "Epoch: 302 Train Loss: 15.717507, Acc: 6937.619\n",
      "Val Loss: 27.730583, Acc: 5610.896\n",
      "5610.895703125\n",
      "Epoch: 303 Train Loss: 13.127252, Acc: 5758.965\n",
      "Val Loss: 25.642535, Acc: 4552.939\n",
      "4552.938671875\n",
      "4552.938671875 4552.938671875\n",
      "Epoch: 304 Train Loss: 15.005432, Acc: 6449.167\n",
      "Val Loss: 30.047799, Acc: 7142.322\n",
      "7142.321875\n",
      "Epoch: 305 Train Loss: 12.452646, Acc: 5241.992\n",
      "Val Loss: 25.301741, Acc: 4512.671\n",
      "4512.670703125\n",
      "4512.670703125 4512.670703125\n",
      "Epoch: 306 Train Loss: 10.826218, Acc: 4289.783\n",
      "Val Loss: 27.291792, Acc: 6296.076\n",
      "6296.0763671875\n",
      "Epoch: 307 Train Loss: 11.230776, Acc: 4375.660\n",
      "Val Loss: 26.386840, Acc: 5002.815\n",
      "5002.8154296875\n",
      "Epoch: 308 Train Loss: 12.570896, Acc: 4844.230\n",
      "Val Loss: 26.263320, Acc: 6061.621\n",
      "6061.620703125\n",
      "Epoch: 309 Train Loss: 12.166836, Acc: 5079.783\n",
      "Val Loss: 26.445637, Acc: 4935.076\n",
      "4935.07578125\n",
      "Epoch: 310 Train Loss: 10.998437, Acc: 4228.400\n",
      "Val Loss: 26.455726, Acc: 4874.538\n",
      "4874.537890625\n",
      "Epoch: 311 Train Loss: 10.724161, Acc: 4131.551\n",
      "Val Loss: 27.312748, Acc: 4688.713\n",
      "4688.712890625\n",
      "Epoch: 312 Train Loss: 12.030648, Acc: 5097.654\n",
      "Val Loss: 27.974137, Acc: 7046.324\n",
      "7046.32421875\n",
      "Epoch: 313 Train Loss: 17.794447, Acc: 8103.330\n",
      "Val Loss: 36.382769, Acc: 9032.412\n",
      "9032.41171875\n",
      "Epoch: 314 Train Loss: 26.695725, Acc: 11221.760\n",
      "Val Loss: 30.832462, Acc: 9733.330\n",
      "9733.3296875\n",
      "Epoch: 315 Train Loss: 21.503878, Acc: 8863.736\n",
      "Val Loss: 61.924776, Acc: 16923.816\n",
      "16923.81640625\n",
      "Epoch: 316 Train Loss: 46.791365, Acc: 15399.240\n",
      "Val Loss: 35.063927, Acc: 11203.388\n",
      "11203.38828125\n",
      "Epoch: 317 Train Loss: 21.237654, Acc: 9022.123\n",
      "Val Loss: 34.667726, Acc: 10119.277\n",
      "10119.2765625\n",
      "Epoch: 318 Train Loss: 24.013789, Acc: 10458.962\n",
      "Val Loss: 33.393618, Acc: 9473.811\n",
      "9473.8109375\n",
      "Epoch: 319 Train Loss: 16.205501, Acc: 7478.637\n",
      "Val Loss: 31.182145, Acc: 4938.704\n",
      "4938.7041015625\n",
      "Epoch: 320 Train Loss: 13.674272, Acc: 5859.882\n",
      "Val Loss: 31.424931, Acc: 9596.480\n",
      "9596.48046875\n",
      "Epoch: 321 Train Loss: 14.743675, Acc: 6152.830\n",
      "Val Loss: 29.025035, Acc: 4760.522\n",
      "4760.521875\n",
      "Epoch: 322 Train Loss: 16.189002, Acc: 6618.889\n",
      "Val Loss: 28.533591, Acc: 5467.878\n",
      "5467.878125\n",
      "Epoch: 323 Train Loss: 39.240364, Acc: 13060.595\n",
      "Val Loss: 137.722198, Acc: 30888.947\n",
      "30888.946875\n",
      "Epoch: 324 Train Loss: 63.094349, Acc: 17660.841\n",
      "Val Loss: 99.162727, Acc: 24611.034\n",
      "24611.034375\n",
      "Epoch: 325 Train Loss: 56.722750, Acc: 16852.633\n",
      "Val Loss: 66.464041, Acc: 16622.934\n",
      "16622.93359375\n",
      "Epoch: 326 Train Loss: 31.925314, Acc: 11122.642\n",
      "Val Loss: 41.634239, Acc: 11031.171\n",
      "11031.171484375\n",
      "Epoch: 327 Train Loss: 24.069589, Acc: 9712.167\n",
      "Val Loss: 30.016835, Acc: 9462.511\n",
      "9462.511328125\n",
      "Epoch: 328 Train Loss: 23.481388, Acc: 9929.106\n",
      "Val Loss: 27.176632, Acc: 5800.359\n",
      "5800.3587890625\n",
      "Epoch: 329 Train Loss: 15.925114, Acc: 6531.391\n",
      "Val Loss: 26.753181, Acc: 6472.268\n",
      "6472.2681640625\n",
      "Epoch: 330 Train Loss: 14.726314, Acc: 5690.961\n",
      "Val Loss: 29.412811, Acc: 7632.087\n",
      "7632.087109375\n",
      "Epoch: 331 Train Loss: 10.958305, Acc: 4698.878\n",
      "Val Loss: 27.415362, Acc: 5032.380\n",
      "5032.3798828125\n",
      "Epoch: 332 Train Loss: 12.610442, Acc: 5231.877\n",
      "Val Loss: 36.180079, Acc: 10974.646\n",
      "10974.64609375\n",
      "Epoch: 333 Train Loss: 14.410789, Acc: 6287.400\n",
      "Val Loss: 26.490977, Acc: 6120.745\n",
      "6120.74453125\n",
      "Epoch: 334 Train Loss: 14.907612, Acc: 5801.195\n",
      "Val Loss: 24.718968, Acc: 5251.487\n",
      "5251.4873046875\n",
      "Epoch: 335 Train Loss: 10.880455, Acc: 4679.359\n",
      "Val Loss: 24.948853, Acc: 6092.643\n",
      "6092.6431640625\n",
      "Epoch: 336 Train Loss: 11.437248, Acc: 4820.595\n",
      "Val Loss: 29.630573, Acc: 7394.721\n",
      "7394.72109375\n",
      "Epoch: 337 Train Loss: 11.854347, Acc: 5360.146\n",
      "Val Loss: 25.457747, Acc: 4758.218\n",
      "4758.218359375\n",
      "Epoch: 338 Train Loss: 13.021197, Acc: 5964.203\n",
      "Val Loss: 36.530492, Acc: 7093.018\n",
      "7093.01796875\n",
      "Epoch: 339 Train Loss: 15.959376, Acc: 7062.472\n",
      "Val Loss: 27.930497, Acc: 5392.135\n",
      "5392.1349609375\n",
      "Epoch: 340 Train Loss: 14.418566, Acc: 5982.269\n",
      "Val Loss: 31.790014, Acc: 8837.987\n",
      "8837.9875\n",
      "Epoch: 341 Train Loss: 29.013924, Acc: 11243.789\n",
      "Val Loss: 53.866282, Acc: 12412.793\n",
      "12412.79296875\n",
      "Epoch: 342 Train Loss: 23.081740, Acc: 9280.587\n",
      "Val Loss: 47.418846, Acc: 15660.271\n",
      "15660.27109375\n",
      "Epoch: 343 Train Loss: 18.620489, Acc: 7856.870\n",
      "Val Loss: 33.735223, Acc: 8307.196\n",
      "8307.19609375\n",
      "Epoch: 344 Train Loss: 12.972651, Acc: 5684.472\n",
      "Val Loss: 26.001993, Acc: 5962.127\n",
      "5962.1271484375\n",
      "Epoch: 345 Train Loss: 10.444922, Acc: 4475.286\n",
      "Val Loss: 24.763280, Acc: 5208.727\n",
      "5208.7271484375\n",
      "Epoch: 346 Train Loss: 10.927004, Acc: 4757.423\n",
      "Val Loss: 23.848397, Acc: 4233.251\n",
      "4233.2513671875\n",
      "4233.2513671875 4233.2513671875\n",
      "Epoch: 347 Train Loss: 10.211541, Acc: 4140.959\n",
      "Val Loss: 23.524446, Acc: 4474.799\n",
      "4474.7994140625\n",
      "Epoch: 348 Train Loss: 9.844764, Acc: 3930.884\n",
      "Val Loss: 25.876101, Acc: 5597.590\n",
      "5597.590234375\n",
      "Epoch: 349 Train Loss: 10.635006, Acc: 4210.020\n",
      "Val Loss: 25.243976, Acc: 4466.119\n",
      "4466.1185546875\n",
      "Epoch: 350 Train Loss: 12.533972, Acc: 5431.500\n",
      "Val Loss: 25.562127, Acc: 4867.306\n",
      "4867.3060546875\n",
      "Epoch: 351 Train Loss: 22.830438, Acc: 9373.143\n",
      "Val Loss: 47.406261, Acc: 15457.386\n",
      "15457.3859375\n",
      "Epoch: 352 Train Loss: 24.073464, Acc: 9973.282\n",
      "Val Loss: 44.022398, Acc: 12370.203\n",
      "12370.202734375\n",
      "Epoch: 353 Train Loss: 17.568073, Acc: 8255.256\n",
      "Val Loss: 36.454706, Acc: 12763.702\n",
      "12763.7015625\n",
      "Epoch: 354 Train Loss: 18.001987, Acc: 7230.304\n",
      "Val Loss: 28.170617, Acc: 7976.155\n",
      "7976.15546875\n",
      "Epoch: 355 Train Loss: 18.416643, Acc: 7831.502\n",
      "Val Loss: 47.338993, Acc: 14539.048\n",
      "14539.04765625\n",
      "Epoch: 356 Train Loss: 15.613756, Acc: 6665.489\n",
      "Val Loss: 24.976752, Acc: 5941.747\n",
      "5941.7470703125\n",
      "Epoch: 357 Train Loss: 10.982932, Acc: 4524.599\n",
      "Val Loss: 24.779806, Acc: 4416.535\n",
      "4416.5349609375\n",
      "Epoch: 358 Train Loss: 11.423885, Acc: 4417.205\n",
      "Val Loss: 26.124719, Acc: 6447.788\n",
      "6447.7880859375\n",
      "Epoch: 359 Train Loss: 10.189157, Acc: 4196.297\n",
      "Val Loss: 25.308830, Acc: 4614.992\n",
      "4614.9916015625\n",
      "Epoch: 360 Train Loss: 9.758468, Acc: 3979.976\n",
      "Val Loss: 27.150305, Acc: 4508.820\n",
      "4508.8197265625\n",
      "Epoch: 361 Train Loss: 14.073665, Acc: 5698.065\n",
      "Val Loss: 23.077844, Acc: 4304.280\n",
      "4304.2802734375\n",
      "Epoch: 362 Train Loss: 10.932783, Acc: 4594.238\n",
      "Val Loss: 26.416182, Acc: 6259.348\n",
      "6259.3482421875\n",
      "Epoch: 363 Train Loss: 14.586834, Acc: 6905.532\n",
      "Val Loss: 31.357821, Acc: 9917.714\n",
      "9917.713671875\n",
      "Epoch: 364 Train Loss: 17.674521, Acc: 7628.184\n",
      "Val Loss: 31.539964, Acc: 9154.337\n",
      "9154.337109375\n",
      "Epoch: 365 Train Loss: 36.920360, Acc: 13208.972\n",
      "Val Loss: 104.532089, Acc: 26243.797\n",
      "26243.796875\n",
      "Epoch: 366 Train Loss: 96.540404, Acc: 22152.836\n",
      "Val Loss: 30.084496, Acc: 6083.638\n",
      "6083.6376953125\n",
      "Epoch: 367 Train Loss: 38.877578, Acc: 13469.484\n",
      "Val Loss: 69.589426, Acc: 17123.223\n",
      "17123.2234375\n",
      "Epoch: 368 Train Loss: 33.315081, Acc: 11654.774\n",
      "Val Loss: 60.800774, Acc: 18965.117\n",
      "18965.1171875\n",
      "Epoch: 369 Train Loss: 30.530599, Acc: 11380.934\n",
      "Val Loss: 32.000082, Acc: 10214.919\n",
      "10214.91875\n",
      "Epoch: 370 Train Loss: 18.913724, Acc: 8157.292\n",
      "Val Loss: 48.012743, Acc: 8663.907\n",
      "8663.906640625\n",
      "Epoch: 371 Train Loss: 19.256112, Acc: 6973.569\n",
      "Val Loss: 32.409468, Acc: 9801.681\n",
      "9801.680859375\n",
      "Epoch: 372 Train Loss: 15.687761, Acc: 6999.964\n",
      "Val Loss: 28.749012, Acc: 7258.732\n",
      "7258.731640625\n",
      "Epoch: 373 Train Loss: 11.760132, Acc: 5059.741\n",
      "Val Loss: 24.951230, Acc: 6238.408\n",
      "6238.408203125\n",
      "Epoch: 374 Train Loss: 12.349098, Acc: 5860.990\n",
      "Val Loss: 29.985577, Acc: 7774.575\n",
      "7774.575\n",
      "Epoch: 375 Train Loss: 10.729741, Acc: 4584.973\n",
      "Val Loss: 24.603243, Acc: 5015.369\n",
      "5015.3685546875\n",
      "Epoch: 376 Train Loss: 11.659306, Acc: 5512.336\n",
      "Val Loss: 25.561680, Acc: 4200.263\n",
      "4200.26328125\n",
      "4200.26328125 4200.26328125\n",
      "Epoch: 377 Train Loss: 14.495652, Acc: 6654.457\n",
      "Val Loss: 36.055428, Acc: 10106.059\n",
      "10106.058984375\n",
      "Epoch: 378 Train Loss: 14.767097, Acc: 7072.007\n",
      "Val Loss: 25.255836, Acc: 5775.450\n",
      "5775.45\n",
      "Epoch: 379 Train Loss: 31.580103, Acc: 10912.993\n",
      "Val Loss: 72.828876, Acc: 22123.952\n",
      "22123.9515625\n",
      "Epoch: 380 Train Loss: 51.229679, Acc: 15603.279\n",
      "Val Loss: 35.280002, Acc: 9547.513\n",
      "9547.51328125\n",
      "Epoch: 381 Train Loss: 16.259616, Acc: 7183.314\n",
      "Val Loss: 26.580562, Acc: 5145.558\n",
      "5145.5583984375\n",
      "Epoch: 382 Train Loss: 9.746389, Acc: 4381.862\n",
      "Val Loss: 25.726889, Acc: 4376.883\n",
      "4376.8828125\n",
      "Epoch: 383 Train Loss: 16.441138, Acc: 7225.263\n",
      "Val Loss: 37.130964, Acc: 12974.337\n",
      "12974.33671875\n",
      "Epoch: 384 Train Loss: 20.728247, Acc: 8681.589\n",
      "Val Loss: 49.628090, Acc: 11770.900\n",
      "11770.9\n",
      "Epoch: 385 Train Loss: 16.740901, Acc: 7339.127\n",
      "Val Loss: 24.034217, Acc: 5084.743\n",
      "5084.742578125\n",
      "Epoch: 386 Train Loss: 10.784569, Acc: 4730.093\n",
      "Val Loss: 25.969387, Acc: 7042.251\n",
      "7042.25078125\n",
      "Epoch: 387 Train Loss: 10.352311, Acc: 4839.972\n",
      "Val Loss: 26.701334, Acc: 4837.243\n",
      "4837.242578125\n",
      "Epoch: 388 Train Loss: 10.271473, Acc: 4499.222\n",
      "Val Loss: 24.312168, Acc: 4207.756\n",
      "4207.7560546875\n",
      "Epoch: 389 Train Loss: 9.705115, Acc: 4104.857\n",
      "Val Loss: 23.500573, Acc: 3955.468\n",
      "3955.46796875\n",
      "3955.46796875 3955.46796875\n",
      "Epoch: 390 Train Loss: 9.257313, Acc: 3664.926\n",
      "Val Loss: 24.778067, Acc: 5934.316\n",
      "5934.3162109375\n",
      "Epoch: 391 Train Loss: 11.023705, Acc: 4669.658\n",
      "Val Loss: 25.535265, Acc: 4525.680\n",
      "4525.6796875\n",
      "Epoch: 392 Train Loss: 10.356771, Acc: 4276.337\n",
      "Val Loss: 23.479864, Acc: 4179.631\n",
      "4179.6310546875\n",
      "Epoch: 393 Train Loss: 10.644541, Acc: 4503.599\n",
      "Val Loss: 26.294593, Acc: 6109.210\n",
      "6109.21015625\n",
      "Epoch: 394 Train Loss: 13.160819, Acc: 6456.134\n",
      "Val Loss: 31.432938, Acc: 9169.482\n",
      "9169.48203125\n",
      "Epoch: 395 Train Loss: 24.661524, Acc: 9891.283\n",
      "Val Loss: 30.360141, Acc: 4775.638\n",
      "4775.6376953125\n",
      "Epoch: 396 Train Loss: 65.168958, Acc: 17318.439\n",
      "Val Loss: 119.932713, Acc: 26658.965\n",
      "26658.96484375\n",
      "Epoch: 397 Train Loss: 188.619055, Acc: 33545.980\n",
      "Val Loss: 205.987160, Acc: 39074.431\n",
      "39074.43125\n",
      "Epoch: 398 Train Loss: 1224.640129, Acc: 85071.790\n",
      "Val Loss: 2292.299194, Acc: 129482.250\n",
      "129482.25\n",
      "Epoch: 399 Train Loss: 812.120927, Acc: 64469.984\n",
      "Val Loss: 400.002884, Acc: 52824.416\n",
      "52824.415625\n",
      "Epoch: 400 Train Loss: 239.364285, Acc: 32921.928\n",
      "Val Loss: 71.958298, Acc: 14995.366\n",
      "14995.365625\n",
      "Epoch: 401 Train Loss: 99.014109, Acc: 20174.118\n",
      "Val Loss: 59.744604, Acc: 12301.093\n",
      "12301.09296875\n",
      "Epoch: 402 Train Loss: 53.542984, Acc: 13442.885\n",
      "Val Loss: 35.232812, Acc: 8982.353\n",
      "8982.353125\n",
      "Epoch: 403 Train Loss: 34.871392, Acc: 11206.918\n",
      "Val Loss: 37.112759, Acc: 11163.650\n",
      "11163.65\n",
      "Epoch: 404 Train Loss: 26.445442, Acc: 9448.839\n",
      "Val Loss: 32.891416, Acc: 10304.391\n",
      "10304.390625\n",
      "Epoch: 405 Train Loss: 23.666787, Acc: 7602.427\n",
      "Val Loss: 27.283231, Acc: 7313.545\n",
      "7313.544921875\n",
      "Epoch: 406 Train Loss: 21.272621, Acc: 7155.481\n",
      "Val Loss: 30.003199, Acc: 9092.526\n",
      "9092.52578125\n",
      "Epoch: 407 Train Loss: 20.531578, Acc: 7286.894\n",
      "Val Loss: 28.811682, Acc: 8332.367\n",
      "8332.3671875\n",
      "Epoch: 408 Train Loss: 16.699747, Acc: 6172.961\n",
      "Val Loss: 25.248889, Acc: 6353.251\n",
      "6353.2505859375\n",
      "Epoch: 409 Train Loss: 15.659820, Acc: 5631.776\n",
      "Val Loss: 25.092079, Acc: 6180.907\n",
      "6180.9068359375\n",
      "Epoch: 410 Train Loss: 14.665756, Acc: 5461.054\n",
      "Val Loss: 25.595607, Acc: 6124.037\n",
      "6124.037109375\n",
      "Epoch: 411 Train Loss: 13.845690, Acc: 5299.911\n",
      "Val Loss: 30.493015, Acc: 6193.183\n",
      "6193.1830078125\n",
      "Epoch: 412 Train Loss: 14.698825, Acc: 5534.108\n",
      "Val Loss: 27.834976, Acc: 6348.256\n",
      "6348.25625\n",
      "Epoch: 413 Train Loss: 13.350648, Acc: 4965.642\n",
      "Val Loss: 27.974122, Acc: 5521.272\n",
      "5521.271875\n",
      "Epoch: 414 Train Loss: 13.250711, Acc: 5086.242\n",
      "Val Loss: 30.405501, Acc: 6303.970\n",
      "6303.9697265625\n",
      "Epoch: 415 Train Loss: 13.304226, Acc: 5312.719\n",
      "Val Loss: 28.015046, Acc: 6756.385\n",
      "6756.384765625\n",
      "Epoch: 416 Train Loss: 12.585905, Acc: 5097.831\n",
      "Val Loss: 26.624023, Acc: 5322.075\n",
      "5322.074609375\n",
      "Epoch: 417 Train Loss: 11.815223, Acc: 4506.491\n",
      "Val Loss: 27.447877, Acc: 5183.406\n",
      "5183.4064453125\n",
      "Epoch: 418 Train Loss: 11.951355, Acc: 4453.858\n",
      "Val Loss: 27.906490, Acc: 5518.080\n",
      "5518.080078125\n",
      "Epoch: 419 Train Loss: 12.876224, Acc: 4739.473\n",
      "Val Loss: 32.331873, Acc: 6611.657\n",
      "6611.656640625\n",
      "Epoch: 420 Train Loss: 13.400820, Acc: 5169.554\n",
      "Val Loss: 30.194549, Acc: 7217.739\n",
      "7217.739453125\n",
      "Epoch: 421 Train Loss: 11.619614, Acc: 4696.493\n",
      "Val Loss: 25.737323, Acc: 5014.551\n",
      "5014.551171875\n",
      "Epoch: 422 Train Loss: 11.118941, Acc: 4319.630\n",
      "Val Loss: 24.496496, Acc: 4694.497\n",
      "4694.496875\n",
      "Epoch: 423 Train Loss: 10.945897, Acc: 4123.554\n",
      "Val Loss: 25.872574, Acc: 4699.483\n",
      "4699.4830078125\n",
      "Epoch: 424 Train Loss: 10.871907, Acc: 3971.608\n",
      "Val Loss: 26.054864, Acc: 4786.554\n",
      "4786.553515625\n",
      "Epoch: 425 Train Loss: 10.727645, Acc: 3899.122\n",
      "Val Loss: 25.871729, Acc: 4690.458\n",
      "4690.4578125\n",
      "Epoch: 426 Train Loss: 10.491665, Acc: 3885.422\n",
      "Val Loss: 25.630417, Acc: 4713.497\n",
      "4713.497265625\n",
      "Epoch: 427 Train Loss: 10.269345, Acc: 3790.727\n",
      "Val Loss: 25.292451, Acc: 4465.726\n",
      "4465.7263671875\n",
      "Epoch: 428 Train Loss: 10.184696, Acc: 3727.356\n",
      "Val Loss: 24.428234, Acc: 4551.679\n",
      "4551.679296875\n",
      "Epoch: 429 Train Loss: 10.724790, Acc: 3957.628\n",
      "Val Loss: 25.071997, Acc: 5220.165\n",
      "5220.165234375\n",
      "Epoch: 430 Train Loss: 10.362397, Acc: 4027.153\n",
      "Val Loss: 25.784179, Acc: 4409.071\n",
      "4409.07109375\n",
      "Epoch: 431 Train Loss: 10.303150, Acc: 3888.149\n",
      "Val Loss: 25.851357, Acc: 4839.710\n",
      "4839.7099609375\n",
      "Epoch: 432 Train Loss: 10.124710, Acc: 3983.725\n",
      "Val Loss: 26.166427, Acc: 4695.371\n",
      "4695.37109375\n",
      "Epoch: 433 Train Loss: 10.408750, Acc: 3775.712\n",
      "Val Loss: 24.856447, Acc: 4230.708\n",
      "4230.7076171875\n",
      "Epoch: 434 Train Loss: 9.671316, Acc: 3628.104\n",
      "Val Loss: 25.489380, Acc: 4558.045\n",
      "4558.0453125\n",
      "Epoch: 435 Train Loss: 9.829422, Acc: 3581.353\n",
      "Val Loss: 25.164295, Acc: 4235.008\n",
      "4235.008203125\n",
      "Epoch: 436 Train Loss: 9.705515, Acc: 3500.911\n",
      "Val Loss: 24.612267, Acc: 4159.279\n",
      "4159.27890625\n",
      "Epoch: 437 Train Loss: 9.631677, Acc: 3598.976\n",
      "Val Loss: 24.220392, Acc: 4280.907\n",
      "4280.9068359375\n",
      "Epoch: 438 Train Loss: 9.671758, Acc: 3572.051\n",
      "Val Loss: 24.112319, Acc: 4123.725\n",
      "4123.7248046875\n",
      "Epoch: 439 Train Loss: 9.752763, Acc: 3531.250\n",
      "Val Loss: 23.898172, Acc: 4108.111\n",
      "4108.110546875\n",
      "Epoch: 440 Train Loss: 9.580866, Acc: 3675.131\n",
      "Val Loss: 24.788023, Acc: 4206.450\n",
      "4206.449609375\n",
      "Epoch: 441 Train Loss: 9.646187, Acc: 3502.254\n",
      "Val Loss: 24.550244, Acc: 4186.235\n",
      "4186.234765625\n",
      "Epoch: 442 Train Loss: 10.286698, Acc: 3887.101\n",
      "Val Loss: 25.141054, Acc: 4779.354\n",
      "4779.353515625\n",
      "Epoch: 443 Train Loss: 9.748887, Acc: 3878.966\n",
      "Val Loss: 23.659697, Acc: 4038.725\n",
      "4038.724609375\n",
      "Epoch: 444 Train Loss: 10.084473, Acc: 3911.130\n",
      "Val Loss: 30.139274, Acc: 4867.330\n",
      "4867.330078125\n",
      "Epoch: 445 Train Loss: 11.404751, Acc: 3992.134\n",
      "Val Loss: 30.270969, Acc: 5882.667\n",
      "5882.6669921875\n",
      "Epoch: 446 Train Loss: 10.993773, Acc: 4371.584\n",
      "Val Loss: 32.183571, Acc: 6839.539\n",
      "6839.5390625\n",
      "Epoch: 447 Train Loss: 11.271915, Acc: 4938.105\n",
      "Val Loss: 30.396720, Acc: 6704.086\n",
      "6704.086328125\n",
      "Epoch: 448 Train Loss: 10.312280, Acc: 4156.351\n",
      "Val Loss: 24.437228, Acc: 4297.722\n",
      "4297.722265625\n",
      "Epoch: 449 Train Loss: 8.936747, Acc: 3337.936\n",
      "Val Loss: 24.190378, Acc: 3947.105\n",
      "3947.105078125\n",
      "3947.105078125 3947.105078125\n",
      "Epoch: 450 Train Loss: 9.180111, Acc: 3391.136\n",
      "Val Loss: 24.834789, Acc: 3939.040\n",
      "3939.0404296875\n",
      "3939.0404296875 3939.0404296875\n",
      "Epoch: 451 Train Loss: 9.786332, Acc: 3914.899\n",
      "Val Loss: 25.810308, Acc: 3968.535\n",
      "3968.53515625\n",
      "Epoch: 452 Train Loss: 10.163608, Acc: 4063.370\n",
      "Val Loss: 23.420494, Acc: 3929.770\n",
      "3929.7703125\n",
      "3929.7703125 3929.7703125\n",
      "Epoch: 453 Train Loss: 11.338578, Acc: 4568.088\n",
      "Val Loss: 23.838776, Acc: 5228.029\n",
      "5228.02890625\n",
      "Epoch: 454 Train Loss: 10.190627, Acc: 4399.119\n",
      "Val Loss: 23.476197, Acc: 5318.295\n",
      "5318.2947265625\n",
      "Epoch: 455 Train Loss: 10.186235, Acc: 4099.456\n",
      "Val Loss: 23.614228, Acc: 4199.062\n",
      "4199.0615234375\n",
      "Epoch: 456 Train Loss: 10.765455, Acc: 3818.297\n",
      "Val Loss: 24.017088, Acc: 4179.491\n",
      "4179.490625\n",
      "Epoch: 457 Train Loss: 10.551593, Acc: 4257.881\n",
      "Val Loss: 24.642748, Acc: 3884.092\n",
      "3884.0916015625\n",
      "3884.0916015625 3884.0916015625\n",
      "Epoch: 458 Train Loss: 9.450370, Acc: 3389.010\n",
      "Val Loss: 27.195170, Acc: 4400.265\n",
      "4400.2650390625\n",
      "Epoch: 459 Train Loss: 10.195480, Acc: 3919.153\n",
      "Val Loss: 25.334960, Acc: 4430.391\n",
      "4430.3908203125\n",
      "Epoch: 460 Train Loss: 9.167383, Acc: 3656.869\n",
      "Val Loss: 23.302514, Acc: 3726.269\n",
      "3726.2685546875\n",
      "3726.2685546875 3726.2685546875\n",
      "Epoch: 461 Train Loss: 8.613345, Acc: 3109.697\n",
      "Val Loss: 23.198808, Acc: 3682.084\n",
      "3682.0841796875\n",
      "3682.0841796875 3682.0841796875\n",
      "Epoch: 462 Train Loss: 8.669036, Acc: 3134.091\n",
      "Val Loss: 23.027833, Acc: 3611.653\n",
      "3611.653125\n",
      "3611.653125 3611.653125\n",
      "Epoch: 463 Train Loss: 8.309391, Acc: 3090.725\n",
      "Val Loss: 24.076834, Acc: 3779.368\n",
      "3779.367578125\n",
      "Epoch: 464 Train Loss: 8.166838, Acc: 2937.644\n",
      "Val Loss: 23.688796, Acc: 3634.200\n",
      "3634.200390625\n",
      "Epoch: 465 Train Loss: 8.137589, Acc: 2935.189\n",
      "Val Loss: 24.294767, Acc: 3779.312\n",
      "3779.3123046875\n",
      "Epoch: 466 Train Loss: 8.294401, Acc: 2983.804\n",
      "Val Loss: 23.353643, Acc: 3587.067\n",
      "3587.066796875\n",
      "3587.066796875 3587.066796875\n",
      "Epoch: 467 Train Loss: 8.132347, Acc: 2949.132\n",
      "Val Loss: 22.972811, Acc: 3749.645\n",
      "3749.6451171875\n",
      "Epoch: 468 Train Loss: 8.496624, Acc: 3112.363\n",
      "Val Loss: 23.203552, Acc: 4241.126\n",
      "4241.1263671875\n",
      "Epoch: 469 Train Loss: 8.531854, Acc: 3274.602\n",
      "Val Loss: 23.618900, Acc: 3615.861\n",
      "3615.861328125\n",
      "Epoch: 470 Train Loss: 8.258845, Acc: 3001.146\n",
      "Val Loss: 23.558517, Acc: 3907.323\n",
      "3907.32265625\n",
      "Epoch: 471 Train Loss: 8.904272, Acc: 3367.153\n",
      "Val Loss: 26.357656, Acc: 5188.010\n",
      "5188.0103515625\n",
      "Epoch: 472 Train Loss: 10.250635, Acc: 4297.662\n",
      "Val Loss: 30.010143, Acc: 6785.302\n",
      "6785.30234375\n",
      "Epoch: 473 Train Loss: 9.999054, Acc: 4129.225\n",
      "Val Loss: 25.623285, Acc: 4528.244\n",
      "4528.244140625\n",
      "Epoch: 474 Train Loss: 8.310766, Acc: 3154.600\n",
      "Val Loss: 23.654798, Acc: 3588.197\n",
      "3588.1974609375\n",
      "Epoch: 475 Train Loss: 8.839038, Acc: 3253.199\n",
      "Val Loss: 22.723909, Acc: 4284.094\n",
      "4284.0943359375\n",
      "Epoch: 476 Train Loss: 9.501656, Acc: 3312.509\n",
      "Val Loss: 23.191468, Acc: 3577.521\n",
      "3577.5205078125\n",
      "3577.5205078125 3577.5205078125\n",
      "Epoch: 477 Train Loss: 8.365154, Acc: 3082.243\n",
      "Val Loss: 29.060051, Acc: 4602.051\n",
      "4602.05078125\n",
      "Epoch: 478 Train Loss: 9.994035, Acc: 3604.909\n",
      "Val Loss: 23.862761, Acc: 4111.288\n",
      "4111.2875\n",
      "Epoch: 479 Train Loss: 8.454032, Acc: 3537.286\n",
      "Val Loss: 24.072970, Acc: 4140.805\n",
      "4140.805078125\n",
      "Epoch: 480 Train Loss: 8.403041, Acc: 3748.012\n",
      "Val Loss: 22.897776, Acc: 3550.041\n",
      "3550.04140625\n",
      "3550.04140625 3550.04140625\n",
      "Epoch: 481 Train Loss: 8.347686, Acc: 3521.330\n",
      "Val Loss: 23.447059, Acc: 3499.071\n",
      "3499.07109375\n",
      "3499.07109375 3499.07109375\n",
      "Epoch: 482 Train Loss: 8.984725, Acc: 3012.867\n",
      "Val Loss: 22.991423, Acc: 3746.196\n",
      "3746.19609375\n",
      "Epoch: 483 Train Loss: 8.313768, Acc: 3316.906\n",
      "Val Loss: 23.020812, Acc: 3601.602\n",
      "3601.60234375\n",
      "Epoch: 484 Train Loss: 8.270920, Acc: 3159.156\n",
      "Val Loss: 23.721923, Acc: 3891.050\n",
      "3891.049609375\n",
      "Epoch: 485 Train Loss: 8.001290, Acc: 3052.978\n",
      "Val Loss: 23.379162, Acc: 3624.458\n",
      "3624.4583984375\n",
      "Epoch: 486 Train Loss: 9.098921, Acc: 3167.978\n",
      "Val Loss: 23.008681, Acc: 4595.249\n",
      "4595.24921875\n",
      "Epoch: 487 Train Loss: 7.848638, Acc: 3010.398\n",
      "Val Loss: 23.558975, Acc: 3793.582\n",
      "3793.5822265625\n",
      "Epoch: 488 Train Loss: 7.605765, Acc: 2845.776\n",
      "Val Loss: 23.067824, Acc: 3447.028\n",
      "3447.027734375\n",
      "3447.027734375 3447.027734375\n",
      "Epoch: 489 Train Loss: 8.323516, Acc: 3225.875\n",
      "Val Loss: 24.752406, Acc: 4944.180\n",
      "4944.1802734375\n",
      "Epoch: 490 Train Loss: 7.961210, Acc: 3376.492\n",
      "Val Loss: 24.586042, Acc: 4007.780\n",
      "4007.7796875\n",
      "Epoch: 491 Train Loss: 7.796251, Acc: 2757.441\n",
      "Val Loss: 22.830601, Acc: 3396.241\n",
      "3396.24140625\n",
      "3396.24140625 3396.24140625\n",
      "Epoch: 492 Train Loss: 7.725850, Acc: 2907.589\n",
      "Val Loss: 23.351434, Acc: 3464.480\n",
      "3464.48046875\n",
      "Epoch: 493 Train Loss: 7.985278, Acc: 2916.881\n",
      "Val Loss: 24.776453, Acc: 3555.015\n",
      "3555.0154296875\n",
      "Epoch: 494 Train Loss: 9.156973, Acc: 3303.332\n",
      "Val Loss: 23.788591, Acc: 4292.163\n",
      "4292.16328125\n",
      "Epoch: 495 Train Loss: 8.417996, Acc: 3429.837\n",
      "Val Loss: 22.889171, Acc: 3956.968\n",
      "3956.967578125\n",
      "Epoch: 496 Train Loss: 8.403339, Acc: 3110.572\n",
      "Val Loss: 23.562108, Acc: 3908.886\n",
      "3908.8859375\n",
      "Epoch: 497 Train Loss: 7.699110, Acc: 3056.177\n",
      "Val Loss: 24.494397, Acc: 4506.600\n",
      "4506.6\n",
      "Epoch: 498 Train Loss: 7.947340, Acc: 3182.242\n",
      "Val Loss: 23.262457, Acc: 3479.992\n",
      "3479.991796875\n",
      "Epoch: 499 Train Loss: 7.927342, Acc: 3092.728\n",
      "Val Loss: 23.141429, Acc: 3356.106\n",
      "3356.10625\n",
      "3356.10625 3356.10625\n",
      "Epoch: 500 Train Loss: 8.286275, Acc: 3075.055\n",
      "Val Loss: 22.372698, Acc: 3351.491\n",
      "3351.491015625\n",
      "3351.491015625 3351.491015625\n",
      "Epoch: 501 Train Loss: 8.556591, Acc: 3084.696\n",
      "Val Loss: 25.445878, Acc: 5255.489\n",
      "5255.4890625\n",
      "Epoch: 502 Train Loss: 10.476807, Acc: 4393.878\n",
      "Val Loss: 24.007888, Acc: 4329.679\n",
      "4329.678515625\n",
      "Epoch: 503 Train Loss: 8.904993, Acc: 3556.962\n",
      "Val Loss: 24.057649, Acc: 4064.126\n",
      "4064.1263671875\n",
      "Epoch: 504 Train Loss: 9.309411, Acc: 4007.408\n",
      "Val Loss: 26.062405, Acc: 6176.233\n",
      "6176.2326171875\n",
      "Epoch: 505 Train Loss: 8.186529, Acc: 3581.461\n",
      "Val Loss: 22.940331, Acc: 4286.110\n",
      "4286.11015625\n",
      "Epoch: 506 Train Loss: 7.139223, Acc: 2883.566\n",
      "Val Loss: 21.504634, Acc: 3364.424\n",
      "3364.42421875\n",
      "Epoch: 507 Train Loss: 7.260165, Acc: 2778.100\n",
      "Val Loss: 22.129738, Acc: 3414.374\n",
      "3414.3736328125\n",
      "Epoch: 508 Train Loss: 7.186401, Acc: 2943.126\n",
      "Val Loss: 23.407890, Acc: 3466.843\n",
      "3466.8431640625\n",
      "Epoch: 509 Train Loss: 8.182670, Acc: 3298.152\n",
      "Val Loss: 24.584182, Acc: 4241.847\n",
      "4241.8470703125\n",
      "Epoch: 510 Train Loss: 7.709914, Acc: 3053.238\n",
      "Val Loss: 24.868182, Acc: 3755.735\n",
      "3755.7349609375\n",
      "Epoch: 511 Train Loss: 7.345132, Acc: 3042.950\n",
      "Val Loss: 22.622149, Acc: 3328.273\n",
      "3328.273046875\n",
      "3328.273046875 3328.273046875\n",
      "Epoch: 512 Train Loss: 7.749314, Acc: 3151.333\n",
      "Val Loss: 22.718322, Acc: 4001.008\n",
      "4001.008203125\n",
      "Epoch: 513 Train Loss: 7.483105, Acc: 2753.637\n",
      "Val Loss: 24.519613, Acc: 3725.824\n",
      "3725.8236328125\n",
      "Epoch: 514 Train Loss: 7.622132, Acc: 3009.758\n",
      "Val Loss: 21.781498, Acc: 3250.830\n",
      "3250.8298828125\n",
      "3250.8298828125 3250.8298828125\n",
      "Epoch: 515 Train Loss: 7.670539, Acc: 2856.442\n",
      "Val Loss: 21.959775, Acc: 3996.838\n",
      "3996.837890625\n",
      "Epoch: 516 Train Loss: 7.718431, Acc: 2983.342\n",
      "Val Loss: 23.294023, Acc: 3661.765\n",
      "3661.7654296875\n",
      "Epoch: 517 Train Loss: 7.171739, Acc: 2855.670\n",
      "Val Loss: 22.516569, Acc: 3509.582\n",
      "3509.58203125\n",
      "Epoch: 518 Train Loss: 7.217418, Acc: 2800.926\n",
      "Val Loss: 21.445495, Acc: 3150.119\n",
      "3150.11884765625\n",
      "3150.11884765625 3150.11884765625\n",
      "Epoch: 519 Train Loss: 7.318284, Acc: 2690.630\n",
      "Val Loss: 21.525284, Acc: 3195.669\n",
      "3195.66923828125\n",
      "Epoch: 520 Train Loss: 7.631013, Acc: 2777.056\n",
      "Val Loss: 22.142215, Acc: 3218.160\n",
      "3218.159765625\n",
      "Epoch: 521 Train Loss: 7.449604, Acc: 2616.036\n",
      "Val Loss: 22.609688, Acc: 3213.224\n",
      "3213.223828125\n",
      "Epoch: 522 Train Loss: 8.235884, Acc: 3179.872\n",
      "Val Loss: 23.572794, Acc: 4145.567\n",
      "4145.5666015625\n",
      "Epoch: 523 Train Loss: 7.214455, Acc: 2654.617\n",
      "Val Loss: 21.508151, Acc: 3230.282\n",
      "3230.28212890625\n",
      "Epoch: 524 Train Loss: 6.996728, Acc: 2495.868\n",
      "Val Loss: 22.272820, Acc: 3185.654\n",
      "3185.65400390625\n",
      "Epoch: 525 Train Loss: 7.570849, Acc: 2745.401\n",
      "Val Loss: 24.402977, Acc: 3354.721\n",
      "3354.72119140625\n",
      "Epoch: 526 Train Loss: 7.933418, Acc: 3318.523\n",
      "Val Loss: 22.212387, Acc: 4013.724\n",
      "4013.7244140625\n",
      "Epoch: 527 Train Loss: 6.967114, Acc: 3017.656\n",
      "Val Loss: 23.574593, Acc: 3337.148\n",
      "3337.14775390625\n",
      "Epoch: 528 Train Loss: 7.159627, Acc: 2694.378\n",
      "Val Loss: 23.605967, Acc: 3383.333\n",
      "3383.3333984375\n",
      "Epoch: 529 Train Loss: 7.521713, Acc: 3092.961\n",
      "Val Loss: 22.143955, Acc: 3832.965\n",
      "3832.9646484375\n",
      "Epoch: 530 Train Loss: 7.740745, Acc: 3047.270\n",
      "Val Loss: 23.196326, Acc: 3283.214\n",
      "3283.2142578125\n",
      "Epoch: 531 Train Loss: 7.886168, Acc: 2811.808\n",
      "Val Loss: 23.098815, Acc: 4201.005\n",
      "4201.0048828125\n",
      "Epoch: 532 Train Loss: 8.170116, Acc: 3484.894\n",
      "Val Loss: 22.084665, Acc: 3406.585\n",
      "3406.584765625\n",
      "Epoch: 533 Train Loss: 7.621171, Acc: 2972.385\n",
      "Val Loss: 24.676332, Acc: 4300.618\n",
      "4300.617578125\n",
      "Epoch: 534 Train Loss: 7.511776, Acc: 2907.732\n",
      "Val Loss: 23.295695, Acc: 3521.631\n",
      "3521.6314453125\n",
      "Epoch: 535 Train Loss: 8.326206, Acc: 3292.916\n",
      "Val Loss: 26.858253, Acc: 5515.610\n",
      "5515.609765625\n",
      "Epoch: 536 Train Loss: 8.191207, Acc: 3902.163\n",
      "Val Loss: 21.120697, Acc: 3457.340\n",
      "3457.3404296875\n",
      "Epoch: 537 Train Loss: 7.003164, Acc: 3089.366\n",
      "Val Loss: 21.849643, Acc: 3932.889\n",
      "3932.889453125\n",
      "Epoch: 538 Train Loss: 7.294880, Acc: 2884.513\n",
      "Val Loss: 21.703473, Acc: 3312.455\n",
      "3312.455078125\n",
      "Epoch: 539 Train Loss: 6.782299, Acc: 2526.461\n",
      "Val Loss: 22.099295, Acc: 3413.225\n",
      "3413.2251953125\n",
      "Epoch: 540 Train Loss: 7.201245, Acc: 2906.276\n",
      "Val Loss: 22.387869, Acc: 4199.729\n",
      "4199.72890625\n",
      "Epoch: 541 Train Loss: 7.144755, Acc: 3015.905\n",
      "Val Loss: 21.631202, Acc: 3947.231\n",
      "3947.2314453125\n",
      "Epoch: 542 Train Loss: 7.576069, Acc: 3613.262\n",
      "Val Loss: 23.561073, Acc: 4812.823\n",
      "4812.8228515625\n",
      "Epoch: 543 Train Loss: 6.982580, Acc: 2742.693\n",
      "Val Loss: 21.169937, Acc: 3029.924\n",
      "3029.9236328125\n",
      "3029.9236328125 3029.9236328125\n",
      "Epoch: 544 Train Loss: 6.352183, Acc: 2345.948\n",
      "Val Loss: 21.217655, Acc: 3034.892\n",
      "3034.89228515625\n",
      "Epoch: 545 Train Loss: 6.550365, Acc: 2431.630\n",
      "Val Loss: 21.943534, Acc: 3065.622\n",
      "3065.621875\n",
      "Epoch: 546 Train Loss: 7.461220, Acc: 2828.334\n",
      "Val Loss: 21.812825, Acc: 3254.293\n",
      "3254.293359375\n",
      "Epoch: 547 Train Loss: 9.332388, Acc: 4095.941\n",
      "Val Loss: 24.432285, Acc: 6674.816\n",
      "6674.81640625\n",
      "Epoch: 548 Train Loss: 8.421188, Acc: 3936.365\n",
      "Val Loss: 20.986189, Acc: 3943.617\n",
      "3943.6171875\n",
      "Epoch: 549 Train Loss: 7.360034, Acc: 3047.068\n",
      "Val Loss: 22.585683, Acc: 4186.232\n",
      "4186.231640625\n",
      "Epoch: 550 Train Loss: 7.216993, Acc: 2869.468\n",
      "Val Loss: 20.806090, Acc: 3413.757\n",
      "3413.7568359375\n",
      "Epoch: 551 Train Loss: 7.197595, Acc: 3133.970\n",
      "Val Loss: 20.652061, Acc: 3461.880\n",
      "3461.8796875\n",
      "Epoch: 552 Train Loss: 7.549945, Acc: 3867.713\n",
      "Val Loss: 27.210829, Acc: 6454.379\n",
      "6454.3794921875\n",
      "Epoch: 553 Train Loss: 8.132803, Acc: 3555.112\n",
      "Val Loss: 24.254621, Acc: 5073.673\n",
      "5073.673046875\n",
      "Epoch: 554 Train Loss: 8.748418, Acc: 3368.853\n",
      "Val Loss: 26.190646, Acc: 5554.734\n",
      "5554.7337890625\n",
      "Epoch: 555 Train Loss: 7.956923, Acc: 3645.701\n",
      "Val Loss: 22.945606, Acc: 3247.031\n",
      "3247.0306640625\n",
      "Epoch: 556 Train Loss: 9.021003, Acc: 2968.590\n",
      "Val Loss: 25.877928, Acc: 5412.628\n",
      "5412.6283203125\n",
      "Epoch: 557 Train Loss: 8.237401, Acc: 3739.658\n",
      "Val Loss: 26.425113, Acc: 6227.696\n",
      "6227.6955078125\n",
      "Epoch: 558 Train Loss: 10.888115, Acc: 3990.793\n",
      "Val Loss: 34.066706, Acc: 6808.473\n",
      "6808.4734375\n",
      "Epoch: 559 Train Loss: 11.530582, Acc: 4334.829\n",
      "Val Loss: 21.922020, Acc: 3374.931\n",
      "3374.9314453125\n",
      "Epoch: 560 Train Loss: 6.807284, Acc: 2805.238\n",
      "Val Loss: 22.450581, Acc: 3891.168\n",
      "3891.1677734375\n",
      "Epoch: 561 Train Loss: 6.807041, Acc: 2848.195\n",
      "Val Loss: 22.384739, Acc: 3494.210\n",
      "3494.2103515625\n",
      "Epoch: 562 Train Loss: 7.085494, Acc: 2829.925\n",
      "Val Loss: 21.765100, Acc: 3664.043\n",
      "3664.043359375\n",
      "Epoch: 563 Train Loss: 6.684865, Acc: 3124.105\n",
      "Val Loss: 24.056151, Acc: 4086.058\n",
      "4086.058203125\n",
      "Epoch: 564 Train Loss: 8.303788, Acc: 3733.994\n",
      "Val Loss: 25.452536, Acc: 6020.728\n",
      "6020.727734375\n",
      "Epoch: 565 Train Loss: 8.306530, Acc: 4064.371\n",
      "Val Loss: 21.839770, Acc: 3241.115\n",
      "3241.1146484375\n",
      "Epoch: 566 Train Loss: 8.569148, Acc: 2960.733\n",
      "Val Loss: 28.139034, Acc: 5895.521\n",
      "5895.520703125\n",
      "Epoch: 567 Train Loss: 9.555943, Acc: 4898.831\n",
      "Val Loss: 22.259867, Acc: 5054.716\n",
      "5054.716015625\n",
      "Epoch: 568 Train Loss: 11.158285, Acc: 4172.370\n",
      "Val Loss: 26.637606, Acc: 5306.461\n",
      "5306.4609375\n",
      "Epoch: 569 Train Loss: 9.220938, Acc: 3916.697\n",
      "Val Loss: 21.592835, Acc: 5732.419\n",
      "5732.419140625\n",
      "Epoch: 570 Train Loss: 10.702526, Acc: 4842.132\n",
      "Val Loss: 20.365537, Acc: 3198.587\n",
      "3198.58671875\n",
      "Epoch: 571 Train Loss: 12.223146, Acc: 5569.493\n",
      "Val Loss: 21.333287, Acc: 4542.099\n",
      "4542.0990234375\n",
      "Epoch: 572 Train Loss: 8.169517, Acc: 2822.416\n",
      "Val Loss: 23.174240, Acc: 3475.521\n",
      "3475.521484375\n",
      "Epoch: 573 Train Loss: 6.979869, Acc: 3029.774\n",
      "Val Loss: 21.240149, Acc: 3296.234\n",
      "3296.233984375\n",
      "Epoch: 574 Train Loss: 7.371233, Acc: 2991.863\n",
      "Val Loss: 20.963901, Acc: 3808.977\n",
      "3808.9767578125\n",
      "Epoch: 575 Train Loss: 7.964921, Acc: 3728.423\n",
      "Val Loss: 24.770701, Acc: 5592.208\n",
      "5592.2078125\n",
      "Epoch: 576 Train Loss: 7.119699, Acc: 3442.155\n",
      "Val Loss: 21.956089, Acc: 3021.297\n",
      "3021.29716796875\n",
      "3021.29716796875 3021.29716796875\n",
      "Epoch: 577 Train Loss: 6.573002, Acc: 2707.105\n",
      "Val Loss: 21.073300, Acc: 3311.304\n",
      "3311.3041015625\n",
      "Epoch: 578 Train Loss: 6.417396, Acc: 3109.933\n",
      "Val Loss: 21.008649, Acc: 3641.830\n",
      "3641.830078125\n",
      "Epoch: 579 Train Loss: 7.068390, Acc: 2839.656\n",
      "Val Loss: 20.759158, Acc: 3408.407\n",
      "3408.4068359375\n",
      "Epoch: 580 Train Loss: 6.418265, Acc: 2677.639\n",
      "Val Loss: 21.101642, Acc: 3042.000\n",
      "3042.0\n",
      "Epoch: 581 Train Loss: 6.475643, Acc: 2609.560\n",
      "Val Loss: 22.340547, Acc: 3336.116\n",
      "3336.115625\n",
      "Epoch: 582 Train Loss: 6.898356, Acc: 2916.730\n",
      "Val Loss: 21.862223, Acc: 4631.924\n",
      "4631.9236328125\n",
      "Epoch: 583 Train Loss: 6.899431, Acc: 3359.770\n",
      "Val Loss: 20.574780, Acc: 4714.481\n",
      "4714.480859375\n",
      "Epoch: 584 Train Loss: 6.929612, Acc: 2676.727\n",
      "Val Loss: 21.006394, Acc: 3230.793\n",
      "3230.793359375\n",
      "Epoch: 585 Train Loss: 6.441065, Acc: 2647.073\n",
      "Val Loss: 23.335893, Acc: 4394.504\n",
      "4394.5037109375\n",
      "Epoch: 586 Train Loss: 8.564206, Acc: 4038.011\n",
      "Val Loss: 20.463902, Acc: 3094.279\n",
      "3094.2787109375\n",
      "Epoch: 587 Train Loss: 8.003861, Acc: 3172.531\n",
      "Val Loss: 22.071048, Acc: 3597.501\n",
      "3597.5013671875\n",
      "Epoch: 588 Train Loss: 6.498707, Acc: 2757.839\n",
      "Val Loss: 19.913985, Acc: 2869.479\n",
      "2869.47919921875\n",
      "2869.47919921875 2869.47919921875\n",
      "Epoch: 589 Train Loss: 6.265720, Acc: 2360.299\n",
      "Val Loss: 22.199543, Acc: 3205.995\n",
      "3205.994921875\n",
      "Epoch: 590 Train Loss: 7.673767, Acc: 3084.261\n",
      "Val Loss: 28.424096, Acc: 6254.398\n",
      "6254.398046875\n",
      "Epoch: 591 Train Loss: 12.711617, Acc: 5309.041\n",
      "Val Loss: 25.182734, Acc: 5938.528\n",
      "5938.527734375\n",
      "Epoch: 592 Train Loss: 8.740732, Acc: 3997.821\n",
      "Val Loss: 22.686901, Acc: 4736.756\n",
      "4736.7560546875\n",
      "Epoch: 593 Train Loss: 7.187052, Acc: 3287.507\n",
      "Val Loss: 22.331791, Acc: 3509.219\n",
      "3509.219140625\n",
      "Epoch: 594 Train Loss: 8.007751, Acc: 3022.142\n",
      "Val Loss: 21.842962, Acc: 2998.609\n",
      "2998.608984375\n",
      "Epoch: 595 Train Loss: 6.159921, Acc: 2469.789\n",
      "Val Loss: 21.504066, Acc: 3394.365\n",
      "3394.3650390625\n",
      "Epoch: 596 Train Loss: 7.793918, Acc: 2869.977\n",
      "Val Loss: 26.193923, Acc: 5440.815\n",
      "5440.815234375\n",
      "Epoch: 597 Train Loss: 9.170166, Acc: 4198.812\n",
      "Val Loss: 24.762561, Acc: 5809.176\n",
      "5809.1755859375\n",
      "Epoch: 598 Train Loss: 8.241974, Acc: 3961.836\n",
      "Val Loss: 22.067248, Acc: 3667.157\n",
      "3667.157421875\n",
      "Epoch: 599 Train Loss: 6.555092, Acc: 3060.486\n",
      "Val Loss: 23.050491, Acc: 4754.343\n",
      "4754.342578125\n",
      "Epoch: 600 Train Loss: 7.043443, Acc: 3317.947\n",
      "Val Loss: 22.990685, Acc: 5539.470\n",
      "5539.46953125\n",
      "Epoch: 601 Train Loss: 6.624490, Acc: 3011.393\n",
      "Val Loss: 23.532366, Acc: 3593.103\n",
      "3593.1025390625\n",
      "Epoch: 602 Train Loss: 5.896596, Acc: 2476.115\n",
      "Val Loss: 20.468565, Acc: 2979.337\n",
      "2979.33671875\n",
      "Epoch: 603 Train Loss: 6.254129, Acc: 2420.879\n",
      "Val Loss: 21.327527, Acc: 3286.363\n",
      "3286.362890625\n",
      "Epoch: 604 Train Loss: 5.863474, Acc: 2482.522\n",
      "Val Loss: 21.100664, Acc: 2866.251\n",
      "2866.25068359375\n",
      "2866.25068359375 2866.25068359375\n",
      "Epoch: 605 Train Loss: 6.270038, Acc: 2449.998\n",
      "Val Loss: 25.437110, Acc: 3911.913\n",
      "3911.91328125\n",
      "Epoch: 606 Train Loss: 7.261018, Acc: 3002.357\n",
      "Val Loss: 20.483058, Acc: 3193.610\n",
      "3193.6099609375\n",
      "Epoch: 607 Train Loss: 8.022096, Acc: 3024.914\n",
      "Val Loss: 19.784340, Acc: 3560.634\n",
      "3560.6341796875\n",
      "Epoch: 608 Train Loss: 7.996083, Acc: 3749.119\n",
      "Val Loss: 21.270464, Acc: 3096.443\n",
      "3096.44345703125\n",
      "Epoch: 609 Train Loss: 7.823102, Acc: 3250.331\n",
      "Val Loss: 20.153810, Acc: 3269.364\n",
      "3269.3638671875\n",
      "Epoch: 610 Train Loss: 6.785112, Acc: 2959.664\n",
      "Val Loss: 22.491850, Acc: 3606.335\n",
      "3606.33515625\n",
      "Epoch: 611 Train Loss: 7.052844, Acc: 2675.497\n",
      "Val Loss: 20.777773, Acc: 3541.697\n",
      "3541.6970703125\n",
      "Epoch: 612 Train Loss: 8.109796, Acc: 3513.529\n",
      "Val Loss: 22.691441, Acc: 4247.370\n",
      "4247.369921875\n",
      "Epoch: 613 Train Loss: 7.503046, Acc: 3225.936\n",
      "Val Loss: 28.652800, Acc: 6719.325\n",
      "6719.325390625\n",
      "Epoch: 614 Train Loss: 9.141180, Acc: 4623.986\n",
      "Val Loss: 28.563308, Acc: 8064.048\n",
      "8064.048046875\n",
      "Epoch: 615 Train Loss: 16.501236, Acc: 6195.429\n",
      "Val Loss: 41.873981, Acc: 10233.639\n",
      "10233.6390625\n",
      "Epoch: 616 Train Loss: 11.837929, Acc: 5253.569\n",
      "Val Loss: 24.118480, Acc: 6394.986\n",
      "6394.986328125\n",
      "Epoch: 617 Train Loss: 8.305259, Acc: 3958.864\n",
      "Val Loss: 22.365290, Acc: 4087.404\n",
      "4087.404296875\n",
      "Epoch: 618 Train Loss: 6.118002, Acc: 2653.671\n",
      "Val Loss: 21.760931, Acc: 2994.362\n",
      "2994.36181640625\n",
      "Epoch: 619 Train Loss: 6.226502, Acc: 2437.042\n",
      "Val Loss: 23.064794, Acc: 4268.461\n",
      "4268.460546875\n",
      "Epoch: 620 Train Loss: 6.210739, Acc: 2549.717\n",
      "Val Loss: 22.440285, Acc: 3917.614\n",
      "3917.6140625\n",
      "Epoch: 621 Train Loss: 6.771711, Acc: 3066.497\n",
      "Val Loss: 21.652163, Acc: 3542.070\n",
      "3542.0697265625\n",
      "Epoch: 622 Train Loss: 5.855262, Acc: 2326.106\n",
      "Val Loss: 21.415860, Acc: 2857.200\n",
      "2857.19951171875\n",
      "2857.19951171875 2857.19951171875\n",
      "Epoch: 623 Train Loss: 5.555320, Acc: 2290.790\n",
      "Val Loss: 21.563153, Acc: 2912.738\n",
      "2912.73798828125\n",
      "Epoch: 624 Train Loss: 5.730176, Acc: 2172.361\n",
      "Val Loss: 21.628261, Acc: 3351.009\n",
      "3351.0091796875\n",
      "Epoch: 625 Train Loss: 5.613154, Acc: 2459.941\n",
      "Val Loss: 21.649806, Acc: 3745.261\n",
      "3745.261328125\n",
      "Epoch: 626 Train Loss: 6.180903, Acc: 2852.685\n",
      "Val Loss: 20.883146, Acc: 2863.412\n",
      "2863.41240234375\n",
      "Epoch: 627 Train Loss: 5.630335, Acc: 2243.769\n",
      "Val Loss: 22.842931, Acc: 3009.270\n",
      "3009.2697265625\n",
      "Epoch: 628 Train Loss: 5.706585, Acc: 2433.955\n",
      "Val Loss: 21.854199, Acc: 3464.392\n",
      "3464.3921875\n",
      "Epoch: 629 Train Loss: 6.063512, Acc: 2955.042\n",
      "Val Loss: 20.818885, Acc: 4151.911\n",
      "4151.9109375\n",
      "Epoch: 630 Train Loss: 6.412319, Acc: 3068.716\n",
      "Val Loss: 21.698558, Acc: 3749.997\n",
      "3749.997265625\n",
      "Epoch: 631 Train Loss: 5.279076, Acc: 2184.790\n",
      "Val Loss: 20.657013, Acc: 2757.449\n",
      "2757.44912109375\n",
      "2757.44912109375 2757.44912109375\n",
      "Epoch: 632 Train Loss: 5.289346, Acc: 2040.489\n",
      "Val Loss: 20.633060, Acc: 2740.667\n",
      "2740.66748046875\n",
      "2740.66748046875 2740.66748046875\n",
      "Epoch: 633 Train Loss: 5.601516, Acc: 2132.048\n",
      "Val Loss: 20.475531, Acc: 2778.511\n",
      "2778.5111328125\n",
      "Epoch: 634 Train Loss: 5.564069, Acc: 2234.330\n",
      "Val Loss: 21.690497, Acc: 3029.929\n",
      "3029.92900390625\n",
      "Epoch: 635 Train Loss: 5.683711, Acc: 2456.785\n",
      "Val Loss: 21.333858, Acc: 3049.556\n",
      "3049.55625\n",
      "Epoch: 636 Train Loss: 6.801629, Acc: 3037.534\n",
      "Val Loss: 20.313193, Acc: 3042.238\n",
      "3042.2375\n",
      "Epoch: 637 Train Loss: 6.992225, Acc: 3858.642\n",
      "Val Loss: 22.595949, Acc: 4379.672\n",
      "4379.6724609375\n",
      "Epoch: 638 Train Loss: 8.494240, Acc: 3257.966\n",
      "Val Loss: 25.729842, Acc: 5992.466\n",
      "5992.4662109375\n",
      "Epoch: 639 Train Loss: 7.730635, Acc: 3583.101\n",
      "Val Loss: 21.050590, Acc: 4603.103\n",
      "4603.1029296875\n",
      "Epoch: 640 Train Loss: 5.936740, Acc: 2968.884\n",
      "Val Loss: 23.633894, Acc: 4602.400\n",
      "4602.4\n",
      "Epoch: 641 Train Loss: 5.942358, Acc: 2693.209\n",
      "Val Loss: 19.992355, Acc: 2908.894\n",
      "2908.89365234375\n",
      "Epoch: 642 Train Loss: 5.966709, Acc: 2549.762\n",
      "Val Loss: 21.735894, Acc: 2805.467\n",
      "2805.46728515625\n",
      "Epoch: 643 Train Loss: 5.401515, Acc: 2109.921\n",
      "Val Loss: 21.226988, Acc: 2855.128\n",
      "2855.12783203125\n",
      "Epoch: 644 Train Loss: 6.128038, Acc: 2289.529\n",
      "Val Loss: 21.468855, Acc: 2982.594\n",
      "2982.5935546875\n",
      "Epoch: 645 Train Loss: 6.653045, Acc: 3094.534\n",
      "Val Loss: 23.540936, Acc: 4335.145\n",
      "4335.1451171875\n",
      "Epoch: 646 Train Loss: 8.148719, Acc: 3461.488\n",
      "Val Loss: 23.198043, Acc: 6348.716\n",
      "6348.715625\n",
      "Epoch: 647 Train Loss: 9.092519, Acc: 4431.646\n",
      "Val Loss: 23.891354, Acc: 6402.007\n",
      "6402.0068359375\n",
      "Epoch: 648 Train Loss: 8.562379, Acc: 4121.388\n",
      "Val Loss: 26.452724, Acc: 6227.473\n",
      "6227.47265625\n",
      "Epoch: 649 Train Loss: 12.309391, Acc: 5187.249\n",
      "Val Loss: 25.985321, Acc: 6436.788\n",
      "6436.787890625\n",
      "Epoch: 650 Train Loss: 10.232605, Acc: 5253.036\n",
      "Val Loss: 22.168174, Acc: 4871.009\n",
      "4871.008984375\n",
      "Epoch: 651 Train Loss: 10.837557, Acc: 4298.939\n",
      "Val Loss: 22.944806, Acc: 5659.196\n",
      "5659.1962890625\n",
      "Epoch: 652 Train Loss: 9.502685, Acc: 4796.137\n",
      "Val Loss: 22.597178, Acc: 3664.638\n",
      "3664.63828125\n",
      "Epoch: 653 Train Loss: 8.500339, Acc: 3585.926\n",
      "Val Loss: 22.331693, Acc: 4300.726\n",
      "4300.7263671875\n",
      "Epoch: 654 Train Loss: 7.537903, Acc: 3853.879\n",
      "Val Loss: 20.953357, Acc: 3561.639\n",
      "3561.6390625\n",
      "Epoch: 655 Train Loss: 5.912757, Acc: 3115.180\n",
      "Val Loss: 20.599395, Acc: 3592.694\n",
      "3592.6943359375\n",
      "Epoch: 656 Train Loss: 5.440897, Acc: 2520.377\n",
      "Val Loss: 21.820670, Acc: 3570.862\n",
      "3570.862109375\n",
      "Epoch: 657 Train Loss: 7.699931, Acc: 3708.145\n",
      "Val Loss: 23.384767, Acc: 3548.579\n",
      "3548.5791015625\n",
      "Epoch: 658 Train Loss: 8.728440, Acc: 4262.041\n",
      "Val Loss: 23.077753, Acc: 2991.502\n",
      "2991.5017578125\n",
      "Epoch: 659 Train Loss: 7.050373, Acc: 2698.898\n",
      "Val Loss: 21.448519, Acc: 4974.754\n",
      "4974.7541015625\n",
      "Epoch: 660 Train Loss: 8.416912, Acc: 4415.061\n",
      "Val Loss: 24.414069, Acc: 4228.017\n",
      "4228.016796875\n",
      "Epoch: 661 Train Loss: 6.674535, Acc: 2878.027\n",
      "Val Loss: 22.177283, Acc: 2909.188\n",
      "2909.1884765625\n",
      "Epoch: 662 Train Loss: 5.677952, Acc: 2718.261\n",
      "Val Loss: 22.067988, Acc: 4200.893\n",
      "4200.893359375\n",
      "Epoch: 663 Train Loss: 6.061949, Acc: 3094.677\n",
      "Val Loss: 21.556378, Acc: 4208.801\n",
      "4208.801171875\n",
      "Epoch: 664 Train Loss: 5.546796, Acc: 2383.671\n",
      "Val Loss: 20.474287, Acc: 3422.830\n",
      "3422.83046875\n",
      "Epoch: 665 Train Loss: 7.461071, Acc: 3813.364\n",
      "Val Loss: 22.270822, Acc: 3031.958\n",
      "3031.9583984375\n",
      "Epoch: 666 Train Loss: 7.163495, Acc: 3534.024\n",
      "Val Loss: 24.584660, Acc: 3185.509\n",
      "3185.50947265625\n",
      "Epoch: 667 Train Loss: 6.733434, Acc: 2984.493\n",
      "Val Loss: 22.306961, Acc: 5708.471\n",
      "5708.47109375\n",
      "Epoch: 668 Train Loss: 5.698304, Acc: 3137.569\n",
      "Val Loss: 23.061671, Acc: 3378.318\n",
      "3378.3181640625\n",
      "Epoch: 669 Train Loss: 5.829844, Acc: 2317.257\n",
      "Val Loss: 21.098256, Acc: 4947.229\n",
      "4947.2294921875\n",
      "Epoch: 670 Train Loss: 5.950901, Acc: 2835.010\n",
      "Val Loss: 25.358040, Acc: 3308.231\n",
      "3308.23125\n",
      "Epoch: 671 Train Loss: 5.735723, Acc: 2632.116\n",
      "Val Loss: 20.998238, Acc: 3616.143\n",
      "3616.143359375\n",
      "Epoch: 672 Train Loss: 5.598956, Acc: 2293.216\n",
      "Val Loss: 20.585348, Acc: 3147.489\n",
      "3147.488671875\n",
      "Epoch: 673 Train Loss: 5.697738, Acc: 2650.525\n",
      "Val Loss: 20.955224, Acc: 3457.598\n",
      "3457.5978515625\n",
      "Epoch: 674 Train Loss: 5.270885, Acc: 2296.439\n",
      "Val Loss: 20.125815, Acc: 2862.453\n",
      "2862.4533203125\n",
      "Epoch: 675 Train Loss: 5.268446, Acc: 2262.617\n",
      "Val Loss: 21.122504, Acc: 2785.446\n",
      "2785.446484375\n",
      "Epoch: 676 Train Loss: 5.909484, Acc: 2550.303\n",
      "Val Loss: 23.259924, Acc: 3047.915\n",
      "3047.91494140625\n",
      "Epoch: 677 Train Loss: 6.102422, Acc: 3191.104\n",
      "Val Loss: 22.253881, Acc: 2834.743\n",
      "2834.74267578125\n",
      "Epoch: 678 Train Loss: 5.499473, Acc: 2132.721\n",
      "Val Loss: 19.775838, Acc: 3752.182\n",
      "3752.182421875\n",
      "Epoch: 679 Train Loss: 5.529849, Acc: 2563.931\n",
      "Val Loss: 20.346748, Acc: 2935.447\n",
      "2935.4466796875\n",
      "Epoch: 680 Train Loss: 5.761670, Acc: 2449.236\n",
      "Val Loss: 21.207246, Acc: 2892.007\n",
      "2892.0068359375\n",
      "Epoch: 681 Train Loss: 5.443859, Acc: 2503.893\n",
      "Val Loss: 22.065703, Acc: 3730.831\n",
      "3730.8314453125\n",
      "Epoch: 682 Train Loss: 5.808100, Acc: 2491.871\n",
      "Val Loss: 20.520766, Acc: 2781.535\n",
      "2781.534765625\n",
      "Epoch: 683 Train Loss: 5.903791, Acc: 2878.124\n",
      "Val Loss: 20.637859, Acc: 2700.810\n",
      "2700.81005859375\n",
      "2700.81005859375 2700.81005859375\n",
      "Epoch: 684 Train Loss: 5.086993, Acc: 2427.963\n",
      "Val Loss: 20.443208, Acc: 2728.401\n",
      "2728.4005859375\n",
      "Epoch: 685 Train Loss: 7.407795, Acc: 3434.810\n",
      "Val Loss: 21.727997, Acc: 4825.127\n",
      "4825.12734375\n",
      "Epoch: 686 Train Loss: 7.174053, Acc: 4264.519\n",
      "Val Loss: 25.473276, Acc: 4249.106\n",
      "4249.105859375\n",
      "Epoch: 687 Train Loss: 7.104325, Acc: 3411.298\n",
      "Val Loss: 21.180395, Acc: 4508.288\n",
      "4508.28828125\n",
      "Epoch: 688 Train Loss: 6.287602, Acc: 3380.192\n",
      "Val Loss: 19.850631, Acc: 2848.528\n",
      "2848.527734375\n",
      "Epoch: 689 Train Loss: 5.520146, Acc: 2315.512\n",
      "Val Loss: 20.463404, Acc: 2806.788\n",
      "2806.78837890625\n",
      "Epoch: 690 Train Loss: 5.213266, Acc: 2289.310\n",
      "Val Loss: 20.261304, Acc: 2831.158\n",
      "2831.1578125\n",
      "Epoch: 691 Train Loss: 5.066828, Acc: 1949.036\n",
      "Val Loss: 20.433038, Acc: 3244.976\n",
      "3244.9759765625\n",
      "Epoch: 692 Train Loss: 5.737140, Acc: 2838.555\n",
      "Val Loss: 20.726002, Acc: 3331.650\n",
      "3331.6498046875\n",
      "Epoch: 693 Train Loss: 5.265277, Acc: 2439.723\n",
      "Val Loss: 26.053449, Acc: 4710.697\n",
      "4710.697265625\n",
      "Epoch: 694 Train Loss: 5.603620, Acc: 2595.156\n",
      "Val Loss: 21.555572, Acc: 3451.923\n",
      "3451.9228515625\n",
      "Epoch: 695 Train Loss: 5.229771, Acc: 2193.695\n",
      "Val Loss: 20.596543, Acc: 3020.563\n",
      "3020.5634765625\n",
      "Epoch: 696 Train Loss: 7.826373, Acc: 3618.075\n",
      "Val Loss: 22.305093, Acc: 4801.103\n",
      "4801.1025390625\n",
      "Epoch: 697 Train Loss: 9.505615, Acc: 5018.875\n",
      "Val Loss: 20.148189, Acc: 2697.940\n",
      "2697.94033203125\n",
      "2697.94033203125 2697.94033203125\n",
      "Epoch: 698 Train Loss: 5.573160, Acc: 2587.974\n",
      "Val Loss: 21.947873, Acc: 4315.520\n",
      "4315.51953125\n",
      "Epoch: 699 Train Loss: 5.672328, Acc: 2737.881\n",
      "Val Loss: 20.197626, Acc: 2665.762\n",
      "2665.76162109375\n",
      "2665.76162109375 2665.76162109375\n",
      "Epoch: 700 Train Loss: 6.413653, Acc: 2626.251\n",
      "Val Loss: 19.273641, Acc: 2776.148\n",
      "2776.1478515625\n",
      "Epoch: 701 Train Loss: 5.423348, Acc: 2725.451\n",
      "Val Loss: 22.550384, Acc: 3811.077\n",
      "3811.0765625\n",
      "Epoch: 702 Train Loss: 5.377049, Acc: 2742.149\n",
      "Val Loss: 21.351578, Acc: 3159.361\n",
      "3159.36123046875\n",
      "Epoch: 703 Train Loss: 5.188418, Acc: 2113.101\n",
      "Val Loss: 20.961477, Acc: 3742.582\n",
      "3742.5822265625\n",
      "Epoch: 704 Train Loss: 5.208621, Acc: 2211.898\n",
      "Val Loss: 20.539295, Acc: 3064.549\n",
      "3064.5490234375\n",
      "Epoch: 705 Train Loss: 5.638828, Acc: 2574.277\n",
      "Val Loss: 21.639219, Acc: 3011.822\n",
      "3011.82197265625\n",
      "Epoch: 706 Train Loss: 5.303085, Acc: 2216.171\n",
      "Val Loss: 20.440047, Acc: 2858.334\n",
      "2858.33447265625\n",
      "Epoch: 707 Train Loss: 6.177483, Acc: 2571.984\n",
      "Val Loss: 22.431499, Acc: 3071.582\n",
      "3071.58173828125\n",
      "Epoch: 708 Train Loss: 5.789708, Acc: 2810.644\n",
      "Val Loss: 22.547837, Acc: 4479.010\n",
      "4479.0099609375\n",
      "Epoch: 709 Train Loss: 8.337054, Acc: 4432.469\n",
      "Val Loss: 24.333636, Acc: 7574.787\n",
      "7574.78671875\n",
      "Epoch: 710 Train Loss: 8.152015, Acc: 4233.956\n",
      "Val Loss: 24.030828, Acc: 4765.810\n",
      "4765.8095703125\n",
      "Epoch: 711 Train Loss: 6.586718, Acc: 2929.978\n",
      "Val Loss: 21.370088, Acc: 3521.789\n",
      "3521.788671875\n",
      "Epoch: 712 Train Loss: 5.476896, Acc: 2762.753\n",
      "Val Loss: 23.439694, Acc: 3175.341\n",
      "3175.34072265625\n",
      "Epoch: 713 Train Loss: 7.761971, Acc: 4155.900\n",
      "Val Loss: 21.573076, Acc: 4539.988\n",
      "4539.987890625\n",
      "Epoch: 714 Train Loss: 7.897385, Acc: 3775.832\n",
      "Val Loss: 20.377653, Acc: 4436.490\n",
      "4436.4900390625\n",
      "Epoch: 715 Train Loss: 6.372462, Acc: 3363.051\n",
      "Val Loss: 22.937923, Acc: 5261.362\n",
      "5261.362109375\n",
      "Epoch: 716 Train Loss: 6.354892, Acc: 2720.474\n",
      "Val Loss: 22.243553, Acc: 2741.668\n",
      "2741.668359375\n",
      "Epoch: 717 Train Loss: 5.191861, Acc: 2381.585\n",
      "Val Loss: 22.694740, Acc: 2958.261\n",
      "2958.2609375\n",
      "Epoch: 718 Train Loss: 5.967040, Acc: 2655.952\n",
      "Val Loss: 22.301419, Acc: 6214.440\n",
      "6214.4396484375\n",
      "Epoch: 719 Train Loss: 6.242101, Acc: 3499.395\n",
      "Val Loss: 19.718840, Acc: 2675.809\n",
      "2675.808984375\n",
      "Epoch: 720 Train Loss: 5.932544, Acc: 3181.589\n",
      "Val Loss: 22.129290, Acc: 2722.129\n",
      "2722.12939453125\n",
      "Epoch: 721 Train Loss: 5.745531, Acc: 2214.585\n",
      "Val Loss: 21.571939, Acc: 2934.177\n",
      "2934.17666015625\n",
      "Epoch: 722 Train Loss: 4.969142, Acc: 2423.667\n",
      "Val Loss: 20.246084, Acc: 2673.739\n",
      "2673.73935546875\n",
      "Epoch: 723 Train Loss: 4.750612, Acc: 2067.365\n",
      "Val Loss: 21.104942, Acc: 3209.035\n",
      "3209.03525390625\n",
      "Epoch: 724 Train Loss: 4.865273, Acc: 2123.991\n",
      "Val Loss: 20.495518, Acc: 2629.547\n",
      "2629.54736328125\n",
      "2629.54736328125 2629.54736328125\n",
      "Epoch: 725 Train Loss: 5.021063, Acc: 2128.165\n",
      "Val Loss: 22.584313, Acc: 4886.643\n",
      "4886.642578125\n",
      "Epoch: 726 Train Loss: 6.401932, Acc: 3369.989\n",
      "Val Loss: 22.568158, Acc: 4134.801\n",
      "4134.8013671875\n",
      "Epoch: 727 Train Loss: 8.420131, Acc: 4405.376\n",
      "Val Loss: 21.123298, Acc: 3434.144\n",
      "3434.14375\n",
      "Epoch: 728 Train Loss: 5.836527, Acc: 3401.544\n",
      "Val Loss: 20.291476, Acc: 3459.117\n",
      "3459.1173828125\n",
      "Epoch: 729 Train Loss: 5.145949, Acc: 2514.627\n",
      "Val Loss: 21.195498, Acc: 2913.587\n",
      "2913.58671875\n",
      "Epoch: 730 Train Loss: 4.610185, Acc: 1897.997\n",
      "Val Loss: 20.739148, Acc: 2607.230\n",
      "2607.230078125\n",
      "2607.230078125 2607.230078125\n",
      "Epoch: 731 Train Loss: 4.853657, Acc: 1935.462\n",
      "Val Loss: 20.812272, Acc: 2817.333\n",
      "2817.33310546875\n",
      "Epoch: 732 Train Loss: 4.595035, Acc: 1977.185\n",
      "Val Loss: 20.195554, Acc: 3033.835\n",
      "3033.8353515625\n",
      "Epoch: 733 Train Loss: 5.154862, Acc: 2458.550\n",
      "Val Loss: 22.197293, Acc: 3003.565\n",
      "3003.5650390625\n",
      "Epoch: 734 Train Loss: 10.533267, Acc: 4157.125\n",
      "Val Loss: 21.273836, Acc: 2892.862\n",
      "2892.86162109375\n",
      "Epoch: 735 Train Loss: 7.949510, Acc: 4090.387\n",
      "Val Loss: 20.606262, Acc: 3600.640\n",
      "3600.6404296875\n",
      "Epoch: 736 Train Loss: 5.403099, Acc: 2488.743\n",
      "Val Loss: 20.053669, Acc: 2809.640\n",
      "2809.63984375\n",
      "Epoch: 737 Train Loss: 5.989315, Acc: 2934.031\n",
      "Val Loss: 20.801698, Acc: 2556.702\n",
      "2556.701953125\n",
      "2556.701953125 2556.701953125\n",
      "Epoch: 738 Train Loss: 5.441721, Acc: 2464.658\n",
      "Val Loss: 26.068759, Acc: 4792.701\n",
      "4792.7005859375\n",
      "Epoch: 739 Train Loss: 7.510037, Acc: 3925.390\n",
      "Val Loss: 21.699229, Acc: 4689.278\n",
      "4689.2783203125\n",
      "Epoch: 740 Train Loss: 6.852459, Acc: 3305.032\n",
      "Val Loss: 24.232858, Acc: 4257.614\n",
      "4257.613671875\n",
      "Epoch: 741 Train Loss: 6.665682, Acc: 3436.840\n",
      "Val Loss: 21.700310, Acc: 3998.177\n",
      "3998.1771484375\n",
      "Epoch: 742 Train Loss: 5.200198, Acc: 2264.927\n",
      "Val Loss: 21.173052, Acc: 2612.963\n",
      "2612.96318359375\n",
      "Epoch: 743 Train Loss: 4.931721, Acc: 2168.418\n",
      "Val Loss: 22.456079, Acc: 3305.277\n",
      "3305.2771484375\n",
      "Epoch: 744 Train Loss: 4.768044, Acc: 2257.758\n",
      "Val Loss: 20.093721, Acc: 2803.905\n",
      "2803.90498046875\n",
      "Epoch: 745 Train Loss: 4.668697, Acc: 2113.151\n",
      "Val Loss: 20.422429, Acc: 2657.547\n",
      "2657.5474609375\n",
      "Epoch: 746 Train Loss: 5.500408, Acc: 2726.991\n",
      "Val Loss: 22.040317, Acc: 4306.241\n",
      "4306.241015625\n",
      "Epoch: 747 Train Loss: 5.525338, Acc: 2628.878\n",
      "Val Loss: 21.938748, Acc: 4106.146\n",
      "4106.14609375\n",
      "Epoch: 748 Train Loss: 5.382448, Acc: 2784.355\n",
      "Val Loss: 21.046595, Acc: 2723.348\n",
      "2723.3484375\n",
      "Epoch: 749 Train Loss: 5.240512, Acc: 2349.730\n",
      "Val Loss: 22.238148, Acc: 5234.597\n",
      "5234.5966796875\n",
      "Epoch: 750 Train Loss: 4.970986, Acc: 2985.417\n",
      "Val Loss: 26.653570, Acc: 5437.866\n",
      "5437.865625\n",
      "Epoch: 751 Train Loss: 9.188019, Acc: 4129.773\n",
      "Val Loss: 20.953164, Acc: 2755.066\n",
      "2755.06640625\n",
      "Epoch: 752 Train Loss: 6.789159, Acc: 2861.782\n",
      "Val Loss: 19.919264, Acc: 2976.371\n",
      "2976.37138671875\n",
      "Epoch: 753 Train Loss: 5.257273, Acc: 2253.576\n",
      "Val Loss: 20.448740, Acc: 2774.653\n",
      "2774.65322265625\n",
      "Epoch: 754 Train Loss: 5.107805, Acc: 2084.137\n",
      "Val Loss: 21.845304, Acc: 2923.848\n",
      "2923.8482421875\n",
      "Epoch: 755 Train Loss: 6.074484, Acc: 3054.474\n",
      "Val Loss: 24.852734, Acc: 5410.624\n",
      "5410.6240234375\n",
      "Epoch: 756 Train Loss: 7.029055, Acc: 3441.723\n",
      "Val Loss: 19.871574, Acc: 2640.469\n",
      "2640.469140625\n",
      "Epoch: 757 Train Loss: 7.081743, Acc: 4070.320\n",
      "Val Loss: 23.420959, Acc: 5997.878\n",
      "5997.8779296875\n",
      "Epoch: 758 Train Loss: 5.834224, Acc: 3089.244\n",
      "Val Loss: 23.218181, Acc: 3137.331\n",
      "3137.330859375\n",
      "Epoch: 759 Train Loss: 6.405360, Acc: 3343.461\n",
      "Val Loss: 21.520559, Acc: 2723.225\n",
      "2723.225390625\n",
      "Epoch: 760 Train Loss: 6.827922, Acc: 3260.322\n",
      "Val Loss: 21.797771, Acc: 3657.523\n",
      "3657.523046875\n",
      "Epoch: 761 Train Loss: 6.774220, Acc: 3151.481\n",
      "Val Loss: 21.263831, Acc: 4032.303\n",
      "4032.303125\n",
      "Epoch: 762 Train Loss: 5.065043, Acc: 2398.199\n",
      "Val Loss: 21.159058, Acc: 2590.085\n",
      "2590.08466796875\n",
      "Epoch: 763 Train Loss: 5.126972, Acc: 2309.990\n",
      "Val Loss: 21.736518, Acc: 2773.127\n",
      "2773.1267578125\n",
      "Epoch: 764 Train Loss: 5.036036, Acc: 2383.075\n",
      "Val Loss: 21.039294, Acc: 3037.634\n",
      "3037.63427734375\n",
      "Epoch: 765 Train Loss: 5.893333, Acc: 2492.243\n",
      "Val Loss: 22.050402, Acc: 5624.581\n",
      "5624.5806640625\n",
      "Epoch: 766 Train Loss: 6.900141, Acc: 4325.551\n",
      "Val Loss: 23.239137, Acc: 3771.964\n",
      "3771.964453125\n",
      "Epoch: 767 Train Loss: 7.216212, Acc: 3689.166\n",
      "Val Loss: 37.283345, Acc: 7188.116\n",
      "7188.11640625\n",
      "Epoch: 768 Train Loss: 12.008352, Acc: 4915.936\n",
      "Val Loss: 21.578473, Acc: 2744.514\n",
      "2744.513671875\n",
      "Epoch: 769 Train Loss: 6.144318, Acc: 3094.407\n",
      "Val Loss: 23.518787, Acc: 5038.528\n",
      "5038.527734375\n",
      "Epoch: 770 Train Loss: 6.139277, Acc: 3532.512\n",
      "Val Loss: 24.041985, Acc: 5602.423\n",
      "5602.423046875\n",
      "Epoch: 771 Train Loss: 5.968533, Acc: 3453.759\n",
      "Val Loss: 20.839802, Acc: 4378.984\n",
      "4378.9837890625\n",
      "Epoch: 772 Train Loss: 7.370898, Acc: 3456.343\n",
      "Val Loss: 21.681660, Acc: 2819.063\n",
      "2819.06259765625\n",
      "Epoch: 773 Train Loss: 9.643727, Acc: 4078.705\n",
      "Val Loss: 28.025588, Acc: 5452.695\n",
      "5452.6953125\n",
      "Epoch: 774 Train Loss: 7.585759, Acc: 3092.541\n",
      "Val Loss: 23.023649, Acc: 3895.254\n",
      "3895.2537109375\n",
      "Epoch: 775 Train Loss: 8.031045, Acc: 3505.539\n",
      "Val Loss: 20.759297, Acc: 4078.050\n",
      "4078.0501953125\n",
      "Epoch: 776 Train Loss: 6.165345, Acc: 3001.184\n",
      "Val Loss: 21.010749, Acc: 3880.364\n",
      "3880.3642578125\n",
      "Epoch: 777 Train Loss: 5.146062, Acc: 2829.092\n",
      "Val Loss: 21.719699, Acc: 3483.535\n",
      "3483.5353515625\n",
      "Epoch: 778 Train Loss: 5.378350, Acc: 2659.073\n",
      "Val Loss: 20.851655, Acc: 4443.223\n",
      "4443.2234375\n",
      "Epoch: 779 Train Loss: 4.997631, Acc: 2523.793\n",
      "Val Loss: 19.791062, Acc: 2560.823\n",
      "2560.8234375\n",
      "Epoch: 780 Train Loss: 4.737639, Acc: 2139.549\n",
      "Val Loss: 20.414114, Acc: 2967.448\n",
      "2967.44794921875\n",
      "Epoch: 781 Train Loss: 4.567074, Acc: 2043.123\n",
      "Val Loss: 20.026465, Acc: 2499.447\n",
      "2499.44697265625\n",
      "2499.44697265625 2499.44697265625\n",
      "Epoch: 782 Train Loss: 4.398129, Acc: 1996.668\n",
      "Val Loss: 20.328689, Acc: 3768.853\n",
      "3768.8529296875\n",
      "Epoch: 783 Train Loss: 5.216551, Acc: 2856.593\n",
      "Val Loss: 24.182833, Acc: 6154.294\n",
      "6154.2943359375\n",
      "Epoch: 784 Train Loss: 5.294163, Acc: 3303.673\n",
      "Val Loss: 20.124314, Acc: 2880.704\n",
      "2880.7044921875\n",
      "Epoch: 785 Train Loss: 4.884368, Acc: 2216.358\n",
      "Val Loss: 20.055177, Acc: 3503.151\n",
      "3503.151171875\n",
      "Epoch: 786 Train Loss: 4.574481, Acc: 1996.840\n",
      "Val Loss: 19.729404, Acc: 2431.766\n",
      "2431.76630859375\n",
      "2431.76630859375 2431.76630859375\n",
      "Epoch: 787 Train Loss: 5.124591, Acc: 2157.161\n",
      "Val Loss: 21.785192, Acc: 4450.300\n",
      "4450.3\n",
      "Epoch: 788 Train Loss: 5.087075, Acc: 2911.538\n",
      "Val Loss: 22.990035, Acc: 5760.492\n",
      "5760.491796875\n",
      "Epoch: 789 Train Loss: 7.831237, Acc: 2959.405\n",
      "Val Loss: 27.629377, Acc: 5928.095\n",
      "5928.0953125\n",
      "Epoch: 790 Train Loss: 9.582302, Acc: 4601.704\n",
      "Val Loss: 22.845544, Acc: 5529.922\n",
      "5529.9220703125\n",
      "Epoch: 791 Train Loss: 8.069659, Acc: 4028.302\n",
      "Val Loss: 25.884517, Acc: 5203.669\n",
      "5203.669140625\n",
      "Epoch: 792 Train Loss: 8.828577, Acc: 4272.497\n",
      "Val Loss: 26.059584, Acc: 6962.022\n",
      "6962.022265625\n",
      "Epoch: 793 Train Loss: 7.567913, Acc: 3506.173\n",
      "Val Loss: 24.210960, Acc: 2964.666\n",
      "2964.66572265625\n",
      "Epoch: 794 Train Loss: 7.038051, Acc: 3599.139\n",
      "Val Loss: 25.904610, Acc: 5465.657\n",
      "5465.657421875\n",
      "Epoch: 795 Train Loss: 11.759733, Acc: 5250.546\n",
      "Val Loss: 23.460921, Acc: 7215.011\n",
      "7215.0109375\n",
      "Epoch: 796 Train Loss: 8.821092, Acc: 3501.300\n",
      "Val Loss: 22.550579, Acc: 4634.682\n",
      "4634.681640625\n",
      "Epoch: 797 Train Loss: 8.125564, Acc: 4316.310\n",
      "Val Loss: 28.638413, Acc: 7423.454\n",
      "7423.454296875\n",
      "Epoch: 798 Train Loss: 7.102752, Acc: 3872.079\n",
      "Val Loss: 23.689792, Acc: 6608.249\n",
      "6608.24921875\n",
      "Epoch: 799 Train Loss: 7.419387, Acc: 4068.645\n",
      "Val Loss: 21.998134, Acc: 5749.031\n",
      "5749.030859375\n",
      "Epoch: 800 Train Loss: 5.803245, Acc: 3062.935\n",
      "Val Loss: 19.794259, Acc: 3314.969\n",
      "3314.96943359375\n",
      "Epoch: 801 Train Loss: 7.085044, Acc: 3486.644\n",
      "Val Loss: 22.588264, Acc: 4036.897\n",
      "4036.8974609375\n",
      "Epoch: 802 Train Loss: 5.504460, Acc: 2584.968\n",
      "Val Loss: 19.811104, Acc: 2535.670\n",
      "2535.6703125\n",
      "Epoch: 803 Train Loss: 4.443577, Acc: 1935.033\n",
      "Val Loss: 20.662066, Acc: 2631.215\n",
      "2631.21533203125\n",
      "Epoch: 804 Train Loss: 5.128043, Acc: 2351.621\n",
      "Val Loss: 21.641933, Acc: 2826.757\n",
      "2826.75732421875\n",
      "Epoch: 805 Train Loss: 5.434307, Acc: 2329.557\n",
      "Val Loss: 22.531566, Acc: 3229.074\n",
      "3229.07421875\n",
      "Epoch: 806 Train Loss: 7.934724, Acc: 3774.357\n",
      "Val Loss: 21.411933, Acc: 4265.163\n",
      "4265.1625\n",
      "Epoch: 807 Train Loss: 5.633322, Acc: 3345.843\n",
      "Val Loss: 21.719606, Acc: 2887.910\n",
      "2887.9103515625\n",
      "Epoch: 808 Train Loss: 5.623945, Acc: 2308.759\n",
      "Val Loss: 21.124557, Acc: 2559.408\n",
      "2559.40810546875\n",
      "Epoch: 809 Train Loss: 5.020460, Acc: 2604.278\n",
      "Val Loss: 19.515832, Acc: 2696.203\n",
      "2696.2033203125\n",
      "Epoch: 810 Train Loss: 5.223630, Acc: 2317.954\n",
      "Val Loss: 20.937404, Acc: 2646.189\n",
      "2646.189453125\n",
      "Epoch: 811 Train Loss: 5.608512, Acc: 2825.338\n",
      "Val Loss: 20.766278, Acc: 3780.256\n",
      "3780.25625\n",
      "Epoch: 812 Train Loss: 4.811195, Acc: 2378.940\n",
      "Val Loss: 20.196371, Acc: 2631.036\n",
      "2631.0357421875\n",
      "Epoch: 813 Train Loss: 4.394240, Acc: 1942.191\n",
      "Val Loss: 20.988056, Acc: 3057.559\n",
      "3057.5587890625\n",
      "Epoch: 814 Train Loss: 4.917786, Acc: 2469.431\n",
      "Val Loss: 20.209667, Acc: 2878.862\n",
      "2878.86201171875\n",
      "Epoch: 815 Train Loss: 5.107721, Acc: 2159.448\n",
      "Val Loss: 20.383183, Acc: 3742.798\n",
      "3742.7984375\n",
      "Epoch: 816 Train Loss: 6.205507, Acc: 3256.846\n",
      "Val Loss: 21.271796, Acc: 3072.341\n",
      "3072.341015625\n",
      "Epoch: 817 Train Loss: 6.891061, Acc: 2981.929\n",
      "Val Loss: 20.100343, Acc: 4443.618\n",
      "4443.61796875\n",
      "Epoch: 818 Train Loss: 5.896380, Acc: 3693.461\n",
      "Val Loss: 18.944262, Acc: 3786.495\n",
      "3786.49453125\n",
      "Epoch: 819 Train Loss: 7.237309, Acc: 2855.850\n",
      "Val Loss: 27.060673, Acc: 3539.085\n",
      "3539.084765625\n",
      "Epoch: 820 Train Loss: 9.899297, Acc: 4966.238\n",
      "Val Loss: 21.604994, Acc: 3273.379\n",
      "3273.3791015625\n",
      "Epoch: 821 Train Loss: 10.521094, Acc: 4136.155\n",
      "Val Loss: 25.874329, Acc: 6274.225\n",
      "6274.2248046875\n",
      "Epoch: 822 Train Loss: 8.605123, Acc: 3816.569\n",
      "Val Loss: 22.621601, Acc: 5403.267\n",
      "5403.2669921875\n",
      "Epoch: 823 Train Loss: 5.568058, Acc: 2726.265\n",
      "Val Loss: 18.733647, Acc: 2942.485\n",
      "2942.48525390625\n",
      "Epoch: 824 Train Loss: 5.023908, Acc: 2256.785\n",
      "Val Loss: 20.265387, Acc: 2932.545\n",
      "2932.5453125\n",
      "Epoch: 825 Train Loss: 4.487343, Acc: 1989.098\n",
      "Val Loss: 20.078659, Acc: 3863.214\n",
      "3863.214453125\n",
      "Epoch: 826 Train Loss: 4.763095, Acc: 2433.495\n",
      "Val Loss: 19.216106, Acc: 3114.550\n",
      "3114.550390625\n",
      "Epoch: 827 Train Loss: 5.817231, Acc: 2380.029\n",
      "Val Loss: 20.211576, Acc: 4184.530\n",
      "4184.5298828125\n",
      "Epoch: 828 Train Loss: 5.923236, Acc: 3422.310\n",
      "Val Loss: 20.825274, Acc: 3222.236\n",
      "3222.23564453125\n",
      "Epoch: 829 Train Loss: 4.918857, Acc: 2143.645\n",
      "Val Loss: 19.912900, Acc: 3077.363\n",
      "3077.36318359375\n",
      "Epoch: 830 Train Loss: 4.524955, Acc: 2275.816\n",
      "Val Loss: 19.788855, Acc: 3379.932\n",
      "3379.931640625\n",
      "Epoch: 831 Train Loss: 4.998983, Acc: 2549.355\n",
      "Val Loss: 21.803382, Acc: 2629.096\n",
      "2629.09580078125\n",
      "Epoch: 832 Train Loss: 4.539812, Acc: 1994.058\n",
      "Val Loss: 19.555262, Acc: 2412.069\n",
      "2412.06943359375\n",
      "2412.06943359375 2412.06943359375\n",
      "Epoch: 833 Train Loss: 4.489776, Acc: 1887.780\n",
      "Val Loss: 19.038424, Acc: 2924.443\n",
      "2924.4427734375\n",
      "Epoch: 834 Train Loss: 4.786244, Acc: 1978.142\n",
      "Val Loss: 19.467419, Acc: 2641.631\n",
      "2641.63115234375\n",
      "Epoch: 835 Train Loss: 4.963867, Acc: 1960.492\n",
      "Val Loss: 19.767877, Acc: 2851.611\n",
      "2851.61142578125\n",
      "Epoch: 836 Train Loss: 4.985070, Acc: 2565.025\n",
      "Val Loss: 20.705270, Acc: 3496.179\n",
      "3496.1791015625\n",
      "Epoch: 837 Train Loss: 4.897615, Acc: 3003.587\n",
      "Val Loss: 20.021297, Acc: 2467.394\n",
      "2467.39384765625\n",
      "Epoch: 838 Train Loss: 4.596511, Acc: 1821.772\n",
      "Val Loss: 21.342292, Acc: 3227.657\n",
      "3227.656640625\n",
      "Epoch: 839 Train Loss: 5.068965, Acc: 2616.242\n",
      "Val Loss: 22.055840, Acc: 4980.693\n",
      "4980.6927734375\n",
      "Epoch: 840 Train Loss: 6.425908, Acc: 3364.485\n",
      "Val Loss: 24.085625, Acc: 5977.607\n",
      "5977.607421875\n",
      "Epoch: 841 Train Loss: 5.764554, Acc: 3598.235\n",
      "Val Loss: 19.984065, Acc: 3044.847\n",
      "3044.84697265625\n",
      "Epoch: 842 Train Loss: 5.949097, Acc: 2710.003\n",
      "Val Loss: 21.191849, Acc: 2838.360\n",
      "2838.3603515625\n",
      "Epoch: 843 Train Loss: 5.417159, Acc: 3092.776\n",
      "Val Loss: 21.475495, Acc: 4756.422\n",
      "4756.4220703125\n",
      "Epoch: 844 Train Loss: 6.234243, Acc: 3840.607\n",
      "Val Loss: 23.560954, Acc: 4754.240\n",
      "4754.240234375\n",
      "Epoch: 845 Train Loss: 4.799559, Acc: 2268.561\n",
      "Val Loss: 18.788737, Acc: 2383.208\n",
      "2383.20751953125\n",
      "2383.20751953125 2383.20751953125\n",
      "Epoch: 846 Train Loss: 5.026427, Acc: 2441.721\n",
      "Val Loss: 20.248830, Acc: 2867.583\n",
      "2867.583203125\n",
      "Epoch: 847 Train Loss: 6.445285, Acc: 2939.039\n",
      "Val Loss: 23.211795, Acc: 4801.652\n",
      "4801.6517578125\n",
      "Epoch: 848 Train Loss: 6.159054, Acc: 2805.701\n",
      "Val Loss: 20.916865, Acc: 2708.514\n",
      "2708.5138671875\n",
      "Epoch: 849 Train Loss: 4.787718, Acc: 2555.542\n",
      "Val Loss: 23.194811, Acc: 3197.839\n",
      "3197.838671875\n",
      "Epoch: 850 Train Loss: 6.112783, Acc: 2978.999\n",
      "Val Loss: 23.608173, Acc: 5197.461\n",
      "5197.460546875\n",
      "Epoch: 851 Train Loss: 5.466942, Acc: 3179.167\n",
      "Val Loss: 19.595484, Acc: 3144.780\n",
      "3144.78046875\n",
      "Epoch: 852 Train Loss: 4.713927, Acc: 2651.702\n",
      "Val Loss: 22.611357, Acc: 2876.848\n",
      "2876.84814453125\n",
      "Epoch: 853 Train Loss: 7.529257, Acc: 3142.486\n",
      "Val Loss: 24.140367, Acc: 6127.169\n",
      "6127.1693359375\n",
      "Epoch: 854 Train Loss: 5.704516, Acc: 2781.274\n",
      "Val Loss: 20.321607, Acc: 2601.310\n",
      "2601.30966796875\n",
      "Epoch: 855 Train Loss: 4.380274, Acc: 2007.389\n",
      "Val Loss: 18.211426, Acc: 2951.810\n",
      "2951.81015625\n",
      "Epoch: 856 Train Loss: 4.522163, Acc: 2576.443\n",
      "Val Loss: 21.510614, Acc: 4297.350\n",
      "4297.3498046875\n",
      "Epoch: 857 Train Loss: 5.034352, Acc: 2518.009\n",
      "Val Loss: 20.761291, Acc: 3448.446\n",
      "3448.4462890625\n",
      "Epoch: 858 Train Loss: 4.170762, Acc: 1994.438\n",
      "Val Loss: 19.801614, Acc: 2339.905\n",
      "2339.9048828125\n",
      "2339.9048828125 2339.9048828125\n",
      "Epoch: 859 Train Loss: 5.059084, Acc: 2494.930\n",
      "Val Loss: 20.969079, Acc: 3778.865\n",
      "3778.865234375\n",
      "Epoch: 860 Train Loss: 4.707495, Acc: 2280.351\n",
      "Val Loss: 20.475889, Acc: 2451.286\n",
      "2451.28623046875\n",
      "Epoch: 861 Train Loss: 4.264497, Acc: 1776.880\n",
      "Val Loss: 19.133885, Acc: 2480.085\n",
      "2480.08505859375\n",
      "Epoch: 862 Train Loss: 4.371095, Acc: 1741.352\n",
      "Val Loss: 20.178909, Acc: 2582.869\n",
      "2582.86875\n",
      "Epoch: 863 Train Loss: 4.134503, Acc: 1884.826\n",
      "Val Loss: 20.298115, Acc: 2535.163\n",
      "2535.16337890625\n",
      "Epoch: 864 Train Loss: 5.230637, Acc: 2293.341\n",
      "Val Loss: 18.480356, Acc: 2521.247\n",
      "2521.2474609375\n",
      "Epoch: 865 Train Loss: 5.753426, Acc: 3519.647\n",
      "Val Loss: 22.402474, Acc: 5943.799\n",
      "5943.7994140625\n",
      "Epoch: 866 Train Loss: 5.538892, Acc: 3528.998\n",
      "Val Loss: 20.637465, Acc: 4599.622\n",
      "4599.6216796875\n",
      "Epoch: 867 Train Loss: 5.105026, Acc: 2286.113\n",
      "Val Loss: 18.430509, Acc: 2684.299\n",
      "2684.29873046875\n",
      "Epoch: 868 Train Loss: 4.652814, Acc: 1989.355\n",
      "Val Loss: 20.015009, Acc: 2705.093\n",
      "2705.092578125\n",
      "Epoch: 869 Train Loss: 4.653844, Acc: 2641.267\n",
      "Val Loss: 19.869946, Acc: 3368.993\n",
      "3368.993359375\n",
      "Epoch: 870 Train Loss: 6.741608, Acc: 3054.425\n",
      "Val Loss: 18.638615, Acc: 3443.793\n",
      "3443.79296875\n",
      "Epoch: 871 Train Loss: 7.170428, Acc: 3487.669\n",
      "Val Loss: 21.949651, Acc: 4610.035\n",
      "4610.034765625\n",
      "Epoch: 872 Train Loss: 5.074494, Acc: 2825.507\n",
      "Val Loss: 19.805434, Acc: 2534.363\n",
      "2534.3634765625\n",
      "Epoch: 873 Train Loss: 4.380198, Acc: 1902.805\n",
      "Val Loss: 20.120315, Acc: 2418.628\n",
      "2418.6283203125\n",
      "Epoch: 874 Train Loss: 4.177937, Acc: 1742.238\n",
      "Val Loss: 19.681892, Acc: 2455.827\n",
      "2455.82734375\n",
      "Epoch: 875 Train Loss: 4.733701, Acc: 1907.028\n",
      "Val Loss: 19.723125, Acc: 4073.838\n",
      "4073.8376953125\n",
      "Epoch: 876 Train Loss: 4.681745, Acc: 2681.532\n",
      "Val Loss: 19.197832, Acc: 2319.037\n",
      "2319.03681640625\n",
      "2319.03681640625 2319.03681640625\n",
      "Epoch: 877 Train Loss: 4.205812, Acc: 1783.320\n",
      "Val Loss: 20.342509, Acc: 2974.688\n",
      "2974.687890625\n",
      "Epoch: 878 Train Loss: 4.330770, Acc: 2133.283\n",
      "Val Loss: 19.542862, Acc: 3494.916\n",
      "3494.915625\n",
      "Epoch: 879 Train Loss: 4.336767, Acc: 2099.452\n",
      "Val Loss: 19.904022, Acc: 2517.993\n",
      "2517.99287109375\n",
      "Epoch: 880 Train Loss: 4.296645, Acc: 1668.276\n",
      "Val Loss: 19.212934, Acc: 2314.847\n",
      "2314.847265625\n",
      "2314.847265625 2314.847265625\n",
      "Epoch: 881 Train Loss: 4.347266, Acc: 1927.249\n",
      "Val Loss: 21.198022, Acc: 3512.669\n",
      "3512.6685546875\n",
      "Epoch: 882 Train Loss: 4.196460, Acc: 1804.792\n",
      "Val Loss: 18.788630, Acc: 2283.416\n",
      "2283.41572265625\n",
      "2283.41572265625 2283.41572265625\n",
      "Epoch: 883 Train Loss: 4.577296, Acc: 2328.086\n",
      "Val Loss: 19.363081, Acc: 2313.433\n",
      "2313.4330078125\n",
      "Epoch: 884 Train Loss: 4.100627, Acc: 2218.558\n",
      "Val Loss: 19.212073, Acc: 2789.782\n",
      "2789.78193359375\n",
      "Epoch: 885 Train Loss: 4.086804, Acc: 1951.832\n",
      "Val Loss: 19.447988, Acc: 2636.161\n",
      "2636.1611328125\n",
      "Epoch: 886 Train Loss: 4.747183, Acc: 1971.636\n",
      "Val Loss: 20.305969, Acc: 3627.098\n",
      "3627.0982421875\n",
      "Epoch: 887 Train Loss: 5.018097, Acc: 2556.176\n",
      "Val Loss: 19.638015, Acc: 2341.607\n",
      "2341.6068359375\n",
      "Epoch: 888 Train Loss: 5.075633, Acc: 2336.714\n",
      "Val Loss: 21.854151, Acc: 2740.257\n",
      "2740.257421875\n",
      "Epoch: 889 Train Loss: 4.951582, Acc: 2218.050\n",
      "Val Loss: 21.937130, Acc: 2663.817\n",
      "2663.81708984375\n",
      "Epoch: 890 Train Loss: 4.482534, Acc: 2358.944\n",
      "Val Loss: 19.669909, Acc: 2990.110\n",
      "2990.109765625\n",
      "Epoch: 891 Train Loss: 4.035018, Acc: 1779.056\n",
      "Val Loss: 18.973978, Acc: 2556.544\n",
      "2556.54384765625\n",
      "Epoch: 892 Train Loss: 4.043448, Acc: 1832.663\n",
      "Val Loss: 19.027393, Acc: 2485.854\n",
      "2485.85390625\n",
      "Epoch: 893 Train Loss: 4.457815, Acc: 2190.230\n",
      "Val Loss: 21.934204, Acc: 3483.000\n",
      "3483.000390625\n",
      "Epoch: 894 Train Loss: 4.687476, Acc: 2314.384\n",
      "Val Loss: 20.865027, Acc: 3378.247\n",
      "3378.2474609375\n",
      "Epoch: 895 Train Loss: 5.409917, Acc: 2486.161\n",
      "Val Loss: 18.365317, Acc: 2356.243\n",
      "2356.242578125\n",
      "Epoch: 896 Train Loss: 4.724099, Acc: 2072.573\n",
      "Val Loss: 22.396375, Acc: 3426.841\n",
      "3426.8408203125\n",
      "Epoch: 897 Train Loss: 5.222604, Acc: 2776.981\n",
      "Val Loss: 22.251229, Acc: 4944.263\n",
      "4944.262890625\n",
      "Epoch: 898 Train Loss: 7.876070, Acc: 3069.244\n",
      "Val Loss: 19.957503, Acc: 3871.342\n",
      "3871.3421875\n",
      "Epoch: 899 Train Loss: 11.225361, Acc: 4641.478\n",
      "Val Loss: 28.941452, Acc: 7348.434\n",
      "7348.433984375\n",
      "Epoch: 900 Train Loss: 8.842784, Acc: 4776.847\n",
      "Val Loss: 21.743301, Acc: 6254.219\n",
      "6254.2193359375\n",
      "Epoch: 901 Train Loss: 8.906310, Acc: 4235.435\n",
      "Val Loss: 27.615934, Acc: 3776.942\n",
      "3776.9423828125\n",
      "Epoch: 902 Train Loss: 5.291653, Acc: 2155.680\n",
      "Val Loss: 18.756461, Acc: 3109.633\n",
      "3109.63291015625\n",
      "Epoch: 903 Train Loss: 4.339426, Acc: 2138.494\n",
      "Val Loss: 19.333918, Acc: 2589.475\n",
      "2589.47529296875\n",
      "Epoch: 904 Train Loss: 4.072585, Acc: 1928.967\n",
      "Val Loss: 18.898832, Acc: 3171.046\n",
      "3171.04638671875\n",
      "Epoch: 905 Train Loss: 4.246246, Acc: 2369.263\n",
      "Val Loss: 17.769920, Acc: 2341.146\n",
      "2341.1458984375\n",
      "Epoch: 906 Train Loss: 4.164365, Acc: 1996.568\n",
      "Val Loss: 19.309042, Acc: 2428.748\n",
      "2428.74814453125\n",
      "Epoch: 907 Train Loss: 4.793825, Acc: 2011.986\n",
      "Val Loss: 18.048653, Acc: 3353.974\n",
      "3353.97421875\n",
      "Epoch: 908 Train Loss: 5.613580, Acc: 2677.523\n",
      "Val Loss: 24.587102, Acc: 4289.851\n",
      "4289.8513671875\n",
      "Epoch: 909 Train Loss: 5.419929, Acc: 2563.594\n",
      "Val Loss: 22.384571, Acc: 3239.751\n",
      "3239.75126953125\n",
      "Epoch: 910 Train Loss: 4.675741, Acc: 1937.060\n",
      "Val Loss: 17.957476, Acc: 2372.415\n",
      "2372.41455078125\n",
      "Epoch: 911 Train Loss: 5.153184, Acc: 2227.794\n",
      "Val Loss: 19.357408, Acc: 3054.481\n",
      "3054.48125\n",
      "Epoch: 912 Train Loss: 5.413889, Acc: 2521.405\n",
      "Val Loss: 20.464798, Acc: 4854.128\n",
      "4854.128125\n",
      "Epoch: 913 Train Loss: 4.672013, Acc: 2916.751\n",
      "Val Loss: 18.659364, Acc: 2340.413\n",
      "2340.41337890625\n",
      "Epoch: 914 Train Loss: 4.020170, Acc: 2013.539\n",
      "Val Loss: 17.976331, Acc: 2405.087\n",
      "2405.087109375\n",
      "Epoch: 915 Train Loss: 3.821044, Acc: 1615.175\n",
      "Val Loss: 17.466335, Acc: 2282.358\n",
      "2282.35810546875\n",
      "2282.35810546875 2282.35810546875\n",
      "Epoch: 916 Train Loss: 3.825968, Acc: 1687.421\n",
      "Val Loss: 18.324492, Acc: 2664.650\n",
      "2664.65048828125\n",
      "Epoch: 917 Train Loss: 3.753860, Acc: 1646.469\n",
      "Val Loss: 19.297688, Acc: 2449.418\n",
      "2449.41767578125\n",
      "Epoch: 918 Train Loss: 3.846583, Acc: 1844.338\n",
      "Val Loss: 18.649929, Acc: 3341.071\n",
      "3341.0705078125\n",
      "Epoch: 919 Train Loss: 4.967279, Acc: 2553.395\n",
      "Val Loss: 19.890882, Acc: 2632.069\n",
      "2632.06923828125\n",
      "Epoch: 920 Train Loss: 3.958162, Acc: 1856.975\n",
      "Val Loss: 18.671123, Acc: 2368.465\n",
      "2368.46484375\n",
      "Epoch: 921 Train Loss: 4.446068, Acc: 1963.952\n",
      "Val Loss: 19.782664, Acc: 3117.788\n",
      "3117.78759765625\n",
      "Epoch: 922 Train Loss: 4.242666, Acc: 1929.547\n",
      "Val Loss: 21.863987, Acc: 2651.378\n",
      "2651.37841796875\n",
      "Epoch: 923 Train Loss: 4.898285, Acc: 2597.241\n",
      "Val Loss: 20.959749, Acc: 4425.200\n",
      "4425.199609375\n",
      "Epoch: 924 Train Loss: 5.034906, Acc: 2564.622\n",
      "Val Loss: 20.544172, Acc: 2943.576\n",
      "2943.57626953125\n",
      "Epoch: 925 Train Loss: 5.348595, Acc: 2564.514\n",
      "Val Loss: 19.679173, Acc: 2845.139\n",
      "2845.1390625\n",
      "Epoch: 926 Train Loss: 5.324119, Acc: 2353.248\n",
      "Val Loss: 18.648916, Acc: 3009.114\n",
      "3009.114453125\n",
      "Epoch: 927 Train Loss: 5.236460, Acc: 2269.450\n",
      "Val Loss: 18.192461, Acc: 2935.535\n",
      "2935.53505859375\n",
      "Epoch: 928 Train Loss: 4.288799, Acc: 1811.051\n",
      "Val Loss: 20.339811, Acc: 2591.169\n",
      "2591.169140625\n",
      "Epoch: 929 Train Loss: 4.439109, Acc: 2116.444\n",
      "Val Loss: 18.899288, Acc: 2420.828\n",
      "2420.82763671875\n",
      "Epoch: 930 Train Loss: 4.267427, Acc: 1887.955\n",
      "Val Loss: 19.085138, Acc: 2319.607\n",
      "2319.6068359375\n",
      "Epoch: 931 Train Loss: 3.682349, Acc: 1599.245\n",
      "Val Loss: 19.007847, Acc: 2570.223\n",
      "2570.22265625\n",
      "Epoch: 932 Train Loss: 4.326716, Acc: 1991.808\n",
      "Val Loss: 18.292400, Acc: 3659.327\n",
      "3659.3265625\n",
      "Epoch: 933 Train Loss: 4.529811, Acc: 2838.411\n",
      "Val Loss: 20.183444, Acc: 2523.855\n",
      "2523.8546875\n",
      "Epoch: 934 Train Loss: 4.472290, Acc: 2040.619\n",
      "Val Loss: 17.357086, Acc: 2345.361\n",
      "2345.3609375\n",
      "Epoch: 935 Train Loss: 3.841523, Acc: 1850.534\n",
      "Val Loss: 18.428689, Acc: 2707.888\n",
      "2707.88837890625\n",
      "Epoch: 936 Train Loss: 3.872765, Acc: 1921.629\n",
      "Val Loss: 18.186400, Acc: 2827.228\n",
      "2827.2283203125\n",
      "Epoch: 937 Train Loss: 3.719104, Acc: 1709.486\n",
      "Val Loss: 18.488727, Acc: 2632.171\n",
      "2632.170703125\n",
      "Epoch: 938 Train Loss: 4.176243, Acc: 2142.017\n",
      "Val Loss: 21.327030, Acc: 4223.304\n",
      "4223.30390625\n",
      "Epoch: 939 Train Loss: 6.025783, Acc: 2403.405\n",
      "Val Loss: 16.945264, Acc: 2433.659\n",
      "2433.6587890625\n",
      "Epoch: 940 Train Loss: 7.928355, Acc: 2970.290\n",
      "Val Loss: 17.561841, Acc: 4461.657\n",
      "4461.6572265625\n",
      "Epoch: 941 Train Loss: 6.131116, Acc: 3002.856\n",
      "Val Loss: 21.270234, Acc: 2935.751\n",
      "2935.7513671875\n",
      "Epoch: 942 Train Loss: 6.413375, Acc: 2476.815\n",
      "Val Loss: 19.222046, Acc: 3760.375\n",
      "3760.375\n",
      "Epoch: 943 Train Loss: 4.832149, Acc: 2425.774\n",
      "Val Loss: 19.173301, Acc: 2669.237\n",
      "2669.2373046875\n",
      "Epoch: 944 Train Loss: 4.227652, Acc: 1992.827\n",
      "Val Loss: 19.068589, Acc: 3382.856\n",
      "3382.855859375\n",
      "Epoch: 945 Train Loss: 4.254880, Acc: 2105.566\n",
      "Val Loss: 17.272812, Acc: 2401.539\n",
      "2401.539453125\n",
      "Epoch: 946 Train Loss: 4.148977, Acc: 1854.315\n",
      "Val Loss: 18.972590, Acc: 3285.024\n",
      "3285.02421875\n",
      "Epoch: 947 Train Loss: 3.811056, Acc: 1815.744\n",
      "Val Loss: 18.158594, Acc: 2362.817\n",
      "2362.81650390625\n",
      "Epoch: 948 Train Loss: 3.492664, Acc: 1476.086\n",
      "Val Loss: 19.712665, Acc: 2419.218\n",
      "2419.2181640625\n",
      "Epoch: 949 Train Loss: 5.506140, Acc: 2339.826\n",
      "Val Loss: 19.064877, Acc: 2574.851\n",
      "2574.8509765625\n",
      "Epoch: 950 Train Loss: 6.729786, Acc: 2924.099\n",
      "Val Loss: 23.325229, Acc: 4170.534\n",
      "4170.533984375\n",
      "Epoch: 951 Train Loss: 5.906510, Acc: 3097.191\n",
      "Val Loss: 18.241277, Acc: 2719.426\n",
      "2719.4255859375\n",
      "Epoch: 952 Train Loss: 5.259841, Acc: 2638.065\n",
      "Val Loss: 20.454978, Acc: 3260.927\n",
      "3260.92724609375\n",
      "Epoch: 953 Train Loss: 4.887460, Acc: 2082.961\n",
      "Val Loss: 17.768765, Acc: 2942.267\n",
      "2942.266796875\n",
      "Epoch: 954 Train Loss: 4.254725, Acc: 1805.989\n",
      "Val Loss: 18.970379, Acc: 2650.629\n",
      "2650.62900390625\n",
      "Epoch: 955 Train Loss: 4.814683, Acc: 2774.664\n",
      "Val Loss: 17.700508, Acc: 2326.232\n",
      "2326.231640625\n",
      "Epoch: 956 Train Loss: 4.155643, Acc: 2043.485\n",
      "Val Loss: 17.610803, Acc: 2306.951\n",
      "2306.95107421875\n",
      "Epoch: 957 Train Loss: 3.862133, Acc: 1572.181\n",
      "Val Loss: 20.970669, Acc: 2945.960\n",
      "2945.9599609375\n",
      "Epoch: 958 Train Loss: 4.221056, Acc: 1755.775\n",
      "Val Loss: 17.508837, Acc: 2213.716\n",
      "2213.71630859375\n",
      "2213.71630859375 2213.71630859375\n",
      "Epoch: 959 Train Loss: 3.869268, Acc: 1697.797\n",
      "Val Loss: 20.551509, Acc: 2434.961\n",
      "2434.96064453125\n",
      "Epoch: 960 Train Loss: 4.454908, Acc: 2369.163\n",
      "Val Loss: 20.121450, Acc: 2605.868\n",
      "2605.867578125\n",
      "Epoch: 961 Train Loss: 5.191901, Acc: 2246.639\n",
      "Val Loss: 18.844189, Acc: 4284.020\n",
      "4284.0203125\n",
      "Epoch: 962 Train Loss: 4.913519, Acc: 2801.794\n",
      "Val Loss: 17.425195, Acc: 3492.265\n",
      "3492.265234375\n",
      "Epoch: 963 Train Loss: 4.082132, Acc: 1931.796\n",
      "Val Loss: 19.013013, Acc: 2761.928\n",
      "2761.9275390625\n",
      "Epoch: 964 Train Loss: 4.029928, Acc: 1651.120\n",
      "Val Loss: 19.439979, Acc: 2433.156\n",
      "2433.15556640625\n",
      "Epoch: 965 Train Loss: 3.905246, Acc: 1909.814\n",
      "Val Loss: 17.389297, Acc: 3111.920\n",
      "3111.91953125\n",
      "Epoch: 966 Train Loss: 5.047194, Acc: 2111.494\n",
      "Val Loss: 18.262218, Acc: 2539.317\n",
      "2539.31689453125\n",
      "Epoch: 967 Train Loss: 4.281048, Acc: 2250.858\n",
      "Val Loss: 20.675011, Acc: 2564.048\n",
      "2564.048046875\n",
      "Epoch: 968 Train Loss: 5.965635, Acc: 2257.138\n",
      "Val Loss: 15.704014, Acc: 3137.844\n",
      "3137.8443359375\n",
      "Epoch: 969 Train Loss: 6.661524, Acc: 2732.711\n",
      "Val Loss: 18.371870, Acc: 2497.147\n",
      "2497.14677734375\n",
      "Epoch: 970 Train Loss: 6.345262, Acc: 2796.298\n",
      "Val Loss: 17.458100, Acc: 2308.350\n",
      "2308.35048828125\n",
      "Epoch: 971 Train Loss: 5.157830, Acc: 2572.804\n",
      "Val Loss: 17.381050, Acc: 2688.255\n",
      "2688.2552734375\n",
      "Epoch: 972 Train Loss: 4.586409, Acc: 2280.964\n",
      "Val Loss: 19.370842, Acc: 3430.011\n",
      "3430.0107421875\n",
      "Epoch: 973 Train Loss: 4.588241, Acc: 1979.904\n",
      "Val Loss: 16.144981, Acc: 2405.525\n",
      "2405.525390625\n",
      "Epoch: 974 Train Loss: 3.977842, Acc: 2035.431\n",
      "Val Loss: 17.626166, Acc: 2646.155\n",
      "2646.1548828125\n",
      "Epoch: 975 Train Loss: 3.858511, Acc: 1659.760\n",
      "Val Loss: 17.091657, Acc: 2157.917\n",
      "2157.91689453125\n",
      "2157.91689453125 2157.91689453125\n",
      "Epoch: 976 Train Loss: 4.283850, Acc: 1761.359\n",
      "Val Loss: 17.728742, Acc: 2622.385\n",
      "2622.3853515625\n",
      "Epoch: 977 Train Loss: 4.544488, Acc: 2207.872\n",
      "Val Loss: 21.623760, Acc: 4367.196\n",
      "4367.1958984375\n",
      "Epoch: 978 Train Loss: 4.320560, Acc: 2123.028\n",
      "Val Loss: 18.105755, Acc: 2581.733\n",
      "2581.73349609375\n",
      "Epoch: 979 Train Loss: 4.794114, Acc: 1833.647\n",
      "Val Loss: 17.712063, Acc: 2304.352\n",
      "2304.3517578125\n",
      "Epoch: 980 Train Loss: 4.652339, Acc: 1931.875\n",
      "Val Loss: 21.248403, Acc: 2727.492\n",
      "2727.49248046875\n",
      "Epoch: 981 Train Loss: 4.662764, Acc: 2069.484\n",
      "Val Loss: 16.197790, Acc: 4017.273\n",
      "4017.27265625\n",
      "Epoch: 982 Train Loss: 3.929238, Acc: 2339.035\n",
      "Val Loss: 20.106521, Acc: 2529.250\n",
      "2529.24970703125\n",
      "Epoch: 983 Train Loss: 4.116976, Acc: 2034.378\n",
      "Val Loss: 18.042151, Acc: 2699.997\n",
      "2699.9974609375\n",
      "Epoch: 984 Train Loss: 4.497097, Acc: 2434.296\n",
      "Val Loss: 18.057653, Acc: 3684.335\n",
      "3684.3345703125\n",
      "Epoch: 985 Train Loss: 3.785619, Acc: 1997.195\n",
      "Val Loss: 17.221886, Acc: 2454.391\n",
      "2454.39111328125\n",
      "Epoch: 986 Train Loss: 3.360571, Acc: 1578.096\n",
      "Val Loss: 17.083552, Acc: 2182.464\n",
      "2182.4638671875\n",
      "Epoch: 987 Train Loss: 3.412729, Acc: 1469.754\n",
      "Val Loss: 17.828017, Acc: 2669.582\n",
      "2669.5818359375\n",
      "Epoch: 988 Train Loss: 3.392173, Acc: 1690.362\n",
      "Val Loss: 17.271625, Acc: 2935.435\n",
      "2935.4345703125\n",
      "Epoch: 989 Train Loss: 3.467930, Acc: 1780.521\n",
      "Val Loss: 17.166388, Acc: 2135.574\n",
      "2135.573828125\n",
      "2135.573828125 2135.573828125\n",
      "Epoch: 990 Train Loss: 3.423668, Acc: 1525.369\n",
      "Val Loss: 16.149231, Acc: 2104.801\n",
      "2104.8009765625\n",
      "2104.8009765625 2104.8009765625\n",
      "Epoch: 991 Train Loss: 3.893642, Acc: 1985.530\n",
      "Val Loss: 17.168978, Acc: 2203.604\n",
      "2203.6037109375\n",
      "Epoch: 992 Train Loss: 4.049429, Acc: 2147.206\n",
      "Val Loss: 17.705132, Acc: 2987.850\n",
      "2987.85009765625\n",
      "Epoch: 993 Train Loss: 4.514540, Acc: 1785.732\n",
      "Val Loss: 16.816258, Acc: 2851.710\n",
      "2851.7099609375\n",
      "Epoch: 994 Train Loss: 6.218203, Acc: 2929.044\n",
      "Val Loss: 25.046030, Acc: 5642.130\n",
      "5642.130078125\n",
      "Epoch: 995 Train Loss: 6.771994, Acc: 2766.157\n",
      "Val Loss: 17.009977, Acc: 2402.915\n",
      "2402.9154296875\n",
      "Epoch: 996 Train Loss: 4.981432, Acc: 2081.870\n",
      "Val Loss: 17.144732, Acc: 2241.161\n",
      "2241.1607421875\n",
      "Epoch: 997 Train Loss: 4.248411, Acc: 1850.956\n",
      "Val Loss: 17.206214, Acc: 3128.642\n",
      "3128.6421875\n",
      "Epoch: 998 Train Loss: 3.988321, Acc: 2240.129\n",
      "Val Loss: 15.769139, Acc: 2561.269\n",
      "2561.26904296875\n",
      "Epoch: 999 Train Loss: 3.679188, Acc: 2236.849\n",
      "Val Loss: 15.204254, Acc: 2554.534\n",
      "2554.534375\n",
      "Epoch: 1000 Train Loss: 3.419689, Acc: 1780.639\n",
      "Val Loss: 16.289492, Acc: 2485.961\n",
      "2485.9611328125\n",
      "Epoch: 1001 Train Loss: 3.793737, Acc: 1974.148\n",
      "Val Loss: 16.743924, Acc: 2083.558\n",
      "2083.55791015625\n",
      "2083.55791015625 2083.55791015625\n",
      "Epoch: 1002 Train Loss: 4.634090, Acc: 2011.061\n",
      "Val Loss: 16.937171, Acc: 2977.544\n",
      "2977.54423828125\n",
      "Epoch: 1003 Train Loss: 4.082100, Acc: 2210.116\n",
      "Val Loss: 17.913003, Acc: 3525.686\n",
      "3525.685546875\n",
      "Epoch: 1004 Train Loss: 4.926023, Acc: 2322.043\n",
      "Val Loss: 17.005771, Acc: 2544.212\n",
      "2544.21181640625\n",
      "Epoch: 1005 Train Loss: 4.228929, Acc: 1778.309\n",
      "Val Loss: 16.346683, Acc: 2566.695\n",
      "2566.69541015625\n",
      "Epoch: 1006 Train Loss: 3.749356, Acc: 1962.473\n",
      "Val Loss: 16.347389, Acc: 2544.214\n",
      "2544.21396484375\n",
      "Epoch: 1007 Train Loss: 3.477168, Acc: 1751.270\n",
      "Val Loss: 16.763135, Acc: 2149.314\n",
      "2149.3138671875\n",
      "Epoch: 1008 Train Loss: 3.249049, Acc: 1512.639\n",
      "Val Loss: 16.157578, Acc: 2409.345\n",
      "2409.3447265625\n",
      "Epoch: 1009 Train Loss: 3.545934, Acc: 1708.301\n",
      "Val Loss: 16.958956, Acc: 2475.110\n",
      "2475.11025390625\n",
      "Epoch: 1010 Train Loss: 3.756521, Acc: 1828.542\n",
      "Val Loss: 16.374691, Acc: 2300.046\n",
      "2300.0455078125\n",
      "Epoch: 1011 Train Loss: 3.623617, Acc: 1470.511\n",
      "Val Loss: 16.576116, Acc: 2865.851\n",
      "2865.8505859375\n",
      "Epoch: 1012 Train Loss: 4.505751, Acc: 2333.586\n",
      "Val Loss: 17.863390, Acc: 3527.097\n",
      "3527.096875\n",
      "Epoch: 1013 Train Loss: 3.971170, Acc: 2374.886\n",
      "Val Loss: 16.713506, Acc: 2626.905\n",
      "2626.90517578125\n",
      "Epoch: 1014 Train Loss: 3.384983, Acc: 1981.811\n",
      "Val Loss: 16.897268, Acc: 2162.562\n",
      "2162.56162109375\n",
      "Epoch: 1015 Train Loss: 3.687513, Acc: 2136.892\n",
      "Val Loss: 16.521390, Acc: 2365.466\n",
      "2365.46640625\n",
      "Epoch: 1016 Train Loss: 3.583927, Acc: 1659.986\n",
      "Val Loss: 16.530531, Acc: 3002.121\n",
      "3002.121484375\n",
      "Epoch: 1017 Train Loss: 3.375400, Acc: 1933.921\n",
      "Val Loss: 18.394896, Acc: 2953.308\n",
      "2953.3083984375\n",
      "Epoch: 1018 Train Loss: 3.584851, Acc: 1733.976\n",
      "Val Loss: 18.486785, Acc: 2276.945\n",
      "2276.94501953125\n",
      "Epoch: 1019 Train Loss: 3.194406, Acc: 1375.497\n",
      "Val Loss: 16.297616, Acc: 2099.209\n",
      "2099.208984375\n",
      "Epoch: 1020 Train Loss: 3.586807, Acc: 1550.549\n",
      "Val Loss: 17.947920, Acc: 2373.620\n",
      "2373.61953125\n",
      "Epoch: 1021 Train Loss: 4.542385, Acc: 1711.241\n",
      "Val Loss: 16.560024, Acc: 2040.426\n",
      "2040.42607421875\n",
      "2040.42607421875 2040.42607421875\n",
      "Epoch: 1022 Train Loss: 3.418094, Acc: 1537.351\n",
      "Val Loss: 15.627359, Acc: 2330.657\n",
      "2330.6568359375\n",
      "Epoch: 1023 Train Loss: 3.339746, Acc: 1622.240\n",
      "Val Loss: 17.445412, Acc: 2375.983\n",
      "2375.98251953125\n",
      "Epoch: 1024 Train Loss: 4.586836, Acc: 1962.451\n",
      "Val Loss: 17.549859, Acc: 3276.667\n",
      "3276.666796875\n",
      "Epoch: 1025 Train Loss: 5.647301, Acc: 2697.711\n",
      "Val Loss: 19.167076, Acc: 2932.596\n",
      "2932.5958984375\n",
      "Epoch: 1026 Train Loss: 4.323663, Acc: 2043.588\n",
      "Val Loss: 14.722412, Acc: 2219.138\n",
      "2219.13818359375\n",
      "Epoch: 1027 Train Loss: 4.172383, Acc: 2011.907\n",
      "Val Loss: 20.255026, Acc: 3051.208\n",
      "3051.2083984375\n",
      "Epoch: 1028 Train Loss: 4.357650, Acc: 2004.160\n",
      "Val Loss: 16.378635, Acc: 2669.538\n",
      "2669.5384765625\n",
      "Epoch: 1029 Train Loss: 3.397109, Acc: 1741.337\n",
      "Val Loss: 16.075293, Acc: 2294.864\n",
      "2294.864453125\n",
      "Epoch: 1030 Train Loss: 3.218979, Acc: 1509.933\n",
      "Val Loss: 15.486299, Acc: 2347.476\n",
      "2347.47607421875\n",
      "Epoch: 1031 Train Loss: 3.343299, Acc: 1499.947\n",
      "Val Loss: 16.450757, Acc: 2240.804\n",
      "2240.804296875\n",
      "Epoch: 1032 Train Loss: 3.451242, Acc: 1437.398\n",
      "Val Loss: 18.050306, Acc: 2335.903\n",
      "2335.9029296875\n",
      "Epoch: 1033 Train Loss: 3.628702, Acc: 1696.294\n",
      "Val Loss: 16.124546, Acc: 2687.392\n",
      "2687.3921875\n",
      "Epoch: 1034 Train Loss: 3.146210, Acc: 1575.173\n",
      "Val Loss: 16.298198, Acc: 2691.631\n",
      "2691.63115234375\n",
      "Epoch: 1035 Train Loss: 3.429926, Acc: 1721.089\n",
      "Val Loss: 15.307885, Acc: 2316.195\n",
      "2316.19462890625\n",
      "Epoch: 1036 Train Loss: 3.565063, Acc: 1546.378\n",
      "Val Loss: 15.920097, Acc: 2308.324\n",
      "2308.3240234375\n",
      "Epoch: 1037 Train Loss: 4.051916, Acc: 2102.402\n",
      "Val Loss: 21.259968, Acc: 3796.291\n",
      "3796.291015625\n",
      "Epoch: 1038 Train Loss: 4.075018, Acc: 1734.988\n",
      "Val Loss: 15.099868, Acc: 2080.743\n",
      "2080.74306640625\n",
      "Epoch: 1039 Train Loss: 4.015685, Acc: 1737.070\n",
      "Val Loss: 15.367732, Acc: 2250.427\n",
      "2250.42705078125\n",
      "Epoch: 1040 Train Loss: 3.908267, Acc: 1539.122\n",
      "Val Loss: 18.316973, Acc: 2889.943\n",
      "2889.943359375\n",
      "Epoch: 1041 Train Loss: 5.103400, Acc: 2115.361\n",
      "Val Loss: 21.798372, Acc: 3105.451\n",
      "3105.4509765625\n",
      "Epoch: 1042 Train Loss: 4.609042, Acc: 1990.825\n",
      "Val Loss: 17.819538, Acc: 2807.342\n",
      "2807.3416015625\n",
      "Epoch: 1043 Train Loss: 3.323109, Acc: 1647.306\n",
      "Val Loss: 15.988684, Acc: 2552.846\n",
      "2552.8458984375\n",
      "Epoch: 1044 Train Loss: 5.075151, Acc: 2409.897\n",
      "Val Loss: 17.590690, Acc: 3573.466\n",
      "3573.465625\n",
      "Epoch: 1045 Train Loss: 4.205729, Acc: 1865.247\n",
      "Val Loss: 16.130350, Acc: 2223.574\n",
      "2223.5744140625\n",
      "Epoch: 1046 Train Loss: 4.048329, Acc: 1663.136\n",
      "Val Loss: 14.285244, Acc: 2367.791\n",
      "2367.79111328125\n",
      "Epoch: 1047 Train Loss: 3.716911, Acc: 1711.065\n",
      "Val Loss: 17.521768, Acc: 2265.785\n",
      "2265.78525390625\n",
      "Epoch: 1048 Train Loss: 3.168025, Acc: 1447.158\n",
      "Val Loss: 15.153331, Acc: 2094.345\n",
      "2094.34462890625\n",
      "Epoch: 1049 Train Loss: 3.975903, Acc: 1585.926\n",
      "Val Loss: 18.096820, Acc: 2259.459\n",
      "2259.45908203125\n",
      "Epoch: 1050 Train Loss: 4.370245, Acc: 1844.239\n",
      "Val Loss: 14.933671, Acc: 2269.246\n",
      "2269.24560546875\n",
      "Epoch: 1051 Train Loss: 3.588410, Acc: 2319.558\n",
      "Val Loss: 19.062731, Acc: 4059.934\n",
      "4059.93359375\n",
      "Epoch: 1052 Train Loss: 4.841996, Acc: 2105.401\n",
      "Val Loss: 18.098202, Acc: 3792.129\n",
      "3792.12890625\n",
      "Epoch: 1053 Train Loss: 4.129317, Acc: 2247.474\n",
      "Val Loss: 15.113960, Acc: 1976.303\n",
      "1976.3033203125\n",
      "1976.3033203125 1976.3033203125\n",
      "Epoch: 1054 Train Loss: 3.054894, Acc: 1667.183\n",
      "Val Loss: 15.367775, Acc: 2175.052\n",
      "2175.05224609375\n",
      "Epoch: 1055 Train Loss: 3.242801, Acc: 1759.432\n",
      "Val Loss: 14.922867, Acc: 2254.218\n",
      "2254.2177734375\n",
      "Epoch: 1056 Train Loss: 2.830540, Acc: 1549.647\n",
      "Val Loss: 15.870718, Acc: 2100.096\n",
      "2100.09638671875\n",
      "Epoch: 1057 Train Loss: 3.440495, Acc: 1547.560\n",
      "Val Loss: 15.060558, Acc: 2132.962\n",
      "2132.96162109375\n",
      "Epoch: 1058 Train Loss: 3.307345, Acc: 1362.917\n",
      "Val Loss: 15.236608, Acc: 2132.012\n",
      "2132.0123046875\n",
      "Epoch: 1059 Train Loss: 5.144309, Acc: 1968.575\n",
      "Val Loss: 17.980140, Acc: 3215.616\n",
      "3215.61572265625\n",
      "Epoch: 1060 Train Loss: 3.684762, Acc: 1670.809\n",
      "Val Loss: 14.475079, Acc: 2036.378\n",
      "2036.3775390625\n",
      "Epoch: 1061 Train Loss: 3.407215, Acc: 1515.241\n",
      "Val Loss: 16.128043, Acc: 2175.230\n",
      "2175.22958984375\n",
      "Epoch: 1062 Train Loss: 2.885518, Acc: 1340.997\n",
      "Val Loss: 14.618763, Acc: 1961.257\n",
      "1961.2568359375\n",
      "1961.2568359375 1961.2568359375\n",
      "Epoch: 1063 Train Loss: 3.082929, Acc: 1323.820\n",
      "Val Loss: 15.811579, Acc: 2232.854\n",
      "2232.85390625\n",
      "Epoch: 1064 Train Loss: 2.793804, Acc: 1365.198\n",
      "Val Loss: 15.919697, Acc: 2088.191\n",
      "2088.19052734375\n",
      "Epoch: 1065 Train Loss: 4.746984, Acc: 1945.412\n",
      "Val Loss: 20.967126, Acc: 3484.484\n",
      "3484.483984375\n",
      "Epoch: 1066 Train Loss: 4.169200, Acc: 2269.967\n",
      "Val Loss: 15.617296, Acc: 3584.959\n",
      "3584.959375\n",
      "Epoch: 1067 Train Loss: 3.095094, Acc: 2114.477\n",
      "Val Loss: 14.647600, Acc: 2190.884\n",
      "2190.88408203125\n",
      "Epoch: 1068 Train Loss: 3.529429, Acc: 1807.097\n",
      "Val Loss: 17.693543, Acc: 3102.436\n",
      "3102.4359375\n",
      "Epoch: 1069 Train Loss: 3.700895, Acc: 1643.772\n",
      "Val Loss: 15.627336, Acc: 2104.436\n",
      "2104.4357421875\n",
      "Epoch: 1070 Train Loss: 2.799665, Acc: 1297.453\n",
      "Val Loss: 15.246133, Acc: 1960.456\n",
      "1960.45576171875\n",
      "1960.45576171875 1960.45576171875\n",
      "Epoch: 1071 Train Loss: 3.009802, Acc: 1608.703\n",
      "Val Loss: 16.299201, Acc: 2845.721\n",
      "2845.72119140625\n",
      "Epoch: 1072 Train Loss: 2.991609, Acc: 1615.941\n",
      "Val Loss: 15.891303, Acc: 2355.064\n",
      "2355.06416015625\n",
      "Epoch: 1073 Train Loss: 2.821474, Acc: 1422.027\n",
      "Val Loss: 14.802221, Acc: 1951.952\n",
      "1951.951953125\n",
      "1951.951953125 1951.951953125\n",
      "Epoch: 1074 Train Loss: 3.595736, Acc: 1600.719\n",
      "Val Loss: 17.187439, Acc: 2186.488\n",
      "2186.48798828125\n",
      "Epoch: 1075 Train Loss: 4.489155, Acc: 2085.709\n",
      "Val Loss: 15.921360, Acc: 2178.505\n",
      "2178.50546875\n",
      "Epoch: 1076 Train Loss: 3.049827, Acc: 1489.777\n",
      "Val Loss: 14.519147, Acc: 1973.748\n",
      "1973.7484375\n",
      "Epoch: 1077 Train Loss: 3.475616, Acc: 1566.113\n",
      "Val Loss: 13.755597, Acc: 2487.941\n",
      "2487.941015625\n",
      "Epoch: 1078 Train Loss: 3.767741, Acc: 1567.696\n",
      "Val Loss: 15.963904, Acc: 2097.745\n",
      "2097.7453125\n",
      "Epoch: 1079 Train Loss: 3.805269, Acc: 1895.065\n",
      "Val Loss: 15.426656, Acc: 2732.160\n",
      "2732.16015625\n",
      "Epoch: 1080 Train Loss: 4.029150, Acc: 1897.637\n",
      "Val Loss: 14.406510, Acc: 2263.376\n",
      "2263.3759765625\n",
      "Epoch: 1081 Train Loss: 3.503205, Acc: 1711.507\n",
      "Val Loss: 18.714256, Acc: 2450.047\n",
      "2450.04677734375\n",
      "Epoch: 1082 Train Loss: 3.003407, Acc: 1378.270\n",
      "Val Loss: 13.769457, Acc: 2109.216\n",
      "2109.21552734375\n",
      "Epoch: 1083 Train Loss: 2.983097, Acc: 1383.232\n",
      "Val Loss: 14.569037, Acc: 1969.147\n",
      "1969.146875\n",
      "Epoch: 1084 Train Loss: 2.890969, Acc: 1405.423\n",
      "Val Loss: 16.153457, Acc: 2070.205\n",
      "2070.2052734375\n",
      "Epoch: 1085 Train Loss: 5.028025, Acc: 1729.761\n",
      "Val Loss: 16.562030, Acc: 2259.504\n",
      "2259.50380859375\n",
      "Epoch: 1086 Train Loss: 4.030101, Acc: 1796.165\n",
      "Val Loss: 14.145588, Acc: 2190.209\n",
      "2190.20859375\n",
      "Epoch: 1087 Train Loss: 3.470561, Acc: 1808.188\n",
      "Val Loss: 14.645645, Acc: 2366.421\n",
      "2366.421484375\n",
      "Epoch: 1088 Train Loss: 4.050318, Acc: 2023.609\n",
      "Val Loss: 16.067212, Acc: 2810.679\n",
      "2810.679296875\n",
      "Epoch: 1089 Train Loss: 3.808073, Acc: 1789.171\n",
      "Val Loss: 15.280969, Acc: 2982.162\n",
      "2982.162109375\n",
      "Epoch: 1090 Train Loss: 3.155494, Acc: 1721.729\n",
      "Val Loss: 13.867980, Acc: 1908.881\n",
      "1908.8810546875\n",
      "1908.8810546875 1908.8810546875\n",
      "Epoch: 1091 Train Loss: 3.580440, Acc: 1427.602\n",
      "Val Loss: 13.684572, Acc: 2139.256\n",
      "2139.2564453125\n",
      "Epoch: 1092 Train Loss: 4.766541, Acc: 1781.120\n",
      "Val Loss: 18.290494, Acc: 2894.202\n",
      "2894.20234375\n",
      "Epoch: 1093 Train Loss: 4.426589, Acc: 2041.731\n",
      "Val Loss: 15.090310, Acc: 2131.426\n",
      "2131.4263671875\n",
      "Epoch: 1094 Train Loss: 4.784624, Acc: 1888.135\n",
      "Val Loss: 13.627870, Acc: 2136.683\n",
      "2136.6830078125\n",
      "Epoch: 1095 Train Loss: 4.291924, Acc: 2107.130\n",
      "Val Loss: 18.074529, Acc: 2331.731\n",
      "2331.73125\n",
      "Epoch: 1096 Train Loss: 3.626888, Acc: 1537.588\n",
      "Val Loss: 13.894844, Acc: 1938.786\n",
      "1938.7861328125\n",
      "Epoch: 1097 Train Loss: 4.038443, Acc: 1700.543\n",
      "Val Loss: 16.081402, Acc: 2564.508\n",
      "2564.508203125\n",
      "Epoch: 1098 Train Loss: 2.999882, Acc: 1578.511\n",
      "Val Loss: 14.637928, Acc: 2356.979\n",
      "2356.978515625\n",
      "Epoch: 1099 Train Loss: 2.737216, Acc: 1325.162\n",
      "Val Loss: 14.858361, Acc: 2094.691\n",
      "2094.6912109375\n",
      "Epoch: 1100 Train Loss: 2.792225, Acc: 1578.400\n",
      "Val Loss: 14.352844, Acc: 2625.921\n",
      "2625.9212890625\n",
      "Epoch: 1101 Train Loss: 3.197382, Acc: 1544.829\n",
      "Val Loss: 14.803373, Acc: 3110.184\n",
      "3110.18359375\n",
      "Epoch: 1102 Train Loss: 3.683149, Acc: 2419.798\n",
      "Val Loss: 14.615515, Acc: 1958.569\n",
      "1958.56904296875\n",
      "Epoch: 1103 Train Loss: 3.475262, Acc: 1715.415\n",
      "Val Loss: 14.371524, Acc: 1914.498\n",
      "1914.49775390625\n",
      "Epoch: 1104 Train Loss: 2.920460, Acc: 1365.598\n",
      "Val Loss: 14.079486, Acc: 1921.932\n",
      "1921.932421875\n",
      "Epoch: 1105 Train Loss: 2.564081, Acc: 1285.645\n",
      "Val Loss: 14.369042, Acc: 1961.032\n",
      "1961.03154296875\n",
      "Epoch: 1106 Train Loss: 2.409550, Acc: 1187.860\n",
      "Val Loss: 15.544654, Acc: 2007.923\n",
      "2007.92294921875\n",
      "Epoch: 1107 Train Loss: 2.721284, Acc: 1206.887\n",
      "Val Loss: 14.744171, Acc: 1890.053\n",
      "1890.0529296875\n",
      "1890.0529296875 1890.0529296875\n",
      "Epoch: 1108 Train Loss: 3.256249, Acc: 1420.510\n",
      "Val Loss: 14.559861, Acc: 2153.004\n",
      "2153.0044921875\n",
      "Epoch: 1109 Train Loss: 3.634995, Acc: 1960.676\n",
      "Val Loss: 13.866215, Acc: 2014.331\n",
      "2014.33125\n",
      "Epoch: 1110 Train Loss: 3.594049, Acc: 1643.927\n",
      "Val Loss: 15.089785, Acc: 2989.934\n",
      "2989.93369140625\n",
      "Epoch: 1111 Train Loss: 4.928665, Acc: 2139.826\n",
      "Val Loss: 18.217061, Acc: 2749.197\n",
      "2749.1974609375\n",
      "Epoch: 1112 Train Loss: 3.876301, Acc: 1557.660\n",
      "Val Loss: 13.220166, Acc: 2017.211\n",
      "2017.2107421875\n",
      "Epoch: 1113 Train Loss: 3.996089, Acc: 1716.992\n",
      "Val Loss: 14.950925, Acc: 1979.149\n",
      "1979.1494140625\n",
      "Epoch: 1114 Train Loss: 3.900530, Acc: 1804.492\n",
      "Val Loss: 16.808020, Acc: 2200.094\n",
      "2200.09365234375\n",
      "Epoch: 1115 Train Loss: 3.074311, Acc: 1381.686\n",
      "Val Loss: 14.821681, Acc: 2450.053\n",
      "2450.0529296875\n",
      "Epoch: 1116 Train Loss: 3.338363, Acc: 1760.692\n",
      "Val Loss: 18.799086, Acc: 2450.109\n",
      "2450.109375\n",
      "Epoch: 1117 Train Loss: 4.408671, Acc: 1601.409\n",
      "Val Loss: 16.245972, Acc: 2504.076\n",
      "2504.07646484375\n",
      "Epoch: 1118 Train Loss: 4.405583, Acc: 1877.494\n",
      "Val Loss: 17.824779, Acc: 2984.031\n",
      "2984.0306640625\n",
      "Epoch: 1119 Train Loss: 4.041584, Acc: 1815.846\n",
      "Val Loss: 14.304264, Acc: 2152.423\n",
      "2152.42255859375\n",
      "Epoch: 1120 Train Loss: 3.545977, Acc: 1932.368\n",
      "Val Loss: 14.233237, Acc: 2387.728\n",
      "2387.7275390625\n",
      "Epoch: 1121 Train Loss: 3.415731, Acc: 1503.098\n",
      "Val Loss: 15.226613, Acc: 2060.255\n",
      "2060.2546875\n",
      "Epoch: 1122 Train Loss: 2.869889, Acc: 1378.179\n",
      "Val Loss: 14.365152, Acc: 1964.578\n",
      "1964.5783203125\n",
      "Epoch: 1123 Train Loss: 3.404279, Acc: 1714.443\n",
      "Val Loss: 14.230932, Acc: 2451.772\n",
      "2451.77197265625\n",
      "Epoch: 1124 Train Loss: 2.845476, Acc: 1607.497\n",
      "Val Loss: 15.720378, Acc: 2386.608\n",
      "2386.608203125\n",
      "Epoch: 1125 Train Loss: 3.027410, Acc: 1501.666\n",
      "Val Loss: 13.646762, Acc: 2280.048\n",
      "2280.0482421875\n",
      "Epoch: 1126 Train Loss: 2.564032, Acc: 1353.077\n",
      "Val Loss: 13.662649, Acc: 1980.429\n",
      "1980.42890625\n",
      "Epoch: 1127 Train Loss: 2.669551, Acc: 1320.547\n",
      "Val Loss: 14.208524, Acc: 1981.236\n",
      "1981.23603515625\n",
      "Epoch: 1128 Train Loss: 2.509349, Acc: 1265.432\n",
      "Val Loss: 15.677047, Acc: 2048.596\n",
      "2048.595703125\n",
      "Epoch: 1129 Train Loss: 2.947141, Acc: 1351.444\n",
      "Val Loss: 13.751890, Acc: 1995.588\n",
      "1995.5884765625\n",
      "Epoch: 1130 Train Loss: 3.402476, Acc: 1585.714\n",
      "Val Loss: 16.383737, Acc: 2992.974\n",
      "2992.97412109375\n",
      "Epoch: 1131 Train Loss: 3.039689, Acc: 1555.581\n",
      "Val Loss: 13.063305, Acc: 1937.728\n",
      "1937.727734375\n",
      "Epoch: 1132 Train Loss: 3.496677, Acc: 1492.233\n",
      "Val Loss: 15.250348, Acc: 2363.596\n",
      "2363.59599609375\n",
      "Epoch: 1133 Train Loss: 3.069908, Acc: 1594.512\n",
      "Val Loss: 13.325278, Acc: 1901.534\n",
      "1901.5337890625\n",
      "Epoch: 1134 Train Loss: 2.431164, Acc: 1390.352\n",
      "Val Loss: 14.825895, Acc: 2230.750\n",
      "2230.7498046875\n",
      "Epoch: 1135 Train Loss: 2.838763, Acc: 1534.989\n",
      "Val Loss: 13.693354, Acc: 1867.371\n",
      "1867.3712890625\n",
      "1867.3712890625 1867.3712890625\n",
      "Epoch: 1136 Train Loss: 2.649190, Acc: 1230.821\n",
      "Val Loss: 14.343323, Acc: 2041.186\n",
      "2041.185546875\n",
      "Epoch: 1137 Train Loss: 2.549550, Acc: 1332.330\n",
      "Val Loss: 13.773083, Acc: 2137.234\n",
      "2137.2341796875\n",
      "Epoch: 1138 Train Loss: 2.540889, Acc: 1343.615\n",
      "Val Loss: 13.759558, Acc: 1916.915\n",
      "1916.9146484375\n",
      "Epoch: 1139 Train Loss: 2.482760, Acc: 1259.724\n",
      "Val Loss: 13.406301, Acc: 1791.994\n",
      "1791.99384765625\n",
      "1791.99384765625 1791.99384765625\n",
      "Epoch: 1140 Train Loss: 2.691752, Acc: 1438.219\n",
      "Val Loss: 13.892992, Acc: 1820.210\n",
      "1820.2103515625\n",
      "Epoch: 1141 Train Loss: 2.426428, Acc: 1275.309\n",
      "Val Loss: 13.241546, Acc: 1962.303\n",
      "1962.3029296875\n",
      "Epoch: 1142 Train Loss: 2.856109, Acc: 1622.947\n",
      "Val Loss: 14.711849, Acc: 2009.904\n",
      "2009.903515625\n",
      "Epoch: 1143 Train Loss: 2.287782, Acc: 1236.999\n",
      "Val Loss: 15.498828, Acc: 2090.562\n",
      "2090.5615234375\n",
      "Epoch: 1144 Train Loss: 2.630758, Acc: 1155.675\n",
      "Val Loss: 14.219642, Acc: 1889.643\n",
      "1889.643310546875\n",
      "Epoch: 1145 Train Loss: 2.312422, Acc: 1182.579\n",
      "Val Loss: 13.679343, Acc: 1990.186\n",
      "1990.18603515625\n",
      "Epoch: 1146 Train Loss: 2.411984, Acc: 1181.949\n",
      "Val Loss: 14.113710, Acc: 1761.683\n",
      "1761.6826171875\n",
      "1761.6826171875 1761.6826171875\n",
      "Epoch: 1147 Train Loss: 2.286219, Acc: 1172.201\n",
      "Val Loss: 14.642185, Acc: 2070.262\n",
      "2070.2623046875\n",
      "Epoch: 1148 Train Loss: 2.444045, Acc: 1197.920\n",
      "Val Loss: 13.156447, Acc: 1895.241\n",
      "1895.241259765625\n",
      "Epoch: 1149 Train Loss: 2.316516, Acc: 1327.080\n",
      "Val Loss: 14.176767, Acc: 1850.950\n",
      "1850.94951171875\n",
      "Epoch: 1150 Train Loss: 2.581469, Acc: 1366.269\n",
      "Val Loss: 13.815961, Acc: 1912.528\n",
      "1912.528369140625\n",
      "Epoch: 1151 Train Loss: 2.467210, Acc: 1419.668\n",
      "Val Loss: 13.004537, Acc: 1908.755\n",
      "1908.7552734375\n",
      "Epoch: 1152 Train Loss: 2.440840, Acc: 1582.465\n",
      "Val Loss: 15.149358, Acc: 2396.130\n",
      "2396.13037109375\n",
      "Epoch: 1153 Train Loss: 2.623981, Acc: 1489.190\n",
      "Val Loss: 14.481493, Acc: 2191.736\n",
      "2191.735546875\n",
      "Epoch: 1154 Train Loss: 3.065393, Acc: 1524.629\n",
      "Val Loss: 14.757667, Acc: 1892.383\n",
      "1892.3833984375\n",
      "Epoch: 1155 Train Loss: 2.520525, Acc: 1251.556\n",
      "Val Loss: 15.919626, Acc: 2231.845\n",
      "2231.8447265625\n",
      "Epoch: 1156 Train Loss: 3.559263, Acc: 1923.515\n",
      "Val Loss: 15.354826, Acc: 3083.281\n",
      "3083.28115234375\n",
      "Epoch: 1157 Train Loss: 3.092749, Acc: 1814.195\n",
      "Val Loss: 17.355783, Acc: 2340.832\n",
      "2340.8322265625\n",
      "Epoch: 1158 Train Loss: 4.172387, Acc: 1675.850\n",
      "Val Loss: 17.167826, Acc: 2518.717\n",
      "2518.71728515625\n",
      "Epoch: 1159 Train Loss: 3.631791, Acc: 1768.406\n",
      "Val Loss: 17.009948, Acc: 1981.710\n",
      "1981.7095703125\n",
      "Epoch: 1160 Train Loss: 4.081721, Acc: 1785.673\n",
      "Val Loss: 14.719858, Acc: 2109.950\n",
      "2109.95029296875\n",
      "Epoch: 1161 Train Loss: 3.565764, Acc: 1616.223\n",
      "Val Loss: 13.410187, Acc: 2019.019\n",
      "2019.0193359375\n",
      "Epoch: 1162 Train Loss: 2.435973, Acc: 1264.890\n",
      "Val Loss: 13.499583, Acc: 1904.281\n",
      "1904.2814453125\n",
      "Epoch: 1163 Train Loss: 3.468686, Acc: 1422.986\n",
      "Val Loss: 14.552299, Acc: 2349.471\n",
      "2349.471484375\n",
      "Epoch: 1164 Train Loss: 3.866615, Acc: 1669.957\n",
      "Val Loss: 16.983463, Acc: 2258.164\n",
      "2258.164453125\n",
      "Epoch: 1165 Train Loss: 2.860415, Acc: 1326.485\n",
      "Val Loss: 13.688247, Acc: 1965.994\n",
      "1965.9943359375\n",
      "Epoch: 1166 Train Loss: 3.551560, Acc: 1844.355\n",
      "Val Loss: 14.530642, Acc: 2608.860\n",
      "2608.859765625\n",
      "Epoch: 1167 Train Loss: 2.786647, Acc: 1792.683\n",
      "Val Loss: 14.134390, Acc: 2030.082\n",
      "2030.0818359375\n",
      "Epoch: 1168 Train Loss: 2.413682, Acc: 1269.168\n",
      "Val Loss: 17.155069, Acc: 2680.854\n",
      "2680.85361328125\n",
      "Epoch: 1169 Train Loss: 3.540289, Acc: 1411.925\n",
      "Val Loss: 15.816963, Acc: 2209.988\n",
      "2209.98798828125\n",
      "Epoch: 1170 Train Loss: 3.391021, Acc: 1487.315\n",
      "Val Loss: 15.184372, Acc: 1902.999\n",
      "1902.99931640625\n",
      "Epoch: 1171 Train Loss: 3.081265, Acc: 1446.024\n",
      "Val Loss: 13.660380, Acc: 2417.438\n",
      "2417.43798828125\n",
      "Epoch: 1172 Train Loss: 3.538210, Acc: 1917.253\n",
      "Val Loss: 13.959210, Acc: 3205.374\n",
      "3205.37421875\n",
      "Epoch: 1173 Train Loss: 3.243054, Acc: 1960.875\n",
      "Val Loss: 13.088829, Acc: 1797.325\n",
      "1797.32451171875\n",
      "Epoch: 1174 Train Loss: 3.245928, Acc: 1622.761\n",
      "Val Loss: 14.974058, Acc: 2491.400\n",
      "2491.4\n",
      "Epoch: 1175 Train Loss: 3.396457, Acc: 1683.878\n",
      "Val Loss: 13.206184, Acc: 1811.693\n",
      "1811.692724609375\n",
      "Epoch: 1176 Train Loss: 3.062131, Acc: 1423.862\n",
      "Val Loss: 15.602061, Acc: 2765.309\n",
      "2765.30869140625\n",
      "Epoch: 1177 Train Loss: 3.690294, Acc: 1880.608\n",
      "Val Loss: 12.428653, Acc: 1736.861\n",
      "1736.861083984375\n",
      "1736.861083984375 1736.861083984375\n",
      "Epoch: 1178 Train Loss: 2.480164, Acc: 1222.816\n",
      "Val Loss: 14.028153, Acc: 1965.902\n",
      "1965.9021484375\n",
      "Epoch: 1179 Train Loss: 3.163488, Acc: 1416.773\n",
      "Val Loss: 14.676291, Acc: 1917.032\n",
      "1917.031640625\n",
      "Epoch: 1180 Train Loss: 4.096037, Acc: 1663.703\n",
      "Val Loss: 20.606239, Acc: 3362.864\n",
      "3362.8638671875\n",
      "Epoch: 1181 Train Loss: 3.856763, Acc: 1775.354\n",
      "Val Loss: 14.011660, Acc: 1871.871\n",
      "1871.870703125\n",
      "Epoch: 1182 Train Loss: 3.604440, Acc: 1575.672\n",
      "Val Loss: 18.000571, Acc: 3249.184\n",
      "3249.18408203125\n",
      "Epoch: 1183 Train Loss: 3.768201, Acc: 1706.520\n",
      "Val Loss: 14.835358, Acc: 2606.619\n",
      "2606.6189453125\n",
      "Epoch: 1184 Train Loss: 3.107135, Acc: 1591.957\n",
      "Val Loss: 12.950364, Acc: 1842.317\n",
      "1842.31650390625\n",
      "Epoch: 1185 Train Loss: 2.785290, Acc: 1386.749\n",
      "Val Loss: 15.790536, Acc: 2395.357\n",
      "2395.35673828125\n",
      "Epoch: 1186 Train Loss: 2.841894, Acc: 1435.300\n",
      "Val Loss: 13.069082, Acc: 2048.922\n",
      "2048.9220703125\n",
      "Epoch: 1187 Train Loss: 2.306877, Acc: 1266.091\n",
      "Val Loss: 13.303813, Acc: 1887.431\n",
      "1887.4314453125\n",
      "Epoch: 1188 Train Loss: 2.500320, Acc: 1169.079\n",
      "Val Loss: 13.526321, Acc: 1917.638\n",
      "1917.63798828125\n",
      "Epoch: 1189 Train Loss: 2.331231, Acc: 1188.907\n",
      "Val Loss: 13.394853, Acc: 1836.182\n",
      "1836.18203125\n",
      "Epoch: 1190 Train Loss: 2.820305, Acc: 1318.785\n",
      "Val Loss: 13.917292, Acc: 1743.166\n",
      "1743.165966796875\n",
      "Epoch: 1191 Train Loss: 3.023380, Acc: 1207.962\n",
      "Val Loss: 15.102855, Acc: 1990.892\n",
      "1990.8921875\n",
      "Epoch: 1192 Train Loss: 2.719257, Acc: 1162.618\n",
      "Val Loss: 13.745626, Acc: 1812.922\n",
      "1812.92158203125\n",
      "Epoch: 1193 Train Loss: 2.103885, Acc: 1058.571\n",
      "Val Loss: 13.255966, Acc: 1745.318\n",
      "1745.31767578125\n",
      "Epoch: 1194 Train Loss: 2.487724, Acc: 1103.300\n",
      "Val Loss: 15.888211, Acc: 2142.631\n",
      "2142.630859375\n",
      "Epoch: 1195 Train Loss: 3.755228, Acc: 1409.475\n",
      "Val Loss: 14.457366, Acc: 1880.312\n",
      "1880.31201171875\n",
      "Epoch: 1196 Train Loss: 5.379147, Acc: 2076.163\n",
      "Val Loss: 14.346324, Acc: 2474.152\n",
      "2474.15205078125\n",
      "Epoch: 1197 Train Loss: 4.815038, Acc: 1676.452\n",
      "Val Loss: 17.095938, Acc: 3253.128\n",
      "3253.128125\n",
      "Epoch: 1198 Train Loss: 5.325377, Acc: 2047.420\n",
      "Val Loss: 15.223394, Acc: 2367.944\n",
      "2367.94375\n",
      "Epoch: 1199 Train Loss: 4.454296, Acc: 1593.553\n",
      "Val Loss: 16.236973, Acc: 2562.531\n",
      "2562.5314453125\n",
      "Epoch: 1200 Train Loss: 3.736954, Acc: 1766.517\n",
      "Val Loss: 12.872830, Acc: 1836.431\n",
      "1836.431298828125\n",
      "Epoch: 1201 Train Loss: 2.502734, Acc: 1406.664\n",
      "Val Loss: 13.212991, Acc: 2042.030\n",
      "2042.02998046875\n",
      "Epoch: 1202 Train Loss: 2.281715, Acc: 1258.460\n",
      "Val Loss: 13.676754, Acc: 1944.128\n",
      "1944.128173828125\n",
      "Epoch: 1203 Train Loss: 2.399898, Acc: 1258.803\n",
      "Val Loss: 14.039938, Acc: 2279.503\n",
      "2279.5025390625\n",
      "Epoch: 1204 Train Loss: 4.533819, Acc: 1646.956\n",
      "Val Loss: 23.410001, Acc: 3852.935\n",
      "3852.934765625\n",
      "Epoch: 1205 Train Loss: 4.876019, Acc: 1823.493\n",
      "Val Loss: 15.149337, Acc: 2190.486\n",
      "2190.4857421875\n",
      "Epoch: 1206 Train Loss: 5.105643, Acc: 1741.041\n",
      "Val Loss: 18.284300, Acc: 3268.235\n",
      "3268.23515625\n",
      "Epoch: 1207 Train Loss: 4.120469, Acc: 1862.409\n",
      "Val Loss: 12.405057, Acc: 1834.640\n",
      "1834.64013671875\n",
      "Epoch: 1208 Train Loss: 3.007208, Acc: 1304.123\n",
      "Val Loss: 12.416302, Acc: 1837.229\n",
      "1837.2287109375\n",
      "Epoch: 1209 Train Loss: 2.616730, Acc: 1474.796\n",
      "Val Loss: 15.535532, Acc: 2656.270\n",
      "2656.2703125\n",
      "Epoch: 1210 Train Loss: 3.048798, Acc: 1462.409\n",
      "Val Loss: 14.713770, Acc: 2239.621\n",
      "2239.62080078125\n",
      "Epoch: 1211 Train Loss: 4.598419, Acc: 2033.066\n",
      "Val Loss: 16.530405, Acc: 2514.803\n",
      "2514.8033203125\n",
      "Epoch: 1212 Train Loss: 3.276951, Acc: 1585.982\n",
      "Val Loss: 14.409291, Acc: 2262.387\n",
      "2262.3873046875\n",
      "Epoch: 1213 Train Loss: 3.601162, Acc: 1753.266\n",
      "Val Loss: 15.208788, Acc: 2047.069\n",
      "2047.06865234375\n",
      "Epoch: 1214 Train Loss: 2.341523, Acc: 1271.967\n",
      "Val Loss: 14.207687, Acc: 1969.124\n",
      "1969.1244140625\n",
      "Epoch: 1215 Train Loss: 2.969128, Acc: 1247.708\n",
      "Val Loss: 14.258664, Acc: 2272.063\n",
      "2272.06328125\n",
      "Epoch: 1216 Train Loss: 2.381207, Acc: 1268.659\n",
      "Val Loss: 13.890950, Acc: 1795.536\n",
      "1795.535791015625\n",
      "Epoch: 1217 Train Loss: 2.752753, Acc: 1301.917\n",
      "Val Loss: 13.252870, Acc: 2145.576\n",
      "2145.57578125\n",
      "Epoch: 1218 Train Loss: 2.263981, Acc: 1293.078\n",
      "Val Loss: 14.239161, Acc: 2050.173\n",
      "2050.173046875\n",
      "Epoch: 1219 Train Loss: 2.441960, Acc: 1162.379\n",
      "Val Loss: 13.641721, Acc: 2324.266\n",
      "2324.26552734375\n",
      "Epoch: 1220 Train Loss: 2.807589, Acc: 1380.252\n",
      "Val Loss: 13.943317, Acc: 1826.093\n",
      "1826.0931640625\n",
      "Epoch: 1221 Train Loss: 2.199720, Acc: 1118.626\n",
      "Val Loss: 12.271132, Acc: 1747.531\n",
      "1747.53095703125\n",
      "Epoch: 1222 Train Loss: 2.382272, Acc: 1132.551\n",
      "Val Loss: 14.568065, Acc: 2046.518\n",
      "2046.51796875\n",
      "Epoch: 1223 Train Loss: 2.342936, Acc: 1236.473\n",
      "Val Loss: 12.713497, Acc: 1717.218\n",
      "1717.217529296875\n",
      "1717.217529296875 1717.217529296875\n",
      "Epoch: 1224 Train Loss: 2.223628, Acc: 1168.306\n",
      "Val Loss: 12.832516, Acc: 1858.806\n",
      "1858.80615234375\n",
      "Epoch: 1225 Train Loss: 2.101254, Acc: 1143.623\n",
      "Val Loss: 14.308073, Acc: 1836.210\n",
      "1836.20986328125\n",
      "Epoch: 1226 Train Loss: 2.031201, Acc: 1082.616\n",
      "Val Loss: 12.981976, Acc: 1661.688\n",
      "1661.687548828125\n",
      "1661.687548828125 1661.687548828125\n",
      "Epoch: 1227 Train Loss: 2.101166, Acc: 1089.052\n",
      "Val Loss: 12.722007, Acc: 1677.110\n",
      "1677.11044921875\n",
      "Epoch: 1228 Train Loss: 2.264754, Acc: 1109.991\n",
      "Val Loss: 12.929440, Acc: 1681.868\n",
      "1681.867626953125\n",
      "Epoch: 1229 Train Loss: 2.315116, Acc: 1257.656\n",
      "Val Loss: 13.575164, Acc: 1969.447\n",
      "1969.446875\n",
      "Epoch: 1230 Train Loss: 2.135956, Acc: 1103.897\n",
      "Val Loss: 12.129269, Acc: 1662.701\n",
      "1662.70087890625\n",
      "Epoch: 1231 Train Loss: 2.823180, Acc: 1225.269\n",
      "Val Loss: 13.177803, Acc: 1652.294\n",
      "1652.293505859375\n",
      "1652.293505859375 1652.293505859375\n",
      "Epoch: 1232 Train Loss: 2.031551, Acc: 1074.763\n",
      "Val Loss: 13.369976, Acc: 1908.627\n",
      "1908.62744140625\n",
      "Epoch: 1233 Train Loss: 2.348755, Acc: 1194.102\n",
      "Val Loss: 12.443310, Acc: 1668.954\n",
      "1668.954296875\n",
      "Epoch: 1234 Train Loss: 2.859076, Acc: 1187.270\n",
      "Val Loss: 12.505228, Acc: 1774.239\n",
      "1774.2392578125\n",
      "Epoch: 1235 Train Loss: 3.381909, Acc: 1378.432\n",
      "Val Loss: 14.367355, Acc: 1884.145\n",
      "1884.14501953125\n",
      "Epoch: 1236 Train Loss: 2.810046, Acc: 1360.550\n",
      "Val Loss: 13.920125, Acc: 2352.266\n",
      "2352.2662109375\n",
      "Epoch: 1237 Train Loss: 3.473057, Acc: 1826.871\n",
      "Val Loss: 12.195292, Acc: 1806.800\n",
      "1806.800390625\n",
      "Epoch: 1238 Train Loss: 3.150565, Acc: 1522.126\n",
      "Val Loss: 13.862635, Acc: 1872.068\n",
      "1872.068017578125\n",
      "Epoch: 1239 Train Loss: 2.497966, Acc: 1135.063\n",
      "Val Loss: 12.954137, Acc: 1788.615\n",
      "1788.614892578125\n",
      "Epoch: 1240 Train Loss: 2.167529, Acc: 1352.674\n",
      "Val Loss: 14.086197, Acc: 2113.720\n",
      "2113.7197265625\n",
      "Epoch: 1241 Train Loss: 2.246080, Acc: 1309.124\n",
      "Val Loss: 12.124685, Acc: 1738.567\n",
      "1738.56689453125\n",
      "Epoch: 1242 Train Loss: 2.227185, Acc: 1200.038\n",
      "Val Loss: 14.008130, Acc: 2083.650\n",
      "2083.6498046875\n",
      "Epoch: 1243 Train Loss: 2.385627, Acc: 1309.136\n",
      "Val Loss: 12.396887, Acc: 1651.258\n",
      "1651.2576171875\n",
      "1651.2576171875 1651.2576171875\n",
      "Epoch: 1244 Train Loss: 2.633511, Acc: 1328.952\n",
      "Val Loss: 13.745695, Acc: 1871.399\n",
      "1871.39853515625\n",
      "Epoch: 1245 Train Loss: 1.993331, Acc: 1068.479\n",
      "Val Loss: 11.983794, Acc: 1855.411\n",
      "1855.411474609375\n",
      "Epoch: 1246 Train Loss: 2.272548, Acc: 1331.734\n",
      "Val Loss: 15.075760, Acc: 1932.106\n",
      "1932.106201171875\n",
      "Epoch: 1247 Train Loss: 4.311770, Acc: 1463.477\n",
      "Val Loss: 13.800725, Acc: 1827.306\n",
      "1827.305859375\n",
      "Epoch: 1248 Train Loss: 3.605506, Acc: 1605.164\n",
      "Val Loss: 15.171761, Acc: 1955.769\n",
      "1955.769140625\n",
      "Epoch: 1249 Train Loss: 3.962480, Acc: 1480.401\n",
      "Val Loss: 12.673821, Acc: 1842.115\n",
      "1842.114990234375\n",
      "Epoch: 1250 Train Loss: 2.459938, Acc: 1343.381\n",
      "Val Loss: 13.702944, Acc: 2114.027\n",
      "2114.02744140625\n",
      "Epoch: 1251 Train Loss: 3.165574, Acc: 1593.354\n",
      "Val Loss: 13.782883, Acc: 1863.131\n",
      "1863.1306640625\n",
      "Epoch: 1252 Train Loss: 2.525408, Acc: 1554.495\n",
      "Val Loss: 15.137461, Acc: 2224.259\n",
      "2224.25859375\n",
      "Epoch: 1253 Train Loss: 2.425485, Acc: 1220.221\n",
      "Val Loss: 12.209514, Acc: 1735.500\n",
      "1735.500244140625\n",
      "Epoch: 1254 Train Loss: 1.986731, Acc: 1187.483\n",
      "Val Loss: 12.392566, Acc: 1697.853\n",
      "1697.8533203125\n",
      "Epoch: 1255 Train Loss: 2.007061, Acc: 1080.446\n",
      "Val Loss: 12.150072, Acc: 1602.035\n",
      "1602.034814453125\n",
      "1602.034814453125 1602.034814453125\n",
      "Epoch: 1256 Train Loss: 2.171078, Acc: 1024.825\n",
      "Val Loss: 14.935696, Acc: 1906.943\n",
      "1906.94306640625\n",
      "Epoch: 1257 Train Loss: 2.852372, Acc: 1297.390\n",
      "Val Loss: 12.755361, Acc: 2022.726\n",
      "2022.72568359375\n",
      "Epoch: 1258 Train Loss: 2.325280, Acc: 1271.344\n",
      "Val Loss: 13.902508, Acc: 2304.802\n",
      "2304.8015625\n",
      "Epoch: 1259 Train Loss: 2.534040, Acc: 1529.309\n",
      "Val Loss: 13.470984, Acc: 2091.044\n",
      "2091.04443359375\n",
      "Epoch: 1260 Train Loss: 2.657622, Acc: 1357.505\n",
      "Val Loss: 13.365458, Acc: 1773.041\n",
      "1773.041162109375\n",
      "Epoch: 1261 Train Loss: 2.289779, Acc: 1235.860\n",
      "Val Loss: 13.934056, Acc: 2108.066\n",
      "2108.0662109375\n",
      "Epoch: 1262 Train Loss: 2.989433, Acc: 1306.096\n",
      "Val Loss: 12.609793, Acc: 2271.567\n",
      "2271.56650390625\n",
      "Epoch: 1263 Train Loss: 3.088390, Acc: 1627.060\n",
      "Val Loss: 12.343930, Acc: 1898.219\n",
      "1898.21923828125\n",
      "Epoch: 1264 Train Loss: 3.438314, Acc: 1382.761\n",
      "Val Loss: 20.945085, Acc: 3170.306\n",
      "3170.30625\n",
      "Epoch: 1265 Train Loss: 4.255781, Acc: 1661.999\n",
      "Val Loss: 14.554638, Acc: 2179.179\n",
      "2179.17880859375\n",
      "Epoch: 1266 Train Loss: 3.330695, Acc: 1438.285\n",
      "Val Loss: 15.327086, Acc: 2364.499\n",
      "2364.49873046875\n",
      "Epoch: 1267 Train Loss: 3.120079, Acc: 1355.402\n",
      "Val Loss: 14.504009, Acc: 1836.892\n",
      "1836.891845703125\n",
      "Epoch: 1268 Train Loss: 2.448632, Acc: 1290.161\n",
      "Val Loss: 14.608777, Acc: 1904.068\n",
      "1904.0677734375\n",
      "Epoch: 1269 Train Loss: 2.657288, Acc: 1300.375\n",
      "Val Loss: 12.177910, Acc: 1787.471\n",
      "1787.471435546875\n",
      "Epoch: 1270 Train Loss: 2.321504, Acc: 1413.059\n",
      "Val Loss: 16.392781, Acc: 2770.174\n",
      "2770.173828125\n",
      "Epoch: 1271 Train Loss: 2.972502, Acc: 1461.920\n",
      "Val Loss: 12.254125, Acc: 1671.977\n",
      "1671.9767578125\n",
      "Epoch: 1272 Train Loss: 2.588883, Acc: 1174.906\n",
      "Val Loss: 12.443660, Acc: 1743.211\n",
      "1743.21142578125\n",
      "Epoch: 1273 Train Loss: 2.733287, Acc: 1410.410\n",
      "Val Loss: 12.598693, Acc: 1731.742\n",
      "1731.741845703125\n",
      "Epoch: 1274 Train Loss: 2.461322, Acc: 1332.746\n",
      "Val Loss: 12.650950, Acc: 1868.585\n",
      "1868.58525390625\n",
      "Epoch: 1275 Train Loss: 1.817301, Acc: 1194.020\n",
      "Val Loss: 13.015017, Acc: 2082.099\n",
      "2082.098828125\n",
      "Epoch: 1276 Train Loss: 1.858446, Acc: 1043.209\n",
      "Val Loss: 13.729978, Acc: 1772.830\n",
      "1772.8296875\n",
      "Epoch: 1277 Train Loss: 2.152042, Acc: 1095.926\n",
      "Val Loss: 12.867528, Acc: 1844.523\n",
      "1844.523486328125\n",
      "Epoch: 1278 Train Loss: 2.121005, Acc: 1267.769\n",
      "Val Loss: 12.028816, Acc: 1975.388\n",
      "1975.3876953125\n",
      "Epoch: 1279 Train Loss: 2.653066, Acc: 1157.276\n",
      "Val Loss: 12.002254, Acc: 1684.276\n",
      "1684.27587890625\n",
      "Epoch: 1280 Train Loss: 2.000990, Acc: 1074.111\n",
      "Val Loss: 12.331515, Acc: 1670.898\n",
      "1670.8982421875\n",
      "Epoch: 1281 Train Loss: 1.714151, Acc: 959.398\n",
      "Val Loss: 12.393231, Acc: 1602.755\n",
      "1602.7552734375\n",
      "Epoch: 1282 Train Loss: 1.759325, Acc: 929.458\n",
      "Val Loss: 14.294554, Acc: 1793.054\n",
      "1793.05419921875\n",
      "Epoch: 1283 Train Loss: 3.006941, Acc: 1367.160\n",
      "Val Loss: 13.177575, Acc: 1892.844\n",
      "1892.8435546875\n",
      "Epoch: 1284 Train Loss: 2.508818, Acc: 1141.611\n",
      "Val Loss: 11.869983, Acc: 1595.944\n",
      "1595.9443359375\n",
      "1595.9443359375 1595.9443359375\n",
      "Epoch: 1285 Train Loss: 1.985599, Acc: 1021.961\n",
      "Val Loss: 13.058125, Acc: 1737.676\n",
      "1737.6755859375\n",
      "Epoch: 1286 Train Loss: 1.818484, Acc: 1123.970\n",
      "Val Loss: 11.613570, Acc: 1548.918\n",
      "1548.918017578125\n",
      "1548.918017578125 1548.918017578125\n",
      "Epoch: 1287 Train Loss: 1.937178, Acc: 1020.220\n",
      "Val Loss: 14.184521, Acc: 2027.899\n",
      "2027.89921875\n",
      "Epoch: 1288 Train Loss: 2.115530, Acc: 1046.791\n",
      "Val Loss: 14.036554, Acc: 1823.056\n",
      "1823.05556640625\n",
      "Epoch: 1289 Train Loss: 2.385632, Acc: 1274.120\n",
      "Val Loss: 13.909824, Acc: 1859.520\n",
      "1859.520458984375\n",
      "Epoch: 1290 Train Loss: 2.511916, Acc: 1230.289\n",
      "Val Loss: 14.264034, Acc: 1860.994\n",
      "1860.99404296875\n",
      "Epoch: 1291 Train Loss: 2.299563, Acc: 1317.430\n",
      "Val Loss: 11.898671, Acc: 1736.796\n",
      "1736.796044921875\n",
      "Epoch: 1292 Train Loss: 2.440431, Acc: 1230.100\n",
      "Val Loss: 13.103531, Acc: 1856.611\n",
      "1856.611083984375\n",
      "Epoch: 1293 Train Loss: 2.499329, Acc: 1402.337\n",
      "Val Loss: 11.898613, Acc: 1586.503\n",
      "1586.503466796875\n",
      "Epoch: 1294 Train Loss: 1.904437, Acc: 998.886\n",
      "Val Loss: 11.886417, Acc: 1597.148\n",
      "1597.147998046875\n",
      "Epoch: 1295 Train Loss: 1.675061, Acc: 1008.373\n",
      "Val Loss: 11.803667, Acc: 1579.120\n",
      "1579.1201171875\n",
      "Epoch: 1296 Train Loss: 1.641619, Acc: 939.974\n",
      "Val Loss: 11.934572, Acc: 1629.362\n",
      "1629.3625\n",
      "Epoch: 1297 Train Loss: 1.899615, Acc: 1051.513\n",
      "Val Loss: 12.934718, Acc: 1664.980\n",
      "1664.98017578125\n",
      "Epoch: 1298 Train Loss: 1.892792, Acc: 1023.660\n",
      "Val Loss: 11.909027, Acc: 1658.288\n",
      "1658.287744140625\n",
      "Epoch: 1299 Train Loss: 1.905590, Acc: 1018.844\n",
      "Val Loss: 13.106114, Acc: 1804.222\n",
      "1804.22177734375\n",
      "Epoch: 1300 Train Loss: 2.155684, Acc: 1060.267\n",
      "Val Loss: 11.179561, Acc: 1520.988\n",
      "1520.98779296875\n",
      "1520.98779296875 1520.98779296875\n",
      "Epoch: 1301 Train Loss: 2.227229, Acc: 1078.414\n",
      "Val Loss: 12.445461, Acc: 1608.412\n",
      "1608.4125\n",
      "Epoch: 1302 Train Loss: 1.659189, Acc: 941.460\n",
      "Val Loss: 11.582065, Acc: 1546.576\n",
      "1546.5759765625\n",
      "Epoch: 1303 Train Loss: 1.815318, Acc: 1106.615\n",
      "Val Loss: 12.345211, Acc: 2001.450\n",
      "2001.4498046875\n",
      "Epoch: 1304 Train Loss: 2.210255, Acc: 1187.308\n",
      "Val Loss: 12.213764, Acc: 1595.385\n",
      "1595.3853515625\n",
      "Epoch: 1305 Train Loss: 2.139755, Acc: 1057.841\n",
      "Val Loss: 13.305133, Acc: 1835.931\n",
      "1835.93115234375\n",
      "Epoch: 1306 Train Loss: 2.143445, Acc: 1032.340\n",
      "Val Loss: 15.186806, Acc: 1869.640\n",
      "1869.6396484375\n",
      "Epoch: 1307 Train Loss: 2.264885, Acc: 1236.520\n",
      "Val Loss: 11.589263, Acc: 2080.591\n",
      "2080.59072265625\n",
      "Epoch: 1308 Train Loss: 2.114837, Acc: 1144.716\n",
      "Val Loss: 11.340213, Acc: 1586.504\n",
      "1586.50361328125\n",
      "Epoch: 1309 Train Loss: 2.092725, Acc: 1103.856\n",
      "Val Loss: 13.368930, Acc: 1820.470\n",
      "1820.469921875\n",
      "Epoch: 1310 Train Loss: 2.064149, Acc: 1111.684\n",
      "Val Loss: 12.926838, Acc: 1751.951\n",
      "1751.951220703125\n",
      "Epoch: 1311 Train Loss: 1.706485, Acc: 996.166\n",
      "Val Loss: 11.586485, Acc: 1786.685\n",
      "1786.6853515625\n",
      "Epoch: 1312 Train Loss: 1.869080, Acc: 1056.853\n",
      "Val Loss: 11.367025, Acc: 1523.038\n",
      "1523.03798828125\n",
      "Epoch: 1313 Train Loss: 1.701278, Acc: 974.275\n",
      "Val Loss: 12.719376, Acc: 1702.797\n",
      "1702.797021484375\n",
      "Epoch: 1314 Train Loss: 3.647929, Acc: 1363.780\n",
      "Val Loss: 13.281155, Acc: 1726.142\n",
      "1726.142333984375\n",
      "Epoch: 1315 Train Loss: 3.902500, Acc: 1653.491\n",
      "Val Loss: 13.364893, Acc: 2167.008\n",
      "2167.008203125\n",
      "Epoch: 1316 Train Loss: 3.232019, Acc: 1724.189\n",
      "Val Loss: 15.434678, Acc: 2846.538\n",
      "2846.53828125\n",
      "Epoch: 1317 Train Loss: 2.865289, Acc: 1558.089\n",
      "Val Loss: 12.114758, Acc: 1704.346\n",
      "1704.34599609375\n",
      "Epoch: 1318 Train Loss: 1.710312, Acc: 1228.269\n",
      "Val Loss: 13.495499, Acc: 2024.559\n",
      "2024.55859375\n",
      "Epoch: 1319 Train Loss: 2.229913, Acc: 1130.928\n",
      "Val Loss: 13.320196, Acc: 1852.510\n",
      "1852.51044921875\n",
      "Epoch: 1320 Train Loss: 1.764490, Acc: 1002.814\n",
      "Val Loss: 11.220546, Acc: 1788.914\n",
      "1788.914306640625\n",
      "Epoch: 1321 Train Loss: 2.514547, Acc: 1156.569\n",
      "Val Loss: 12.393173, Acc: 1745.005\n",
      "1745.005078125\n",
      "Epoch: 1322 Train Loss: 1.964285, Acc: 1146.225\n",
      "Val Loss: 11.979818, Acc: 1666.213\n",
      "1666.2126953125\n",
      "Epoch: 1323 Train Loss: 1.876952, Acc: 1044.181\n",
      "Val Loss: 13.605731, Acc: 1810.209\n",
      "1810.20927734375\n",
      "Epoch: 1324 Train Loss: 1.894661, Acc: 1003.616\n",
      "Val Loss: 12.135299, Acc: 1795.159\n",
      "1795.159423828125\n",
      "Epoch: 1325 Train Loss: 2.259325, Acc: 1094.332\n",
      "Val Loss: 12.160493, Acc: 1630.831\n",
      "1630.83095703125\n",
      "Epoch: 1326 Train Loss: 1.799373, Acc: 1019.628\n",
      "Val Loss: 12.941378, Acc: 1786.748\n",
      "1786.74814453125\n",
      "Epoch: 1327 Train Loss: 2.049117, Acc: 1066.331\n",
      "Val Loss: 11.090803, Acc: 1673.835\n",
      "1673.835205078125\n",
      "Epoch: 1328 Train Loss: 1.870897, Acc: 997.858\n",
      "Val Loss: 11.743404, Acc: 1632.937\n",
      "1632.93681640625\n",
      "Epoch: 1329 Train Loss: 2.252694, Acc: 1057.829\n",
      "Val Loss: 13.803117, Acc: 1832.676\n",
      "1832.6763671875\n",
      "Epoch: 1330 Train Loss: 2.921941, Acc: 1244.418\n",
      "Val Loss: 15.323111, Acc: 2032.502\n",
      "2032.501708984375\n",
      "Epoch: 1331 Train Loss: 2.540867, Acc: 1197.520\n",
      "Val Loss: 12.530323, Acc: 1759.378\n",
      "1759.37841796875\n",
      "Epoch: 1332 Train Loss: 2.471110, Acc: 1390.339\n",
      "Val Loss: 15.018875, Acc: 2239.793\n",
      "2239.793359375\n",
      "Epoch: 1333 Train Loss: 2.093621, Acc: 1276.162\n",
      "Val Loss: 11.699298, Acc: 1919.946\n",
      "1919.94619140625\n",
      "Epoch: 1334 Train Loss: 2.003542, Acc: 1161.498\n",
      "Val Loss: 13.010271, Acc: 1882.745\n",
      "1882.7451171875\n",
      "Epoch: 1335 Train Loss: 1.769624, Acc: 1108.513\n",
      "Val Loss: 11.911651, Acc: 1546.453\n",
      "1546.453125\n",
      "Epoch: 1336 Train Loss: 1.589673, Acc: 996.856\n",
      "Val Loss: 11.353812, Acc: 1619.784\n",
      "1619.78408203125\n",
      "Epoch: 1337 Train Loss: 1.778877, Acc: 1026.850\n",
      "Val Loss: 11.449778, Acc: 1466.503\n",
      "1466.502978515625\n",
      "1466.502978515625 1466.502978515625\n",
      "Epoch: 1338 Train Loss: 2.307617, Acc: 1096.551\n",
      "Val Loss: 11.939742, Acc: 1925.013\n",
      "1925.0125\n",
      "Epoch: 1339 Train Loss: 2.614165, Acc: 1365.843\n",
      "Val Loss: 12.980569, Acc: 1924.424\n",
      "1924.42421875\n",
      "Epoch: 1340 Train Loss: 1.965052, Acc: 1100.110\n",
      "Val Loss: 12.169963, Acc: 1634.892\n",
      "1634.891943359375\n",
      "Epoch: 1341 Train Loss: 1.768215, Acc: 1220.179\n",
      "Val Loss: 12.813314, Acc: 1737.426\n",
      "1737.42578125\n",
      "Epoch: 1342 Train Loss: 1.724510, Acc: 1049.586\n",
      "Val Loss: 11.562315, Acc: 1527.324\n",
      "1527.323828125\n",
      "Epoch: 1343 Train Loss: 1.533686, Acc: 988.577\n",
      "Val Loss: 11.296398, Acc: 1537.431\n",
      "1537.4310546875\n",
      "Epoch: 1344 Train Loss: 1.533915, Acc: 1058.624\n",
      "Val Loss: 12.435751, Acc: 1874.349\n",
      "1874.34892578125\n",
      "Epoch: 1345 Train Loss: 1.985982, Acc: 1137.525\n",
      "Val Loss: 11.777275, Acc: 2052.878\n",
      "2052.8783203125\n",
      "Epoch: 1346 Train Loss: 2.057767, Acc: 1474.208\n",
      "Val Loss: 12.353917, Acc: 2111.603\n",
      "2111.6025390625\n",
      "Epoch: 1347 Train Loss: 2.058297, Acc: 1409.886\n",
      "Val Loss: 11.097069, Acc: 2043.334\n",
      "2043.33369140625\n",
      "Epoch: 1348 Train Loss: 2.281384, Acc: 1261.137\n",
      "Val Loss: 12.402955, Acc: 2316.021\n",
      "2316.0208984375\n",
      "Epoch: 1349 Train Loss: 1.960538, Acc: 1398.276\n",
      "Val Loss: 11.643913, Acc: 1969.975\n",
      "1969.9751953125\n",
      "Epoch: 1350 Train Loss: 2.013136, Acc: 1225.395\n",
      "Val Loss: 14.603903, Acc: 2388.622\n",
      "2388.6224609375\n",
      "Epoch: 1351 Train Loss: 2.343676, Acc: 1136.640\n",
      "Val Loss: 12.251768, Acc: 1803.110\n",
      "1803.1095703125\n",
      "Epoch: 1352 Train Loss: 1.846240, Acc: 1147.847\n",
      "Val Loss: 12.371497, Acc: 1650.496\n",
      "1650.496337890625\n",
      "Epoch: 1353 Train Loss: 1.794770, Acc: 1107.169\n",
      "Val Loss: 13.475984, Acc: 1730.109\n",
      "1730.1087890625\n",
      "Epoch: 1354 Train Loss: 1.756248, Acc: 975.383\n",
      "Val Loss: 11.942062, Acc: 1619.744\n",
      "1619.743994140625\n",
      "Epoch: 1355 Train Loss: 1.987204, Acc: 1093.758\n",
      "Val Loss: 11.436397, Acc: 1523.376\n",
      "1523.3755859375\n",
      "Epoch: 1356 Train Loss: 2.463721, Acc: 1270.202\n",
      "Val Loss: 10.928481, Acc: 1593.942\n",
      "1593.94248046875\n",
      "Epoch: 1357 Train Loss: 3.242735, Acc: 1308.633\n",
      "Val Loss: 12.679189, Acc: 1664.557\n",
      "1664.55712890625\n",
      "Epoch: 1358 Train Loss: 3.527339, Acc: 1647.571\n",
      "Val Loss: 15.426661, Acc: 2151.856\n",
      "2151.855859375\n",
      "Epoch: 1359 Train Loss: 2.984104, Acc: 1267.141\n",
      "Val Loss: 12.191995, Acc: 1763.404\n",
      "1763.40400390625\n",
      "Epoch: 1360 Train Loss: 1.821290, Acc: 1085.118\n",
      "Val Loss: 11.647403, Acc: 1643.673\n",
      "1643.67275390625\n",
      "Epoch: 1361 Train Loss: 1.624886, Acc: 1006.290\n",
      "Val Loss: 11.694537, Acc: 1638.925\n",
      "1638.925439453125\n",
      "Epoch: 1362 Train Loss: 2.539287, Acc: 1194.775\n",
      "Val Loss: 11.659799, Acc: 1787.908\n",
      "1787.9076171875\n",
      "Epoch: 1363 Train Loss: 2.167533, Acc: 1292.824\n",
      "Val Loss: 11.951547, Acc: 2088.624\n",
      "2088.62373046875\n",
      "Epoch: 1364 Train Loss: 1.767100, Acc: 1168.181\n",
      "Val Loss: 11.210229, Acc: 1617.780\n",
      "1617.78037109375\n",
      "Epoch: 1365 Train Loss: 1.809352, Acc: 952.825\n",
      "Val Loss: 11.203305, Acc: 1549.250\n",
      "1549.25029296875\n",
      "Epoch: 1366 Train Loss: 1.517007, Acc: 927.090\n",
      "Val Loss: 14.457432, Acc: 1962.763\n",
      "1962.76259765625\n",
      "Epoch: 1367 Train Loss: 2.094272, Acc: 1075.945\n",
      "Val Loss: 11.716585, Acc: 1804.580\n",
      "1804.58046875\n",
      "Epoch: 1368 Train Loss: 2.359996, Acc: 1172.519\n",
      "Val Loss: 11.667222, Acc: 1775.422\n",
      "1775.422314453125\n",
      "Epoch: 1369 Train Loss: 1.967088, Acc: 1079.801\n",
      "Val Loss: 13.968663, Acc: 1771.132\n",
      "1771.131982421875\n",
      "Epoch: 1370 Train Loss: 2.648476, Acc: 1096.066\n",
      "Val Loss: 11.413380, Acc: 1532.140\n",
      "1532.13955078125\n",
      "Epoch: 1371 Train Loss: 2.487628, Acc: 1219.600\n",
      "Val Loss: 11.546502, Acc: 1527.102\n",
      "1527.10244140625\n",
      "Epoch: 1372 Train Loss: 2.161178, Acc: 1109.529\n",
      "Val Loss: 19.610733, Acc: 2718.513\n",
      "2718.51279296875\n",
      "Epoch: 1373 Train Loss: 3.670072, Acc: 1699.081\n",
      "Val Loss: 11.823497, Acc: 1594.524\n",
      "1594.52373046875\n",
      "Epoch: 1374 Train Loss: 2.442334, Acc: 1513.714\n",
      "Val Loss: 11.621958, Acc: 1962.420\n",
      "1962.42041015625\n",
      "Epoch: 1375 Train Loss: 2.169365, Acc: 1419.008\n",
      "Val Loss: 12.355019, Acc: 2340.968\n",
      "2340.968359375\n",
      "Epoch: 1376 Train Loss: 1.704817, Acc: 1156.791\n",
      "Val Loss: 12.484304, Acc: 1894.265\n",
      "1894.2654296875\n",
      "Epoch: 1377 Train Loss: 1.547782, Acc: 1038.749\n",
      "Val Loss: 11.470065, Acc: 1577.548\n",
      "1577.548388671875\n",
      "Epoch: 1378 Train Loss: 1.879922, Acc: 1024.829\n",
      "Val Loss: 11.809294, Acc: 1640.127\n",
      "1640.12666015625\n",
      "Epoch: 1379 Train Loss: 2.286456, Acc: 1281.019\n",
      "Val Loss: 14.198536, Acc: 1864.092\n",
      "1864.0921875\n",
      "Epoch: 1380 Train Loss: 1.770656, Acc: 982.678\n",
      "Val Loss: 11.095710, Acc: 1487.530\n",
      "1487.52998046875\n",
      "Epoch: 1381 Train Loss: 2.451447, Acc: 1165.151\n",
      "Val Loss: 12.524921, Acc: 1639.622\n",
      "1639.621875\n",
      "Epoch: 1382 Train Loss: 2.577681, Acc: 1168.397\n",
      "Val Loss: 12.772618, Acc: 1639.068\n",
      "1639.0681640625\n",
      "Epoch: 1383 Train Loss: 1.803920, Acc: 962.416\n",
      "Val Loss: 12.053063, Acc: 1579.498\n",
      "1579.498193359375\n",
      "Epoch: 1384 Train Loss: 1.743230, Acc: 1040.548\n",
      "Val Loss: 11.733931, Acc: 1660.574\n",
      "1660.57373046875\n",
      "Epoch: 1385 Train Loss: 1.533464, Acc: 923.395\n",
      "Val Loss: 12.977127, Acc: 1880.665\n",
      "1880.665380859375\n",
      "Epoch: 1386 Train Loss: 1.548924, Acc: 1035.718\n",
      "Val Loss: 12.362295, Acc: 1944.555\n",
      "1944.555078125\n",
      "Epoch: 1387 Train Loss: 1.473554, Acc: 1116.409\n",
      "Val Loss: 11.984431, Acc: 1794.722\n",
      "1794.721826171875\n",
      "Epoch: 1388 Train Loss: 1.888159, Acc: 1065.892\n",
      "Val Loss: 10.796069, Acc: 1421.622\n",
      "1421.62177734375\n",
      "1421.62177734375 1421.62177734375\n",
      "Epoch: 1389 Train Loss: 2.202499, Acc: 1071.131\n",
      "Val Loss: 11.341047, Acc: 1844.909\n",
      "1844.9087890625\n",
      "Epoch: 1390 Train Loss: 2.114717, Acc: 1180.350\n",
      "Val Loss: 13.571978, Acc: 1792.237\n",
      "1792.236865234375\n",
      "Epoch: 1391 Train Loss: 2.296743, Acc: 1045.954\n",
      "Val Loss: 14.732703, Acc: 2033.042\n",
      "2033.04189453125\n",
      "Epoch: 1392 Train Loss: 2.424275, Acc: 1159.469\n",
      "Val Loss: 11.903136, Acc: 1699.368\n",
      "1699.368115234375\n",
      "Epoch: 1393 Train Loss: 1.511440, Acc: 911.817\n",
      "Val Loss: 12.083603, Acc: 1608.461\n",
      "1608.460546875\n",
      "Epoch: 1394 Train Loss: 1.471535, Acc: 921.671\n",
      "Val Loss: 11.176308, Acc: 1472.378\n",
      "1472.3779296875\n",
      "Epoch: 1395 Train Loss: 1.381745, Acc: 937.382\n",
      "Val Loss: 11.175484, Acc: 1514.283\n",
      "1514.282666015625\n",
      "Epoch: 1396 Train Loss: 1.573121, Acc: 967.483\n",
      "Val Loss: 10.894330, Acc: 1495.589\n",
      "1495.588916015625\n",
      "Epoch: 1397 Train Loss: 1.955605, Acc: 1221.444\n",
      "Val Loss: 12.637324, Acc: 2242.607\n",
      "2242.60654296875\n",
      "Epoch: 1398 Train Loss: 1.698776, Acc: 1093.123\n",
      "Val Loss: 10.969543, Acc: 1468.279\n",
      "1468.2787109375\n",
      "Epoch: 1399 Train Loss: 2.686312, Acc: 1276.381\n",
      "Val Loss: 11.655757, Acc: 1554.215\n",
      "1554.21494140625\n",
      "Epoch: 1400 Train Loss: 2.955155, Acc: 1274.000\n",
      "Val Loss: 15.789838, Acc: 2872.171\n",
      "2872.17099609375\n",
      "Epoch: 1401 Train Loss: 4.052535, Acc: 2084.096\n",
      "Val Loss: 16.434265, Acc: 2764.055\n",
      "2764.0552734375\n",
      "Epoch: 1402 Train Loss: 2.718075, Acc: 1575.469\n",
      "Val Loss: 11.931845, Acc: 1909.237\n",
      "1909.2375\n",
      "Epoch: 1403 Train Loss: 2.074285, Acc: 1200.614\n",
      "Val Loss: 14.339310, Acc: 2220.072\n",
      "2220.07216796875\n",
      "Epoch: 1404 Train Loss: 1.978771, Acc: 1158.714\n",
      "Val Loss: 11.799632, Acc: 1561.669\n",
      "1561.669287109375\n",
      "Epoch: 1405 Train Loss: 1.527977, Acc: 976.446\n",
      "Val Loss: 10.847763, Acc: 1513.560\n",
      "1513.559521484375\n",
      "Epoch: 1406 Train Loss: 1.746820, Acc: 987.251\n",
      "Val Loss: 15.000321, Acc: 1801.536\n",
      "1801.5361328125\n",
      "Epoch: 1407 Train Loss: 2.362173, Acc: 1171.224\n",
      "Val Loss: 11.128444, Acc: 1576.267\n",
      "1576.266845703125\n",
      "Epoch: 1408 Train Loss: 2.131999, Acc: 1028.444\n",
      "Val Loss: 11.686600, Acc: 1739.158\n",
      "1739.158203125\n",
      "Epoch: 1409 Train Loss: 3.148453, Acc: 1199.130\n",
      "Val Loss: 13.262404, Acc: 1835.840\n",
      "1835.83955078125\n",
      "Epoch: 1410 Train Loss: 3.255583, Acc: 1213.937\n",
      "Val Loss: 14.903564, Acc: 1839.480\n",
      "1839.480078125\n",
      "Epoch: 1411 Train Loss: 3.500203, Acc: 1314.186\n",
      "Val Loss: 12.269834, Acc: 1702.714\n",
      "1702.71435546875\n",
      "Epoch: 1412 Train Loss: 2.603637, Acc: 1237.959\n",
      "Val Loss: 15.482565, Acc: 2214.629\n",
      "2214.6287109375\n",
      "Epoch: 1413 Train Loss: 2.567958, Acc: 1160.838\n",
      "Val Loss: 11.206475, Acc: 1626.836\n",
      "1626.8359375\n",
      "Epoch: 1414 Train Loss: 1.945049, Acc: 1034.465\n",
      "Val Loss: 11.368173, Acc: 1560.194\n",
      "1560.194091796875\n",
      "Epoch: 1415 Train Loss: 1.533745, Acc: 912.028\n",
      "Val Loss: 12.126316, Acc: 1581.537\n",
      "1581.53740234375\n",
      "Epoch: 1416 Train Loss: 1.464368, Acc: 850.626\n",
      "Val Loss: 11.153059, Acc: 1494.418\n",
      "1494.4181640625\n",
      "Epoch: 1417 Train Loss: 1.967920, Acc: 1079.609\n",
      "Val Loss: 14.937724, Acc: 2105.835\n",
      "2105.83525390625\n",
      "Epoch: 1418 Train Loss: 2.364451, Acc: 1142.785\n",
      "Val Loss: 12.762707, Acc: 1738.050\n",
      "1738.050048828125\n",
      "Epoch: 1419 Train Loss: 1.740461, Acc: 994.204\n",
      "Val Loss: 11.689420, Acc: 1759.195\n",
      "1759.194921875\n",
      "Epoch: 1420 Train Loss: 1.283359, Acc: 990.243\n",
      "Val Loss: 11.537953, Acc: 1706.243\n",
      "1706.242578125\n",
      "Epoch: 1421 Train Loss: 1.880531, Acc: 1062.884\n",
      "Val Loss: 11.071063, Acc: 1645.287\n",
      "1645.28681640625\n",
      "Epoch: 1422 Train Loss: 2.134638, Acc: 1115.059\n",
      "Val Loss: 11.429601, Acc: 1515.654\n",
      "1515.6544921875\n",
      "Epoch: 1423 Train Loss: 1.887218, Acc: 1044.513\n",
      "Val Loss: 12.859947, Acc: 1678.256\n",
      "1678.25634765625\n",
      "Epoch: 1424 Train Loss: 1.648886, Acc: 934.343\n",
      "Val Loss: 10.428207, Acc: 1483.948\n",
      "1483.948046875\n",
      "Epoch: 1425 Train Loss: 1.644137, Acc: 961.578\n",
      "Val Loss: 10.943937, Acc: 1808.923\n",
      "1808.92275390625\n",
      "Epoch: 1426 Train Loss: 1.745624, Acc: 973.375\n",
      "Val Loss: 11.773465, Acc: 1715.500\n",
      "1715.50009765625\n",
      "Epoch: 1427 Train Loss: 1.268137, Acc: 883.767\n",
      "Val Loss: 11.908821, Acc: 1536.506\n",
      "1536.506005859375\n",
      "Epoch: 1428 Train Loss: 1.377493, Acc: 801.770\n",
      "Val Loss: 10.956575, Acc: 1545.925\n",
      "1545.92451171875\n",
      "Epoch: 1429 Train Loss: 1.384908, Acc: 913.887\n",
      "Val Loss: 11.652818, Acc: 1582.416\n",
      "1582.416162109375\n",
      "Epoch: 1430 Train Loss: 1.570606, Acc: 900.809\n",
      "Val Loss: 11.525110, Acc: 1454.965\n",
      "1454.965380859375\n",
      "Epoch: 1431 Train Loss: 1.820729, Acc: 925.815\n",
      "Val Loss: 10.607334, Acc: 1534.900\n",
      "1534.8998046875\n",
      "Epoch: 1432 Train Loss: 1.514384, Acc: 924.070\n",
      "Val Loss: 14.611824, Acc: 1782.382\n",
      "1782.38203125\n",
      "Epoch: 1433 Train Loss: 2.169892, Acc: 1179.077\n",
      "Val Loss: 10.673273, Acc: 2107.755\n",
      "2107.7546875\n",
      "Epoch: 1434 Train Loss: 2.450199, Acc: 1210.664\n",
      "Val Loss: 11.115242, Acc: 1752.014\n",
      "1752.014306640625\n",
      "Epoch: 1435 Train Loss: 1.920929, Acc: 1090.230\n",
      "Val Loss: 13.024511, Acc: 1702.656\n",
      "1702.65615234375\n",
      "Epoch: 1436 Train Loss: 1.563865, Acc: 891.426\n",
      "Val Loss: 11.241252, Acc: 1427.491\n",
      "1427.49052734375\n",
      "Epoch: 1437 Train Loss: 1.589623, Acc: 924.262\n",
      "Val Loss: 11.578958, Acc: 1601.905\n",
      "1601.90498046875\n",
      "Epoch: 1438 Train Loss: 1.427153, Acc: 984.197\n",
      "Val Loss: 10.977612, Acc: 1563.875\n",
      "1563.875048828125\n",
      "Epoch: 1439 Train Loss: 1.329686, Acc: 817.628\n",
      "Val Loss: 11.343069, Acc: 1442.759\n",
      "1442.75859375\n",
      "Epoch: 1440 Train Loss: 1.381687, Acc: 846.990\n",
      "Val Loss: 10.380803, Acc: 1483.597\n",
      "1483.5966796875\n",
      "Epoch: 1441 Train Loss: 1.368944, Acc: 844.634\n",
      "Val Loss: 11.739516, Acc: 1515.668\n",
      "1515.66787109375\n",
      "Epoch: 1442 Train Loss: 1.199349, Acc: 821.960\n",
      "Val Loss: 11.084188, Acc: 1448.802\n",
      "1448.80224609375\n",
      "Epoch: 1443 Train Loss: 1.137194, Acc: 793.143\n",
      "Val Loss: 11.018984, Acc: 1393.660\n",
      "1393.65986328125\n",
      "1393.65986328125 1393.65986328125\n",
      "Epoch: 1444 Train Loss: 1.321052, Acc: 825.132\n",
      "Val Loss: 13.466310, Acc: 1709.514\n",
      "1709.51435546875\n",
      "Epoch: 1445 Train Loss: 1.875628, Acc: 995.167\n",
      "Val Loss: 10.846177, Acc: 1706.058\n",
      "1706.057861328125\n",
      "Epoch: 1446 Train Loss: 1.856409, Acc: 1118.075\n",
      "Val Loss: 11.346380, Acc: 1877.658\n",
      "1877.658203125\n",
      "Epoch: 1447 Train Loss: 1.593222, Acc: 1216.075\n",
      "Val Loss: 12.177512, Acc: 2224.440\n",
      "2224.4400390625\n",
      "Epoch: 1448 Train Loss: 1.970180, Acc: 1455.394\n",
      "Val Loss: 15.855263, Acc: 2495.816\n",
      "2495.815625\n",
      "Epoch: 1449 Train Loss: 2.740809, Acc: 1605.320\n",
      "Val Loss: 13.030053, Acc: 2789.000\n",
      "2789.0001953125\n",
      "Epoch: 1450 Train Loss: 2.247009, Acc: 1236.265\n",
      "Val Loss: 11.232900, Acc: 1606.543\n",
      "1606.5431640625\n",
      "Epoch: 1451 Train Loss: 2.060492, Acc: 1030.104\n",
      "Val Loss: 12.154916, Acc: 1767.792\n",
      "1767.79228515625\n",
      "Epoch: 1452 Train Loss: 2.084768, Acc: 1344.100\n",
      "Val Loss: 11.200420, Acc: 1952.799\n",
      "1952.79931640625\n",
      "Epoch: 1453 Train Loss: 1.543803, Acc: 1041.147\n",
      "Val Loss: 10.936259, Acc: 1545.210\n",
      "1545.21005859375\n",
      "Epoch: 1454 Train Loss: 1.218422, Acc: 809.692\n",
      "Val Loss: 11.124967, Acc: 1429.502\n",
      "1429.501953125\n",
      "Epoch: 1455 Train Loss: 1.409699, Acc: 824.539\n",
      "Val Loss: 12.144854, Acc: 1626.640\n",
      "1626.64033203125\n",
      "Epoch: 1456 Train Loss: 1.782075, Acc: 1119.083\n",
      "Val Loss: 12.873237, Acc: 1829.550\n",
      "1829.5498046875\n",
      "Epoch: 1457 Train Loss: 1.365961, Acc: 937.089\n",
      "Val Loss: 11.102524, Acc: 1565.000\n",
      "1564.999658203125\n",
      "Epoch: 1458 Train Loss: 1.299412, Acc: 820.002\n",
      "Val Loss: 10.562803, Acc: 1365.691\n",
      "1365.69091796875\n",
      "1365.69091796875 1365.69091796875\n",
      "Epoch: 1459 Train Loss: 1.429657, Acc: 1043.074\n",
      "Val Loss: 12.903607, Acc: 2140.460\n",
      "2140.46044921875\n",
      "Epoch: 1460 Train Loss: 2.638158, Acc: 1237.442\n",
      "Val Loss: 13.173362, Acc: 1581.734\n",
      "1581.73427734375\n",
      "Epoch: 1461 Train Loss: 2.675547, Acc: 1122.547\n",
      "Val Loss: 11.935982, Acc: 1877.295\n",
      "1877.2947265625\n",
      "Epoch: 1462 Train Loss: 1.977935, Acc: 1294.759\n",
      "Val Loss: 10.955382, Acc: 1477.122\n",
      "1477.12236328125\n",
      "Epoch: 1463 Train Loss: 1.514629, Acc: 1105.813\n",
      "Val Loss: 11.715574, Acc: 1554.339\n",
      "1554.339453125\n",
      "Epoch: 1464 Train Loss: 1.422718, Acc: 1015.632\n",
      "Val Loss: 11.464734, Acc: 1507.990\n",
      "1507.989501953125\n",
      "Epoch: 1465 Train Loss: 1.336488, Acc: 941.777\n",
      "Val Loss: 11.078852, Acc: 1575.626\n",
      "1575.6259765625\n",
      "Epoch: 1466 Train Loss: 1.263279, Acc: 919.031\n",
      "Val Loss: 11.461338, Acc: 1769.004\n",
      "1769.00419921875\n",
      "Epoch: 1467 Train Loss: 1.182907, Acc: 916.946\n",
      "Val Loss: 10.362964, Acc: 1541.685\n",
      "1541.6853515625\n",
      "Epoch: 1468 Train Loss: 1.293623, Acc: 895.039\n",
      "Val Loss: 10.883318, Acc: 1375.163\n",
      "1375.16298828125\n",
      "Epoch: 1469 Train Loss: 1.182886, Acc: 775.676\n",
      "Val Loss: 10.845285, Acc: 1418.430\n",
      "1418.4298828125\n",
      "Epoch: 1470 Train Loss: 1.437376, Acc: 864.530\n",
      "Val Loss: 12.589232, Acc: 1551.015\n",
      "1551.01484375\n",
      "Epoch: 1471 Train Loss: 1.809676, Acc: 1031.881\n",
      "Val Loss: 12.311549, Acc: 1720.287\n",
      "1720.28662109375\n",
      "Epoch: 1472 Train Loss: 1.706402, Acc: 1199.827\n",
      "Val Loss: 10.978331, Acc: 1563.527\n",
      "1563.52724609375\n",
      "Epoch: 1473 Train Loss: 1.468109, Acc: 1159.856\n",
      "Val Loss: 10.595589, Acc: 1471.323\n",
      "1471.32265625\n",
      "Epoch: 1474 Train Loss: 1.495977, Acc: 1027.927\n",
      "Val Loss: 11.089801, Acc: 2781.626\n",
      "2781.6263671875\n",
      "Epoch: 1475 Train Loss: 4.605144, Acc: 1430.522\n",
      "Val Loss: 11.919925, Acc: 2275.998\n",
      "2275.9978515625\n",
      "Epoch: 1476 Train Loss: 3.616625, Acc: 1508.710\n",
      "Val Loss: 16.639273, Acc: 2477.800\n",
      "2477.80048828125\n",
      "Epoch: 1477 Train Loss: 3.739893, Acc: 1422.530\n",
      "Val Loss: 11.000665, Acc: 1893.213\n",
      "1893.212890625\n",
      "Epoch: 1478 Train Loss: 3.703411, Acc: 1476.133\n",
      "Val Loss: 14.694518, Acc: 1831.385\n",
      "1831.38505859375\n",
      "Epoch: 1479 Train Loss: 2.439106, Acc: 1294.333\n",
      "Val Loss: 11.877907, Acc: 1897.198\n",
      "1897.1978515625\n",
      "Epoch: 1480 Train Loss: 1.775734, Acc: 1037.511\n",
      "Val Loss: 11.114967, Acc: 1585.927\n",
      "1585.92666015625\n",
      "Epoch: 1481 Train Loss: 1.441222, Acc: 896.746\n",
      "Val Loss: 11.518375, Acc: 1529.563\n",
      "1529.563232421875\n",
      "Epoch: 1482 Train Loss: 1.253463, Acc: 846.530\n",
      "Val Loss: 11.246446, Acc: 1472.547\n",
      "1472.547412109375\n",
      "Epoch: 1483 Train Loss: 1.213694, Acc: 778.817\n",
      "Val Loss: 10.641522, Acc: 1373.193\n",
      "1373.193115234375\n",
      "Epoch: 1484 Train Loss: 1.331003, Acc: 861.931\n",
      "Val Loss: 10.716925, Acc: 1436.367\n",
      "1436.36728515625\n",
      "Epoch: 1485 Train Loss: 1.348358, Acc: 885.068\n",
      "Val Loss: 12.662741, Acc: 1592.963\n",
      "1592.9626953125\n",
      "Epoch: 1486 Train Loss: 1.704044, Acc: 1018.033\n",
      "Val Loss: 11.346518, Acc: 1755.719\n",
      "1755.719189453125\n",
      "Epoch: 1487 Train Loss: 1.576633, Acc: 964.147\n",
      "Val Loss: 11.797714, Acc: 1726.452\n",
      "1726.45224609375\n",
      "Epoch: 1488 Train Loss: 1.305355, Acc: 964.988\n",
      "Val Loss: 11.364887, Acc: 1557.157\n",
      "1557.15732421875\n",
      "Epoch: 1489 Train Loss: 1.327589, Acc: 851.801\n",
      "Val Loss: 12.834171, Acc: 1615.135\n",
      "1615.134765625\n",
      "Epoch: 1490 Train Loss: 1.568764, Acc: 986.819\n",
      "Val Loss: 11.469797, Acc: 1658.223\n",
      "1658.22255859375\n",
      "Epoch: 1491 Train Loss: 1.304730, Acc: 901.238\n",
      "Val Loss: 10.850182, Acc: 1496.644\n",
      "1496.644482421875\n",
      "Epoch: 1492 Train Loss: 1.258912, Acc: 993.368\n",
      "Val Loss: 11.834528, Acc: 1603.607\n",
      "1603.6068359375\n",
      "Epoch: 1493 Train Loss: 2.135814, Acc: 1251.441\n",
      "Val Loss: 13.252679, Acc: 1683.309\n",
      "1683.309375\n",
      "Epoch: 1494 Train Loss: 2.543385, Acc: 1086.417\n",
      "Val Loss: 13.678380, Acc: 1678.197\n",
      "1678.196923828125\n",
      "Epoch: 1495 Train Loss: 1.871004, Acc: 985.224\n",
      "Val Loss: 11.620059, Acc: 1536.384\n",
      "1536.38447265625\n",
      "Epoch: 1496 Train Loss: 1.617904, Acc: 938.562\n",
      "Val Loss: 11.509608, Acc: 1664.680\n",
      "1664.680126953125\n",
      "Epoch: 1497 Train Loss: 1.643738, Acc: 971.605\n",
      "Val Loss: 12.077863, Acc: 1667.583\n",
      "1667.58271484375\n",
      "Epoch: 1498 Train Loss: 1.644161, Acc: 946.610\n",
      "Val Loss: 9.900925, Acc: 1372.114\n",
      "1372.114453125\n",
      "Epoch: 1499 Train Loss: 1.560371, Acc: 869.996\n",
      "Val Loss: 15.726750, Acc: 1861.920\n",
      "1861.919775390625\n",
      "Epoch: 1500 Train Loss: 2.141683, Acc: 1158.119\n",
      "Val Loss: 10.927548, Acc: 1851.822\n",
      "1851.82158203125\n",
      "Epoch: 1501 Train Loss: 1.570882, Acc: 1013.968\n",
      "Val Loss: 10.942489, Acc: 1492.938\n",
      "1492.938330078125\n",
      "Epoch: 1502 Train Loss: 1.470176, Acc: 873.742\n",
      "Val Loss: 11.289453, Acc: 1407.603\n",
      "1407.6033203125\n",
      "Epoch: 1503 Train Loss: 1.791436, Acc: 861.273\n",
      "Val Loss: 13.115512, Acc: 1566.830\n",
      "1566.829931640625\n",
      "Epoch: 1504 Train Loss: 2.023528, Acc: 1015.854\n",
      "Val Loss: 11.540546, Acc: 1508.179\n",
      "1508.17890625\n",
      "Epoch: 1505 Train Loss: 2.857632, Acc: 1178.672\n",
      "Val Loss: 13.874713, Acc: 1878.182\n",
      "1878.182421875\n",
      "Epoch: 1506 Train Loss: 2.575828, Acc: 1216.276\n",
      "Val Loss: 13.672395, Acc: 1578.809\n",
      "1578.8087890625\n",
      "Epoch: 1507 Train Loss: 1.365826, Acc: 792.638\n",
      "Val Loss: 10.206393, Acc: 1389.817\n",
      "1389.8166015625\n",
      "Epoch: 1508 Train Loss: 1.519335, Acc: 868.521\n",
      "Val Loss: 12.837933, Acc: 1555.476\n",
      "1555.476171875\n",
      "Epoch: 1509 Train Loss: 1.414367, Acc: 1039.332\n",
      "Val Loss: 10.823219, Acc: 1439.326\n",
      "1439.32578125\n",
      "Epoch: 1510 Train Loss: 1.259958, Acc: 983.152\n",
      "Val Loss: 11.909419, Acc: 1568.643\n",
      "1568.643017578125\n",
      "Epoch: 1511 Train Loss: 1.373172, Acc: 861.244\n",
      "Val Loss: 11.749636, Acc: 1509.658\n",
      "1509.65771484375\n",
      "Epoch: 1512 Train Loss: 1.301955, Acc: 838.836\n",
      "Val Loss: 11.448816, Acc: 1554.361\n",
      "1554.36142578125\n",
      "Epoch: 1513 Train Loss: 1.152902, Acc: 874.638\n",
      "Val Loss: 10.623515, Acc: 1453.748\n",
      "1453.74794921875\n",
      "Epoch: 1514 Train Loss: 1.117325, Acc: 824.395\n",
      "Val Loss: 10.292710, Acc: 1310.676\n",
      "1310.676318359375\n",
      "1310.676318359375 1310.676318359375\n",
      "Epoch: 1515 Train Loss: 1.269757, Acc: 790.331\n",
      "Val Loss: 10.833692, Acc: 1377.662\n",
      "1377.6619140625\n",
      "Epoch: 1516 Train Loss: 1.157654, Acc: 764.851\n",
      "Val Loss: 11.452569, Acc: 1417.334\n",
      "1417.33447265625\n",
      "Epoch: 1517 Train Loss: 1.188170, Acc: 787.927\n",
      "Val Loss: 10.412094, Acc: 1396.133\n",
      "1396.133203125\n",
      "Epoch: 1518 Train Loss: 1.221866, Acc: 752.649\n",
      "Val Loss: 10.705191, Acc: 1364.766\n",
      "1364.76640625\n",
      "Epoch: 1519 Train Loss: 1.636096, Acc: 942.389\n",
      "Val Loss: 13.883142, Acc: 2168.319\n",
      "2168.31865234375\n",
      "Epoch: 1520 Train Loss: 1.294216, Acc: 979.061\n",
      "Val Loss: 10.762973, Acc: 1502.594\n",
      "1502.594140625\n",
      "Epoch: 1521 Train Loss: 1.292625, Acc: 889.958\n",
      "Val Loss: 10.471225, Acc: 1293.858\n",
      "1293.85771484375\n",
      "1293.85771484375 1293.85771484375\n",
      "Epoch: 1522 Train Loss: 1.035215, Acc: 729.467\n",
      "Val Loss: 11.262707, Acc: 1377.484\n",
      "1377.48388671875\n",
      "Epoch: 1523 Train Loss: 1.045398, Acc: 765.477\n",
      "Val Loss: 10.914536, Acc: 1400.675\n",
      "1400.675\n",
      "Epoch: 1524 Train Loss: 1.040760, Acc: 759.540\n",
      "Val Loss: 10.314449, Acc: 1297.980\n",
      "1297.979931640625\n",
      "Epoch: 1525 Train Loss: 1.053520, Acc: 808.462\n",
      "Val Loss: 10.390011, Acc: 1313.127\n",
      "1313.1267578125\n",
      "Epoch: 1526 Train Loss: 1.063066, Acc: 768.957\n",
      "Val Loss: 10.797713, Acc: 1317.126\n",
      "1317.125634765625\n",
      "Epoch: 1527 Train Loss: 1.015823, Acc: 740.121\n",
      "Val Loss: 10.282997, Acc: 1404.661\n",
      "1404.660986328125\n",
      "Epoch: 1528 Train Loss: 1.198719, Acc: 807.851\n",
      "Val Loss: 14.131896, Acc: 1649.096\n",
      "1649.0958984375\n",
      "Epoch: 1529 Train Loss: 1.937616, Acc: 965.816\n",
      "Val Loss: 14.228345, Acc: 2022.764\n",
      "2022.7640625\n",
      "Epoch: 1530 Train Loss: 1.622163, Acc: 984.999\n",
      "Val Loss: 10.922244, Acc: 1543.596\n",
      "1543.595703125\n",
      "Epoch: 1531 Train Loss: 1.139274, Acc: 876.460\n",
      "Val Loss: 10.556218, Acc: 1329.112\n",
      "1329.11171875\n",
      "Epoch: 1532 Train Loss: 1.233189, Acc: 806.313\n",
      "Val Loss: 10.713595, Acc: 1329.215\n",
      "1329.21513671875\n",
      "Epoch: 1533 Train Loss: 1.066358, Acc: 758.497\n",
      "Val Loss: 11.112251, Acc: 1339.774\n",
      "1339.773583984375\n",
      "Epoch: 1534 Train Loss: 1.037796, Acc: 713.733\n",
      "Val Loss: 10.165649, Acc: 1355.265\n",
      "1355.26494140625\n",
      "Epoch: 1535 Train Loss: 1.299228, Acc: 834.633\n",
      "Val Loss: 10.681232, Acc: 1328.105\n",
      "1328.105322265625\n",
      "Epoch: 1536 Train Loss: 1.129603, Acc: 907.372\n",
      "Val Loss: 11.086444, Acc: 1509.706\n",
      "1509.70634765625\n",
      "Epoch: 1537 Train Loss: 1.176318, Acc: 938.485\n",
      "Val Loss: 10.201969, Acc: 1489.799\n",
      "1489.798876953125\n",
      "Epoch: 1538 Train Loss: 1.399446, Acc: 1074.946\n",
      "Val Loss: 13.358393, Acc: 1971.567\n",
      "1971.56708984375\n",
      "Epoch: 1539 Train Loss: 1.973298, Acc: 969.028\n",
      "Val Loss: 11.926836, Acc: 1474.911\n",
      "1474.911376953125\n",
      "Epoch: 1540 Train Loss: 1.245600, Acc: 817.206\n",
      "Val Loss: 11.655019, Acc: 1483.621\n",
      "1483.6205078125\n",
      "Epoch: 1541 Train Loss: 1.078327, Acc: 753.357\n",
      "Val Loss: 10.346024, Acc: 1345.761\n",
      "1345.76123046875\n",
      "Epoch: 1542 Train Loss: 1.106106, Acc: 820.810\n",
      "Val Loss: 10.732314, Acc: 1321.953\n",
      "1321.95322265625\n",
      "Epoch: 1543 Train Loss: 1.506519, Acc: 968.564\n",
      "Val Loss: 13.809637, Acc: 1640.814\n",
      "1640.81416015625\n",
      "Epoch: 1544 Train Loss: 2.349296, Acc: 1111.884\n",
      "Val Loss: 15.990917, Acc: 1826.494\n",
      "1826.49443359375\n",
      "Epoch: 1545 Train Loss: 1.735447, Acc: 967.522\n",
      "Val Loss: 10.737900, Acc: 1632.658\n",
      "1632.657958984375\n",
      "Epoch: 1546 Train Loss: 1.696114, Acc: 1014.869\n",
      "Val Loss: 12.513506, Acc: 1587.400\n",
      "1587.39970703125\n",
      "Epoch: 1547 Train Loss: 1.550594, Acc: 869.517\n",
      "Val Loss: 11.715384, Acc: 1512.599\n",
      "1512.5994140625\n",
      "Epoch: 1548 Train Loss: 1.332524, Acc: 868.572\n",
      "Val Loss: 11.511873, Acc: 1424.199\n",
      "1424.1994140625\n",
      "Epoch: 1549 Train Loss: 1.560804, Acc: 856.372\n",
      "Val Loss: 13.386738, Acc: 1862.420\n",
      "1862.420263671875\n",
      "Epoch: 1550 Train Loss: 1.496662, Acc: 945.117\n",
      "Val Loss: 12.692245, Acc: 1938.518\n",
      "1938.5177734375\n",
      "Epoch: 1551 Train Loss: 1.335364, Acc: 927.176\n",
      "Val Loss: 10.647611, Acc: 1453.994\n",
      "1453.994287109375\n",
      "Epoch: 1552 Train Loss: 1.684556, Acc: 919.107\n",
      "Val Loss: 10.372484, Acc: 1298.903\n",
      "1298.902587890625\n",
      "Epoch: 1553 Train Loss: 1.283888, Acc: 780.084\n",
      "Val Loss: 9.849133, Acc: 1291.939\n",
      "1291.93935546875\n",
      "1291.93935546875 1291.93935546875\n",
      "Epoch: 1554 Train Loss: 1.186626, Acc: 775.532\n",
      "Val Loss: 11.721925, Acc: 1444.797\n",
      "1444.7974609375\n",
      "Epoch: 1555 Train Loss: 1.230344, Acc: 777.882\n",
      "Val Loss: 11.020263, Acc: 1408.276\n",
      "1408.27626953125\n",
      "Epoch: 1556 Train Loss: 1.357819, Acc: 835.173\n",
      "Val Loss: 10.239245, Acc: 1434.608\n",
      "1434.608203125\n",
      "Epoch: 1557 Train Loss: 1.155466, Acc: 805.263\n",
      "Val Loss: 11.677130, Acc: 1458.512\n",
      "1458.5123046875\n",
      "Epoch: 1558 Train Loss: 1.081451, Acc: 734.489\n",
      "Val Loss: 11.053205, Acc: 1346.428\n",
      "1346.4279296875\n",
      "Epoch: 1559 Train Loss: 0.937636, Acc: 664.827\n",
      "Val Loss: 10.122738, Acc: 1252.661\n",
      "1252.661474609375\n",
      "1252.661474609375 1252.661474609375\n",
      "Epoch: 1560 Train Loss: 1.376391, Acc: 804.817\n",
      "Val Loss: 9.878395, Acc: 1292.036\n",
      "1292.036474609375\n",
      "Epoch: 1561 Train Loss: 1.351114, Acc: 812.851\n",
      "Val Loss: 11.372180, Acc: 1368.642\n",
      "1368.64208984375\n",
      "Epoch: 1562 Train Loss: 1.147609, Acc: 773.416\n",
      "Val Loss: 10.652906, Acc: 1543.062\n",
      "1543.06162109375\n",
      "Epoch: 1563 Train Loss: 1.117162, Acc: 824.525\n",
      "Val Loss: 10.790980, Acc: 1584.719\n",
      "1584.71904296875\n",
      "Epoch: 1564 Train Loss: 1.589589, Acc: 955.716\n",
      "Val Loss: 9.770012, Acc: 1254.321\n",
      "1254.32080078125\n",
      "Epoch: 1565 Train Loss: 1.458376, Acc: 904.660\n",
      "Val Loss: 10.742822, Acc: 1398.386\n",
      "1398.38642578125\n",
      "Epoch: 1566 Train Loss: 1.386581, Acc: 875.135\n",
      "Val Loss: 11.493930, Acc: 1473.258\n",
      "1473.25791015625\n",
      "Epoch: 1567 Train Loss: 1.137106, Acc: 775.382\n",
      "Val Loss: 9.994721, Acc: 1421.911\n",
      "1421.91123046875\n",
      "Epoch: 1568 Train Loss: 1.340729, Acc: 969.903\n",
      "Val Loss: 11.551015, Acc: 1815.303\n",
      "1815.303125\n",
      "Epoch: 1569 Train Loss: 1.127982, Acc: 879.358\n",
      "Val Loss: 12.735685, Acc: 1565.834\n",
      "1565.833984375\n",
      "Epoch: 1570 Train Loss: 1.059224, Acc: 833.925\n",
      "Val Loss: 10.523194, Acc: 1523.000\n",
      "1523.000390625\n",
      "Epoch: 1571 Train Loss: 1.244612, Acc: 839.558\n",
      "Val Loss: 11.575850, Acc: 1466.616\n",
      "1466.615966796875\n",
      "Epoch: 1572 Train Loss: 2.009000, Acc: 974.066\n",
      "Val Loss: 17.050086, Acc: 2255.481\n",
      "2255.48056640625\n",
      "Epoch: 1573 Train Loss: 2.114163, Acc: 1061.047\n",
      "Val Loss: 12.892133, Acc: 1950.974\n",
      "1950.9736328125\n",
      "Epoch: 1574 Train Loss: 2.008570, Acc: 1101.775\n",
      "Val Loss: 10.647356, Acc: 1346.352\n",
      "1346.351904296875\n",
      "Epoch: 1575 Train Loss: 1.580268, Acc: 918.371\n",
      "Val Loss: 10.960426, Acc: 1498.519\n",
      "1498.518505859375\n",
      "Epoch: 1576 Train Loss: 1.596968, Acc: 961.069\n",
      "Val Loss: 10.250842, Acc: 1548.804\n",
      "1548.8044921875\n",
      "Epoch: 1577 Train Loss: 1.347617, Acc: 905.703\n",
      "Val Loss: 10.918355, Acc: 1396.385\n",
      "1396.385205078125\n",
      "Epoch: 1578 Train Loss: 1.524433, Acc: 935.933\n",
      "Val Loss: 11.127061, Acc: 1411.230\n",
      "1411.23037109375\n",
      "Epoch: 1579 Train Loss: 1.358488, Acc: 809.879\n",
      "Val Loss: 11.169965, Acc: 1457.412\n",
      "1457.411767578125\n",
      "Epoch: 1580 Train Loss: 1.403350, Acc: 811.585\n",
      "Val Loss: 11.056616, Acc: 1351.482\n",
      "1351.48212890625\n",
      "Epoch: 1581 Train Loss: 1.471404, Acc: 920.336\n",
      "Val Loss: 10.445086, Acc: 1372.344\n",
      "1372.344384765625\n",
      "Epoch: 1582 Train Loss: 1.259080, Acc: 854.951\n",
      "Val Loss: 10.237986, Acc: 1317.862\n",
      "1317.862060546875\n",
      "Epoch: 1583 Train Loss: 1.590635, Acc: 887.897\n",
      "Val Loss: 14.445843, Acc: 1822.866\n",
      "1822.86552734375\n",
      "Epoch: 1584 Train Loss: 2.598670, Acc: 1080.562\n",
      "Val Loss: 13.129970, Acc: 1949.455\n",
      "1949.4548828125\n",
      "Epoch: 1585 Train Loss: 1.616390, Acc: 1075.160\n",
      "Val Loss: 10.453066, Acc: 1308.468\n",
      "1308.467578125\n",
      "Epoch: 1586 Train Loss: 1.663973, Acc: 948.643\n",
      "Val Loss: 10.157446, Acc: 1656.761\n",
      "1656.76103515625\n",
      "Epoch: 1587 Train Loss: 2.797864, Acc: 1280.608\n",
      "Val Loss: 17.475707, Acc: 2601.101\n",
      "2601.1009765625\n",
      "Epoch: 1588 Train Loss: 4.040610, Acc: 1457.033\n",
      "Val Loss: 18.313431, Acc: 2372.524\n",
      "2372.52421875\n",
      "Epoch: 1589 Train Loss: 2.907097, Acc: 1250.232\n",
      "Val Loss: 12.672011, Acc: 1923.766\n",
      "1923.766455078125\n",
      "Epoch: 1590 Train Loss: 2.481452, Acc: 1391.148\n",
      "Val Loss: 10.872673, Acc: 1781.919\n",
      "1781.91875\n",
      "Epoch: 1591 Train Loss: 1.964305, Acc: 1156.491\n",
      "Val Loss: 11.911921, Acc: 1557.338\n",
      "1557.3384765625\n",
      "Epoch: 1592 Train Loss: 1.510601, Acc: 949.220\n",
      "Val Loss: 13.858744, Acc: 1713.104\n",
      "1713.10390625\n",
      "Epoch: 1593 Train Loss: 1.574737, Acc: 974.876\n",
      "Val Loss: 11.193006, Acc: 1533.283\n",
      "1533.2826171875\n",
      "Epoch: 1594 Train Loss: 1.357668, Acc: 838.701\n",
      "Val Loss: 10.599640, Acc: 1317.784\n",
      "1317.78427734375\n",
      "Epoch: 1595 Train Loss: 1.148740, Acc: 872.903\n",
      "Val Loss: 12.004549, Acc: 1498.235\n",
      "1498.234912109375\n",
      "Epoch: 1596 Train Loss: 1.221775, Acc: 773.692\n",
      "Val Loss: 10.299432, Acc: 1288.227\n",
      "1288.227099609375\n",
      "Epoch: 1597 Train Loss: 1.081207, Acc: 723.634\n",
      "Val Loss: 11.507911, Acc: 1368.030\n",
      "1368.030078125\n",
      "Epoch: 1598 Train Loss: 0.911658, Acc: 680.067\n",
      "Val Loss: 10.290359, Acc: 1283.165\n",
      "1283.165478515625\n",
      "Epoch: 1599 Train Loss: 0.919922, Acc: 670.285\n",
      "Val Loss: 10.270843, Acc: 1279.104\n",
      "1279.10361328125\n",
      "Epoch: 1600 Train Loss: 0.965332, Acc: 728.131\n",
      "Val Loss: 10.597427, Acc: 1346.723\n",
      "1346.723388671875\n",
      "Epoch: 1601 Train Loss: 1.352943, Acc: 805.594\n",
      "Val Loss: 12.648194, Acc: 1611.559\n",
      "1611.55927734375\n",
      "Epoch: 1602 Train Loss: 1.168441, Acc: 721.317\n",
      "Val Loss: 10.673808, Acc: 1278.633\n",
      "1278.63330078125\n",
      "Epoch: 1603 Train Loss: 1.018656, Acc: 672.850\n",
      "Val Loss: 11.272428, Acc: 1292.046\n",
      "1292.045947265625\n",
      "Epoch: 1604 Train Loss: 1.008852, Acc: 666.517\n",
      "Val Loss: 11.003580, Acc: 1288.715\n",
      "1288.71455078125\n",
      "Epoch: 1605 Train Loss: 1.122374, Acc: 713.371\n",
      "Val Loss: 12.607305, Acc: 1474.540\n",
      "1474.540234375\n",
      "Epoch: 1606 Train Loss: 1.142417, Acc: 787.699\n",
      "Val Loss: 11.503699, Acc: 1523.232\n",
      "1523.23193359375\n",
      "Epoch: 1607 Train Loss: 1.134963, Acc: 743.870\n",
      "Val Loss: 10.553440, Acc: 1326.455\n",
      "1326.455078125\n",
      "Epoch: 1608 Train Loss: 1.005246, Acc: 667.291\n",
      "Val Loss: 11.462159, Acc: 1418.230\n",
      "1418.230126953125\n",
      "Epoch: 1609 Train Loss: 0.888655, Acc: 676.823\n",
      "Val Loss: 11.156252, Acc: 1354.208\n",
      "1354.208447265625\n",
      "Epoch: 1610 Train Loss: 0.862947, Acc: 707.419\n",
      "Val Loss: 11.187166, Acc: 1336.956\n",
      "1336.95576171875\n",
      "Epoch: 1611 Train Loss: 0.871926, Acc: 708.220\n",
      "Val Loss: 10.970329, Acc: 1552.580\n",
      "1552.580078125\n",
      "Epoch: 1612 Train Loss: 1.134645, Acc: 755.875\n",
      "Val Loss: 10.760469, Acc: 1305.885\n",
      "1305.88544921875\n",
      "Epoch: 1613 Train Loss: 1.057205, Acc: 759.841\n",
      "Val Loss: 10.101107, Acc: 1292.992\n",
      "1292.9916015625\n",
      "Epoch: 1614 Train Loss: 1.096116, Acc: 837.928\n",
      "Val Loss: 14.331116, Acc: 2091.283\n",
      "2091.28349609375\n",
      "Epoch: 1615 Train Loss: 1.664499, Acc: 1096.197\n",
      "Val Loss: 12.105950, Acc: 1593.607\n",
      "1593.6072265625\n",
      "Epoch: 1616 Train Loss: 1.798660, Acc: 969.875\n",
      "Val Loss: 11.498433, Acc: 1534.500\n",
      "1534.499609375\n",
      "Epoch: 1617 Train Loss: 1.557005, Acc: 919.120\n",
      "Val Loss: 10.440188, Acc: 1364.818\n",
      "1364.817578125\n",
      "Epoch: 1618 Train Loss: 1.163582, Acc: 831.224\n",
      "Val Loss: 10.875173, Acc: 1512.627\n",
      "1512.62724609375\n",
      "Epoch: 1619 Train Loss: 1.343809, Acc: 1032.848\n",
      "Val Loss: 11.739841, Acc: 1475.342\n",
      "1475.34169921875\n",
      "Epoch: 1620 Train Loss: 1.289312, Acc: 812.517\n",
      "Val Loss: 11.493176, Acc: 1502.712\n",
      "1502.711865234375\n",
      "Epoch: 1621 Train Loss: 1.363867, Acc: 877.618\n",
      "Val Loss: 10.028824, Acc: 1404.327\n",
      "1404.3271484375\n",
      "Epoch: 1622 Train Loss: 1.723068, Acc: 1034.815\n",
      "Val Loss: 10.544906, Acc: 1353.207\n",
      "1353.20732421875\n",
      "Epoch: 1623 Train Loss: 1.282945, Acc: 848.650\n",
      "Val Loss: 10.936589, Acc: 1405.665\n",
      "1405.66513671875\n",
      "Epoch: 1624 Train Loss: 1.318426, Acc: 918.205\n",
      "Val Loss: 10.885508, Acc: 1517.978\n",
      "1517.978173828125\n",
      "Epoch: 1625 Train Loss: 1.062403, Acc: 778.740\n",
      "Val Loss: 12.406051, Acc: 1464.810\n",
      "1464.810107421875\n",
      "Epoch: 1626 Train Loss: 1.234444, Acc: 790.426\n",
      "Val Loss: 11.380638, Acc: 1332.203\n",
      "1332.203173828125\n",
      "Epoch: 1627 Train Loss: 1.126275, Acc: 735.534\n",
      "Val Loss: 10.087392, Acc: 1192.437\n",
      "1192.43671875\n",
      "1192.43671875 1192.43671875\n",
      "Epoch: 1628 Train Loss: 0.879452, Acc: 670.908\n",
      "Val Loss: 12.247738, Acc: 1410.374\n",
      "1410.37373046875\n",
      "Epoch: 1629 Train Loss: 1.366124, Acc: 722.975\n",
      "Val Loss: 12.201231, Acc: 1439.124\n",
      "1439.12373046875\n",
      "Epoch: 1630 Train Loss: 1.230830, Acc: 875.182\n",
      "Val Loss: 10.623330, Acc: 1505.481\n",
      "1505.48115234375\n",
      "Epoch: 1631 Train Loss: 0.990502, Acc: 718.133\n",
      "Val Loss: 11.420876, Acc: 1362.959\n",
      "1362.9591796875\n",
      "Epoch: 1632 Train Loss: 0.913699, Acc: 651.129\n",
      "Val Loss: 12.589576, Acc: 1440.935\n",
      "1440.93466796875\n",
      "Epoch: 1633 Train Loss: 1.216419, Acc: 737.850\n",
      "Val Loss: 11.428549, Acc: 1324.463\n",
      "1324.462890625\n",
      "Epoch: 1634 Train Loss: 1.339377, Acc: 763.169\n",
      "Val Loss: 10.985499, Acc: 1340.607\n",
      "1340.60712890625\n",
      "Epoch: 1635 Train Loss: 1.139685, Acc: 725.619\n",
      "Val Loss: 11.248858, Acc: 1362.933\n",
      "1362.93291015625\n",
      "Epoch: 1636 Train Loss: 0.938736, Acc: 689.074\n",
      "Val Loss: 10.407694, Acc: 1322.074\n",
      "1322.07412109375\n",
      "Epoch: 1637 Train Loss: 0.886627, Acc: 715.971\n",
      "Val Loss: 10.469208, Acc: 1618.514\n",
      "1618.513623046875\n",
      "Epoch: 1638 Train Loss: 0.986963, Acc: 824.099\n",
      "Val Loss: 11.705255, Acc: 1369.089\n",
      "1369.08876953125\n",
      "Epoch: 1639 Train Loss: 1.082426, Acc: 713.195\n",
      "Val Loss: 13.239012, Acc: 1606.849\n",
      "1606.84873046875\n",
      "Epoch: 1640 Train Loss: 1.893266, Acc: 927.446\n",
      "Val Loss: 15.545066, Acc: 2365.192\n",
      "2365.19228515625\n",
      "Epoch: 1641 Train Loss: 2.709576, Acc: 1412.743\n",
      "Val Loss: 12.434740, Acc: 2079.117\n",
      "2079.11708984375\n",
      "Epoch: 1642 Train Loss: 2.002746, Acc: 1280.905\n",
      "Val Loss: 11.687367, Acc: 1508.237\n",
      "1508.2369140625\n",
      "Epoch: 1643 Train Loss: 1.605904, Acc: 1006.303\n",
      "Val Loss: 11.143059, Acc: 1701.749\n",
      "1701.74931640625\n",
      "Epoch: 1644 Train Loss: 1.411399, Acc: 889.729\n",
      "Val Loss: 12.011581, Acc: 1471.856\n",
      "1471.855859375\n",
      "Epoch: 1645 Train Loss: 1.281637, Acc: 838.611\n",
      "Val Loss: 13.221603, Acc: 1738.606\n",
      "1738.60576171875\n",
      "Epoch: 1646 Train Loss: 1.344481, Acc: 854.088\n",
      "Val Loss: 11.737576, Acc: 1399.640\n",
      "1399.640283203125\n",
      "Epoch: 1647 Train Loss: 1.397059, Acc: 826.465\n",
      "Val Loss: 10.726341, Acc: 1374.713\n",
      "1374.71328125\n",
      "Epoch: 1648 Train Loss: 1.050136, Acc: 714.511\n",
      "Val Loss: 9.782450, Acc: 1223.804\n",
      "1223.80439453125\n",
      "Epoch: 1649 Train Loss: 1.006842, Acc: 703.324\n",
      "Val Loss: 10.911964, Acc: 1345.356\n",
      "1345.35595703125\n",
      "Epoch: 1650 Train Loss: 0.957553, Acc: 692.949\n",
      "Val Loss: 10.022273, Acc: 1274.155\n",
      "1274.154541015625\n",
      "Epoch: 1651 Train Loss: 1.080190, Acc: 730.434\n",
      "Val Loss: 9.975508, Acc: 1202.786\n",
      "1202.786474609375\n",
      "Epoch: 1652 Train Loss: 1.494210, Acc: 910.911\n",
      "Val Loss: 10.409968, Acc: 1463.048\n",
      "1463.04814453125\n",
      "Epoch: 1653 Train Loss: 1.200979, Acc: 928.204\n",
      "Val Loss: 11.826595, Acc: 1587.858\n",
      "1587.8583984375\n",
      "Epoch: 1654 Train Loss: 0.998843, Acc: 786.621\n",
      "Val Loss: 10.535573, Acc: 1303.011\n",
      "1303.010546875\n",
      "Epoch: 1655 Train Loss: 0.871318, Acc: 642.368\n",
      "Val Loss: 11.107270, Acc: 1285.411\n",
      "1285.410546875\n",
      "Epoch: 1656 Train Loss: 0.863310, Acc: 633.063\n",
      "Val Loss: 10.627219, Acc: 1278.870\n",
      "1278.8697265625\n",
      "Epoch: 1657 Train Loss: 0.935036, Acc: 714.205\n",
      "Val Loss: 10.725893, Acc: 1278.565\n",
      "1278.5650390625\n",
      "Epoch: 1658 Train Loss: 0.893644, Acc: 718.036\n",
      "Val Loss: 12.240959, Acc: 1440.073\n",
      "1440.073486328125\n",
      "Epoch: 1659 Train Loss: 1.119216, Acc: 768.273\n",
      "Val Loss: 10.510539, Acc: 1302.399\n",
      "1302.39931640625\n",
      "Epoch: 1660 Train Loss: 1.150246, Acc: 775.570\n",
      "Val Loss: 11.173697, Acc: 1354.146\n",
      "1354.1455078125\n",
      "Epoch: 1661 Train Loss: 1.237192, Acc: 878.317\n",
      "Val Loss: 12.026530, Acc: 1449.358\n",
      "1449.358447265625\n",
      "Epoch: 1662 Train Loss: 1.315110, Acc: 764.215\n",
      "Val Loss: 12.364076, Acc: 1434.591\n",
      "1434.591064453125\n",
      "Epoch: 1663 Train Loss: 1.375249, Acc: 762.296\n",
      "Val Loss: 11.141087, Acc: 1277.947\n",
      "1277.94697265625\n",
      "Epoch: 1664 Train Loss: 1.386170, Acc: 811.242\n",
      "Val Loss: 10.508783, Acc: 1286.320\n",
      "1286.31982421875\n",
      "Epoch: 1665 Train Loss: 1.928280, Acc: 905.888\n",
      "Val Loss: 9.981501, Acc: 1225.314\n",
      "1225.3142578125\n",
      "Epoch: 1666 Train Loss: 1.685988, Acc: 864.382\n",
      "Val Loss: 11.886044, Acc: 1550.584\n",
      "1550.584375\n",
      "Epoch: 1667 Train Loss: 1.596007, Acc: 863.251\n",
      "Val Loss: 11.750517, Acc: 1484.287\n",
      "1484.28720703125\n",
      "Epoch: 1668 Train Loss: 1.636651, Acc: 1019.978\n",
      "Val Loss: 10.726727, Acc: 2138.279\n",
      "2138.27900390625\n",
      "Epoch: 1669 Train Loss: 1.718962, Acc: 1154.984\n",
      "Val Loss: 11.589352, Acc: 1937.844\n",
      "1937.8439453125\n",
      "Epoch: 1670 Train Loss: 1.534974, Acc: 1100.087\n",
      "Val Loss: 10.939995, Acc: 1319.286\n",
      "1319.2859375\n",
      "Epoch: 1671 Train Loss: 1.189917, Acc: 844.519\n",
      "Val Loss: 10.911894, Acc: 1504.675\n",
      "1504.675\n",
      "Epoch: 1672 Train Loss: 1.342515, Acc: 772.367\n",
      "Val Loss: 13.014897, Acc: 1528.828\n",
      "1528.828125\n",
      "Epoch: 1673 Train Loss: 1.990382, Acc: 886.034\n",
      "Val Loss: 15.337258, Acc: 1675.035\n",
      "1675.03544921875\n",
      "Epoch: 1674 Train Loss: 2.302234, Acc: 1093.795\n",
      "Val Loss: 10.581972, Acc: 1560.292\n",
      "1560.291796875\n",
      "Epoch: 1675 Train Loss: 1.933849, Acc: 1100.512\n",
      "Val Loss: 10.717574, Acc: 1500.272\n",
      "1500.271728515625\n",
      "Epoch: 1676 Train Loss: 1.454250, Acc: 975.478\n",
      "Val Loss: 11.674905, Acc: 1759.726\n",
      "1759.72646484375\n",
      "Epoch: 1677 Train Loss: 1.227659, Acc: 869.349\n",
      "Val Loss: 11.153545, Acc: 1444.920\n",
      "1444.9201171875\n",
      "Epoch: 1678 Train Loss: 1.307471, Acc: 907.591\n",
      "Val Loss: 10.118368, Acc: 1389.661\n",
      "1389.660546875\n",
      "Epoch: 1679 Train Loss: 0.942572, Acc: 733.730\n",
      "Val Loss: 12.250658, Acc: 1465.727\n",
      "1465.72724609375\n",
      "Epoch: 1680 Train Loss: 1.197831, Acc: 742.327\n",
      "Val Loss: 10.669866, Acc: 1253.461\n",
      "1253.46103515625\n",
      "Epoch: 1681 Train Loss: 1.335711, Acc: 786.650\n",
      "Val Loss: 10.600499, Acc: 1532.029\n",
      "1532.028955078125\n",
      "Epoch: 1682 Train Loss: 1.247574, Acc: 872.203\n",
      "Val Loss: 10.552308, Acc: 1737.594\n",
      "1737.59423828125\n",
      "Epoch: 1683 Train Loss: 1.173169, Acc: 882.194\n",
      "Val Loss: 9.437313, Acc: 1300.487\n",
      "1300.486962890625\n",
      "Epoch: 1684 Train Loss: 1.118705, Acc: 744.699\n",
      "Val Loss: 10.959295, Acc: 1458.975\n",
      "1458.97548828125\n",
      "Epoch: 1685 Train Loss: 0.949377, Acc: 712.857\n",
      "Val Loss: 11.379141, Acc: 1379.814\n",
      "1379.81357421875\n",
      "Epoch: 1686 Train Loss: 1.241792, Acc: 836.446\n",
      "Val Loss: 12.843417, Acc: 1769.533\n",
      "1769.5326171875\n",
      "Epoch: 1687 Train Loss: 2.572959, Acc: 1183.428\n",
      "Val Loss: 10.306147, Acc: 1610.391\n",
      "1610.391064453125\n",
      "Epoch: 1688 Train Loss: 1.280110, Acc: 860.432\n",
      "Val Loss: 11.338801, Acc: 1665.603\n",
      "1665.60263671875\n",
      "Epoch: 1689 Train Loss: 1.171353, Acc: 853.905\n",
      "Val Loss: 12.384451, Acc: 1612.022\n",
      "1612.021826171875\n",
      "Epoch: 1690 Train Loss: 1.310848, Acc: 890.107\n",
      "Val Loss: 12.586801, Acc: 1619.904\n",
      "1619.9037109375\n",
      "Epoch: 1691 Train Loss: 1.034175, Acc: 764.061\n",
      "Val Loss: 10.089050, Acc: 1365.301\n",
      "1365.30068359375\n",
      "Epoch: 1692 Train Loss: 1.026697, Acc: 666.518\n",
      "Val Loss: 10.329687, Acc: 1266.704\n",
      "1266.704345703125\n",
      "Epoch: 1693 Train Loss: 1.131504, Acc: 707.005\n",
      "Val Loss: 13.878315, Acc: 1557.273\n",
      "1557.273291015625\n",
      "Epoch: 1694 Train Loss: 2.034640, Acc: 842.528\n",
      "Val Loss: 12.584256, Acc: 1431.780\n",
      "1431.77958984375\n",
      "Epoch: 1695 Train Loss: 1.236924, Acc: 705.080\n",
      "Val Loss: 10.063043, Acc: 1203.177\n",
      "1203.1767578125\n",
      "Epoch: 1696 Train Loss: 1.271528, Acc: 781.378\n",
      "Val Loss: 11.102552, Acc: 1425.745\n",
      "1425.74453125\n",
      "Epoch: 1697 Train Loss: 1.106997, Acc: 851.677\n",
      "Val Loss: 11.857742, Acc: 1590.727\n",
      "1590.7265625\n",
      "Epoch: 1698 Train Loss: 1.056409, Acc: 764.605\n",
      "Val Loss: 9.696750, Acc: 1410.940\n",
      "1410.940380859375\n",
      "Epoch: 1699 Train Loss: 1.332455, Acc: 788.049\n",
      "Val Loss: 10.396293, Acc: 1367.143\n",
      "1367.1431640625\n",
      "Epoch: 1700 Train Loss: 1.194425, Acc: 761.191\n",
      "Val Loss: 12.220646, Acc: 1393.315\n",
      "1393.3146484375\n",
      "Epoch: 1701 Train Loss: 1.123631, Acc: 716.910\n",
      "Val Loss: 11.668550, Acc: 1424.388\n",
      "1424.38779296875\n",
      "Epoch: 1702 Train Loss: 1.116191, Acc: 699.120\n",
      "Val Loss: 10.203809, Acc: 1202.811\n",
      "1202.810986328125\n",
      "Epoch: 1703 Train Loss: 1.399863, Acc: 819.799\n",
      "Val Loss: 10.841673, Acc: 1419.987\n",
      "1419.986572265625\n",
      "Epoch: 1704 Train Loss: 1.064625, Acc: 689.071\n",
      "Val Loss: 10.165464, Acc: 1273.362\n",
      "1273.36201171875\n",
      "Epoch: 1705 Train Loss: 1.135617, Acc: 794.134\n",
      "Val Loss: 10.426754, Acc: 1495.325\n",
      "1495.32509765625\n",
      "Epoch: 1706 Train Loss: 1.078855, Acc: 798.636\n",
      "Val Loss: 10.399590, Acc: 1325.408\n",
      "1325.4080078125\n",
      "Epoch: 1707 Train Loss: 1.186443, Acc: 799.058\n",
      "Val Loss: 12.449489, Acc: 1378.002\n",
      "1378.002099609375\n",
      "Epoch: 1708 Train Loss: 1.159929, Acc: 674.386\n",
      "Val Loss: 10.461311, Acc: 1246.555\n",
      "1246.5546875\n",
      "Epoch: 1709 Train Loss: 0.899497, Acc: 617.066\n",
      "Val Loss: 11.218951, Acc: 1281.210\n",
      "1281.21044921875\n",
      "Epoch: 1710 Train Loss: 1.156343, Acc: 661.898\n",
      "Val Loss: 11.263477, Acc: 1321.643\n",
      "1321.642529296875\n",
      "Epoch: 1711 Train Loss: 0.895423, Acc: 580.551\n",
      "Val Loss: 10.320055, Acc: 1223.442\n",
      "1223.44248046875\n",
      "Epoch: 1712 Train Loss: 0.942695, Acc: 660.828\n",
      "Val Loss: 11.809748, Acc: 1340.405\n",
      "1340.404833984375\n",
      "Epoch: 1713 Train Loss: 1.180286, Acc: 693.790\n",
      "Val Loss: 11.044373, Acc: 1405.050\n",
      "1405.050341796875\n",
      "Epoch: 1714 Train Loss: 1.066524, Acc: 769.504\n",
      "Val Loss: 10.928345, Acc: 1309.643\n",
      "1309.64345703125\n",
      "Epoch: 1715 Train Loss: 1.027402, Acc: 728.948\n",
      "Val Loss: 11.089561, Acc: 1340.758\n",
      "1340.75830078125\n",
      "Epoch: 1716 Train Loss: 0.958883, Acc: 660.459\n",
      "Val Loss: 9.950433, Acc: 1378.505\n",
      "1378.5046875\n",
      "Epoch: 1717 Train Loss: 1.294228, Acc: 692.230\n",
      "Val Loss: 10.503914, Acc: 1224.917\n",
      "1224.916796875\n",
      "Epoch: 1718 Train Loss: 0.978600, Acc: 701.996\n",
      "Val Loss: 10.677504, Acc: 1325.563\n",
      "1325.56279296875\n",
      "Epoch: 1719 Train Loss: 0.840829, Acc: 657.492\n",
      "Val Loss: 11.079453, Acc: 1249.855\n",
      "1249.854736328125\n",
      "Epoch: 1720 Train Loss: 0.864034, Acc: 580.404\n",
      "Val Loss: 11.731561, Acc: 1287.627\n",
      "1287.627001953125\n",
      "Epoch: 1721 Train Loss: 0.805319, Acc: 565.853\n",
      "Val Loss: 11.197807, Acc: 1262.680\n",
      "1262.6802734375\n",
      "Epoch: 1722 Train Loss: 1.181347, Acc: 717.220\n",
      "Val Loss: 13.984122, Acc: 1567.356\n",
      "1567.35625\n",
      "Epoch: 1723 Train Loss: 1.885093, Acc: 954.095\n",
      "Val Loss: 12.278428, Acc: 1497.029\n",
      "1497.029296875\n",
      "Epoch: 1724 Train Loss: 1.730842, Acc: 924.304\n",
      "Val Loss: 11.554670, Acc: 1516.007\n",
      "1516.006787109375\n",
      "Epoch: 1725 Train Loss: 1.288543, Acc: 799.063\n",
      "Val Loss: 10.559430, Acc: 1480.563\n",
      "1480.5630859375\n",
      "Epoch: 1726 Train Loss: 1.019691, Acc: 691.263\n",
      "Val Loss: 10.564957, Acc: 1282.426\n",
      "1282.4255859375\n",
      "Epoch: 1727 Train Loss: 0.801376, Acc: 583.449\n",
      "Val Loss: 10.002099, Acc: 1190.799\n",
      "1190.798974609375\n",
      "1190.798974609375 1190.798974609375\n",
      "Epoch: 1728 Train Loss: 0.909129, Acc: 617.570\n",
      "Val Loss: 11.179213, Acc: 1290.474\n",
      "1290.47373046875\n",
      "Epoch: 1729 Train Loss: 0.846489, Acc: 593.009\n",
      "Val Loss: 11.309130, Acc: 1296.018\n",
      "1296.017578125\n",
      "Epoch: 1730 Train Loss: 0.773994, Acc: 564.231\n",
      "Val Loss: 10.714931, Acc: 1177.556\n",
      "1177.556494140625\n",
      "1177.556494140625 1177.556494140625\n",
      "Epoch: 1731 Train Loss: 0.825356, Acc: 557.863\n",
      "Val Loss: 12.004649, Acc: 1343.956\n",
      "1343.956005859375\n",
      "Epoch: 1732 Train Loss: 1.090264, Acc: 636.437\n",
      "Val Loss: 13.275662, Acc: 1584.229\n",
      "1584.228955078125\n",
      "Epoch: 1733 Train Loss: 1.527102, Acc: 753.588\n",
      "Val Loss: 11.898270, Acc: 1342.830\n",
      "1342.82998046875\n",
      "Epoch: 1734 Train Loss: 1.375476, Acc: 719.227\n",
      "Val Loss: 10.589503, Acc: 1277.275\n",
      "1277.27490234375\n",
      "Epoch: 1735 Train Loss: 1.194976, Acc: 697.254\n",
      "Val Loss: 11.375410, Acc: 1225.599\n",
      "1225.598779296875\n",
      "Epoch: 1736 Train Loss: 1.281449, Acc: 667.886\n",
      "Val Loss: 11.930732, Acc: 1330.410\n",
      "1330.41044921875\n",
      "Epoch: 1737 Train Loss: 0.886574, Acc: 561.546\n",
      "Val Loss: 10.783368, Acc: 1245.755\n",
      "1245.75546875\n",
      "Epoch: 1738 Train Loss: 0.896966, Acc: 658.734\n",
      "Val Loss: 10.754650, Acc: 1331.988\n",
      "1331.988037109375\n",
      "Epoch: 1739 Train Loss: 0.832826, Acc: 668.530\n",
      "Val Loss: 11.017239, Acc: 1273.638\n",
      "1273.637548828125\n",
      "Epoch: 1740 Train Loss: 0.835614, Acc: 579.470\n",
      "Val Loss: 10.937979, Acc: 1194.127\n",
      "1194.127294921875\n",
      "Epoch: 1741 Train Loss: 0.750590, Acc: 528.685\n",
      "Val Loss: 11.293622, Acc: 1213.527\n",
      "1213.526708984375\n",
      "Epoch: 1742 Train Loss: 0.759436, Acc: 502.524\n",
      "Val Loss: 10.309169, Acc: 1106.106\n",
      "1106.1060546875\n",
      "1106.1060546875 1106.1060546875\n",
      "Epoch: 1743 Train Loss: 1.099748, Acc: 620.910\n",
      "Val Loss: 11.133965, Acc: 1213.183\n",
      "1213.1826171875\n",
      "Epoch: 1744 Train Loss: 0.834043, Acc: 538.562\n",
      "Val Loss: 10.628662, Acc: 1186.703\n",
      "1186.7029296875\n",
      "Epoch: 1745 Train Loss: 0.785858, Acc: 542.804\n",
      "Val Loss: 11.957947, Acc: 1357.753\n",
      "1357.753076171875\n",
      "Epoch: 1746 Train Loss: 0.956250, Acc: 581.058\n",
      "Val Loss: 11.886930, Acc: 1392.754\n",
      "1392.754248046875\n",
      "Epoch: 1747 Train Loss: 0.789884, Acc: 539.431\n",
      "Val Loss: 10.132709, Acc: 1084.841\n",
      "1084.841015625\n",
      "1084.841015625 1084.841015625\n",
      "Epoch: 1748 Train Loss: 0.748379, Acc: 517.479\n",
      "Val Loss: 11.665433, Acc: 1252.951\n",
      "1252.950537109375\n",
      "Epoch: 1749 Train Loss: 0.802405, Acc: 530.738\n",
      "Val Loss: 12.065518, Acc: 1283.750\n",
      "1283.74990234375\n",
      "Epoch: 1750 Train Loss: 0.830852, Acc: 596.645\n",
      "Val Loss: 10.359952, Acc: 1382.615\n",
      "1382.614501953125\n",
      "Epoch: 1751 Train Loss: 1.008159, Acc: 659.794\n",
      "Val Loss: 10.460754, Acc: 1217.004\n",
      "1217.00390625\n",
      "Epoch: 1752 Train Loss: 1.484628, Acc: 670.889\n",
      "Val Loss: 13.080785, Acc: 1361.617\n",
      "1361.616650390625\n",
      "Epoch: 1753 Train Loss: 1.991705, Acc: 724.679\n",
      "Val Loss: 12.150108, Acc: 1303.299\n",
      "1303.299365234375\n",
      "Epoch: 1754 Train Loss: 1.678187, Acc: 748.105\n",
      "Val Loss: 12.636864, Acc: 1521.283\n",
      "1521.2833984375\n",
      "Epoch: 1755 Train Loss: 1.321051, Acc: 729.225\n",
      "Val Loss: 12.213693, Acc: 1391.899\n",
      "1391.89892578125\n",
      "Epoch: 1756 Train Loss: 1.198289, Acc: 634.517\n",
      "Val Loss: 10.621774, Acc: 1202.230\n",
      "1202.23046875\n",
      "Epoch: 1757 Train Loss: 1.006847, Acc: 634.384\n",
      "Val Loss: 11.399559, Acc: 1257.372\n",
      "1257.37236328125\n",
      "Epoch: 1758 Train Loss: 0.888441, Acc: 576.261\n",
      "Val Loss: 10.886352, Acc: 1165.303\n",
      "1165.3025146484374\n",
      "Epoch: 1759 Train Loss: 0.963084, Acc: 579.783\n",
      "Val Loss: 11.820808, Acc: 1243.439\n",
      "1243.438916015625\n",
      "Epoch: 1760 Train Loss: 1.018788, Acc: 584.794\n",
      "Val Loss: 11.248651, Acc: 1193.387\n",
      "1193.3873046875\n",
      "Epoch: 1761 Train Loss: 0.794553, Acc: 522.075\n",
      "Val Loss: 9.958437, Acc: 1174.324\n",
      "1174.32392578125\n",
      "Epoch: 1762 Train Loss: 1.621497, Acc: 669.492\n",
      "Val Loss: 9.718859, Acc: 1149.857\n",
      "1149.8572265625\n",
      "Epoch: 1763 Train Loss: 2.141552, Acc: 802.366\n",
      "Val Loss: 11.930250, Acc: 1252.099\n",
      "1252.0986328125\n",
      "Epoch: 1764 Train Loss: 2.019648, Acc: 844.846\n",
      "Val Loss: 13.287480, Acc: 1582.224\n",
      "1582.223828125\n",
      "Epoch: 1765 Train Loss: 1.572403, Acc: 755.192\n",
      "Val Loss: 11.385118, Acc: 1277.083\n",
      "1277.0830078125\n",
      "Epoch: 1766 Train Loss: 1.107352, Acc: 581.971\n",
      "Val Loss: 11.505069, Acc: 1234.656\n",
      "1234.6564453125\n",
      "Epoch: 1767 Train Loss: 0.902902, Acc: 554.531\n",
      "Val Loss: 10.841987, Acc: 1193.666\n",
      "1193.665625\n",
      "Epoch: 1768 Train Loss: 0.983854, Acc: 555.555\n",
      "Val Loss: 10.195116, Acc: 1069.110\n",
      "1069.109619140625\n",
      "1069.109619140625 1069.109619140625\n",
      "Epoch: 1769 Train Loss: 0.976741, Acc: 524.304\n",
      "Val Loss: 10.193195, Acc: 1103.253\n",
      "1103.25283203125\n",
      "Epoch: 1770 Train Loss: 1.047937, Acc: 561.072\n",
      "Val Loss: 12.850994, Acc: 1276.545\n",
      "1276.5451904296874\n",
      "Epoch: 1771 Train Loss: 1.148848, Acc: 568.618\n",
      "Val Loss: 11.377062, Acc: 1193.057\n",
      "1193.056689453125\n",
      "Epoch: 1772 Train Loss: 0.939513, Acc: 505.129\n",
      "Val Loss: 10.408093, Acc: 1083.283\n",
      "1083.28349609375\n",
      "Epoch: 1773 Train Loss: 0.743703, Acc: 449.576\n",
      "Val Loss: 10.703939, Acc: 1108.089\n",
      "1108.0892578125\n",
      "Epoch: 1774 Train Loss: 0.790555, Acc: 483.652\n",
      "Val Loss: 10.312223, Acc: 1119.648\n",
      "1119.6482421875\n",
      "Epoch: 1775 Train Loss: 0.696824, Acc: 457.341\n",
      "Val Loss: 11.192456, Acc: 1149.555\n",
      "1149.5547119140624\n",
      "Epoch: 1776 Train Loss: 0.749126, Acc: 471.458\n",
      "Val Loss: 10.548185, Acc: 1076.052\n",
      "1076.051904296875\n",
      "Epoch: 1777 Train Loss: 0.893947, Acc: 485.907\n",
      "Val Loss: 12.671867, Acc: 1307.221\n",
      "1307.22119140625\n",
      "Epoch: 1778 Train Loss: 1.131492, Acc: 539.521\n",
      "Val Loss: 13.653605, Acc: 1355.772\n",
      "1355.7724609375\n",
      "Epoch: 1779 Train Loss: 1.484872, Acc: 615.383\n",
      "Val Loss: 14.521924, Acc: 1377.256\n",
      "1377.256005859375\n",
      "Epoch: 1780 Train Loss: 1.456373, Acc: 622.053\n",
      "Val Loss: 11.151737, Acc: 1139.619\n",
      "1139.618798828125\n",
      "Epoch: 1781 Train Loss: 1.193721, Acc: 585.992\n",
      "Val Loss: 12.184945, Acc: 1333.036\n",
      "1333.03603515625\n",
      "Epoch: 1782 Train Loss: 0.956955, Acc: 583.267\n",
      "Val Loss: 10.869758, Acc: 1154.831\n",
      "1154.83076171875\n",
      "Epoch: 1783 Train Loss: 0.952658, Acc: 571.672\n",
      "Val Loss: 13.830790, Acc: 1397.751\n",
      "1397.75078125\n",
      "Epoch: 1784 Train Loss: 0.948655, Acc: 564.780\n",
      "Val Loss: 10.247545, Acc: 1106.095\n",
      "1106.0947265625\n",
      "Epoch: 1785 Train Loss: 0.888013, Acc: 540.736\n",
      "Val Loss: 12.971540, Acc: 1359.660\n",
      "1359.6595703125\n",
      "Epoch: 1786 Train Loss: 1.842378, Acc: 694.560\n",
      "Val Loss: 11.675115, Acc: 1263.770\n",
      "1263.769775390625\n",
      "Epoch: 1787 Train Loss: 2.487319, Acc: 793.803\n",
      "Val Loss: 12.154509, Acc: 1433.996\n",
      "1433.9958984375\n",
      "Epoch: 1788 Train Loss: 2.427781, Acc: 772.044\n",
      "Val Loss: 10.593817, Acc: 1163.075\n",
      "1163.07509765625\n",
      "Epoch: 1789 Train Loss: 1.932342, Acc: 672.799\n",
      "Val Loss: 11.383866, Acc: 1264.955\n",
      "1264.95498046875\n",
      "Epoch: 1790 Train Loss: 1.396384, Acc: 607.134\n",
      "Val Loss: 11.910762, Acc: 1255.174\n",
      "1255.173974609375\n",
      "Epoch: 1791 Train Loss: 1.890173, Acc: 668.615\n",
      "Val Loss: 15.590148, Acc: 1573.140\n",
      "1573.13984375\n",
      "Epoch: 1792 Train Loss: 1.982832, Acc: 701.876\n",
      "Val Loss: 10.762599, Acc: 1176.007\n",
      "1176.006982421875\n",
      "Epoch: 1793 Train Loss: 1.682961, Acc: 682.132\n",
      "Val Loss: 11.825486, Acc: 1318.085\n",
      "1318.08505859375\n",
      "Epoch: 1794 Train Loss: 1.486469, Acc: 646.461\n",
      "Val Loss: 10.795897, Acc: 1193.262\n",
      "1193.2619140625\n",
      "Epoch: 1795 Train Loss: 0.996356, Acc: 546.991\n",
      "Val Loss: 13.742543, Acc: 1540.008\n",
      "1540.00791015625\n",
      "Epoch: 1796 Train Loss: 1.139546, Acc: 572.491\n",
      "Val Loss: 11.095610, Acc: 1167.390\n",
      "1167.3896484375\n",
      "Epoch: 1797 Train Loss: 0.899408, Acc: 508.777\n",
      "Val Loss: 10.809866, Acc: 1101.782\n",
      "1101.7822998046875\n",
      "Epoch: 1798 Train Loss: 0.779458, Acc: 469.503\n",
      "Val Loss: 9.803923, Acc: 1029.443\n",
      "1029.44287109375\n",
      "1029.44287109375 1029.44287109375\n",
      "Epoch: 1799 Train Loss: 0.932136, Acc: 502.309\n",
      "Val Loss: 12.178317, Acc: 1235.039\n",
      "1235.039013671875\n",
      "Epoch: 1800 Train Loss: 0.994161, Acc: 502.463\n",
      "Val Loss: 11.002162, Acc: 1092.909\n",
      "1092.9092041015624\n",
      "Epoch: 1801 Train Loss: 0.719892, Acc: 448.053\n",
      "Val Loss: 9.939513, Acc: 1008.272\n",
      "1008.272216796875\n",
      "1008.272216796875 1008.272216796875\n",
      "Epoch: 1802 Train Loss: 0.724008, Acc: 446.962\n",
      "Val Loss: 10.248656, Acc: 1051.349\n",
      "1051.3494140625\n",
      "Epoch: 1803 Train Loss: 0.776584, Acc: 466.315\n",
      "Val Loss: 10.055666, Acc: 1050.223\n",
      "1050.2226318359376\n",
      "Epoch: 1804 Train Loss: 0.672157, Acc: 424.915\n",
      "Val Loss: 10.698579, Acc: 1060.135\n",
      "1060.134716796875\n",
      "Epoch: 1805 Train Loss: 0.814766, Acc: 462.758\n",
      "Val Loss: 10.367495, Acc: 1034.935\n",
      "1034.9348388671874\n",
      "Epoch: 1806 Train Loss: 0.700139, Acc: 432.929\n",
      "Val Loss: 11.393262, Acc: 1158.155\n",
      "1158.1546630859375\n",
      "Epoch: 1807 Train Loss: 0.740627, Acc: 446.559\n",
      "Val Loss: 10.320762, Acc: 1067.973\n",
      "1067.972900390625\n",
      "Epoch: 1808 Train Loss: 0.958945, Acc: 490.541\n",
      "Val Loss: 10.151091, Acc: 1073.732\n",
      "1073.7323974609376\n",
      "Epoch: 1809 Train Loss: 0.937338, Acc: 503.460\n",
      "Val Loss: 12.446155, Acc: 1249.521\n",
      "1249.52099609375\n",
      "Epoch: 1810 Train Loss: 1.208135, Acc: 551.769\n",
      "Val Loss: 11.711221, Acc: 1219.587\n",
      "1219.5873046875\n",
      "Epoch: 1811 Train Loss: 1.736355, Acc: 644.925\n",
      "Val Loss: 10.260734, Acc: 1183.466\n",
      "1183.466162109375\n",
      "Epoch: 1812 Train Loss: 1.942790, Acc: 676.754\n",
      "Val Loss: 11.774358, Acc: 1277.646\n",
      "1277.646435546875\n",
      "Epoch: 1813 Train Loss: 2.503290, Acc: 765.445\n",
      "Val Loss: 16.116460, Acc: 1555.131\n",
      "1555.130810546875\n",
      "Epoch: 1814 Train Loss: 1.913398, Acc: 702.717\n",
      "Val Loss: 9.654115, Acc: 1048.372\n",
      "1048.371533203125\n",
      "Epoch: 1815 Train Loss: 1.664478, Acc: 630.026\n",
      "Val Loss: 10.882493, Acc: 1143.217\n",
      "1143.216796875\n",
      "Epoch: 1816 Train Loss: 1.740412, Acc: 635.097\n",
      "Val Loss: 13.695428, Acc: 1353.910\n",
      "1353.909619140625\n",
      "Epoch: 1817 Train Loss: 1.577360, Acc: 622.557\n",
      "Val Loss: 11.759127, Acc: 1242.921\n",
      "1242.92138671875\n",
      "Epoch: 1818 Train Loss: 1.766165, Acc: 636.666\n",
      "Val Loss: 10.187796, Acc: 1060.568\n",
      "1060.567822265625\n",
      "Epoch: 1819 Train Loss: 1.398183, Acc: 594.187\n",
      "Val Loss: 10.202335, Acc: 1138.115\n",
      "1138.115478515625\n",
      "Epoch: 1820 Train Loss: 1.619553, Acc: 625.443\n",
      "Val Loss: 13.473564, Acc: 1328.320\n",
      "1328.31962890625\n",
      "Epoch: 1821 Train Loss: 1.561291, Acc: 599.224\n",
      "Val Loss: 11.853376, Acc: 1251.210\n",
      "1251.21015625\n",
      "Epoch: 1822 Train Loss: 1.855519, Acc: 647.343\n",
      "Val Loss: 10.055454, Acc: 1021.739\n",
      "1021.738671875\n",
      "Epoch: 1823 Train Loss: 1.262191, Acc: 559.590\n",
      "Val Loss: 13.635552, Acc: 1353.930\n",
      "1353.9302734375\n",
      "Epoch: 1824 Train Loss: 1.243917, Acc: 534.688\n",
      "Val Loss: 9.860240, Acc: 1016.255\n",
      "1016.25517578125\n",
      "Epoch: 1825 Train Loss: 1.037130, Acc: 502.141\n",
      "Val Loss: 10.313903, Acc: 1037.538\n",
      "1037.53837890625\n",
      "Epoch: 1826 Train Loss: 0.809282, Acc: 447.072\n",
      "Val Loss: 10.131731, Acc: 1053.797\n",
      "1053.797216796875\n",
      "Epoch: 1827 Train Loss: 0.664734, Acc: 415.994\n",
      "Val Loss: 11.585985, Acc: 1143.451\n",
      "1143.4510986328125\n",
      "Epoch: 1828 Train Loss: 0.754820, Acc: 427.587\n",
      "Val Loss: 11.074230, Acc: 1105.975\n",
      "1105.97451171875\n",
      "Epoch: 1829 Train Loss: 0.745473, Acc: 429.808\n",
      "Val Loss: 10.824922, Acc: 1072.874\n",
      "1072.8739501953125\n",
      "Epoch: 1830 Train Loss: 0.697459, Acc: 420.641\n",
      "Val Loss: 10.520455, Acc: 1038.608\n",
      "1038.6083740234376\n",
      "Epoch: 1831 Train Loss: 0.668567, Acc: 406.276\n",
      "Val Loss: 9.891065, Acc: 994.914\n",
      "994.914404296875\n",
      "994.914404296875 994.914404296875\n",
      "Epoch: 1832 Train Loss: 0.701779, Acc: 414.027\n",
      "Val Loss: 9.789265, Acc: 985.677\n",
      "985.6772216796875\n",
      "985.6772216796875 985.6772216796875\n",
      "Epoch: 1833 Train Loss: 0.848258, Acc: 450.724\n",
      "Val Loss: 11.526961, Acc: 1142.787\n",
      "1142.786572265625\n",
      "Epoch: 1834 Train Loss: 0.763661, Acc: 432.144\n",
      "Val Loss: 11.946112, Acc: 1173.496\n",
      "1173.4955078125\n",
      "Epoch: 1835 Train Loss: 0.774385, Acc: 427.020\n",
      "Val Loss: 10.295230, Acc: 1016.697\n",
      "1016.69716796875\n",
      "Epoch: 1836 Train Loss: 0.633837, Acc: 398.796\n",
      "Val Loss: 11.287110, Acc: 1099.929\n",
      "1099.929052734375\n",
      "Epoch: 1837 Train Loss: 0.655764, Acc: 393.611\n",
      "Val Loss: 11.360048, Acc: 1106.005\n",
      "1106.00537109375\n",
      "Epoch: 1838 Train Loss: 0.766398, Acc: 424.734\n",
      "Val Loss: 10.486916, Acc: 1019.775\n",
      "1019.7747802734375\n",
      "Epoch: 1839 Train Loss: 0.738495, Acc: 414.081\n",
      "Val Loss: 12.303297, Acc: 1208.001\n",
      "1208.001416015625\n",
      "Epoch: 1840 Train Loss: 0.888496, Acc: 449.832\n",
      "Val Loss: 12.214018, Acc: 1182.565\n",
      "1182.564892578125\n",
      "Epoch: 1841 Train Loss: 0.995550, Acc: 471.235\n",
      "Val Loss: 10.496141, Acc: 1020.224\n",
      "1020.2237548828125\n",
      "Epoch: 1842 Train Loss: 0.971245, Acc: 471.748\n",
      "Val Loss: 9.999318, Acc: 1033.960\n",
      "1033.9597900390625\n",
      "Epoch: 1843 Train Loss: 0.856267, Acc: 435.127\n",
      "Val Loss: 10.616036, Acc: 1017.100\n",
      "1017.10009765625\n",
      "Epoch: 1844 Train Loss: 0.791467, Acc: 422.798\n",
      "Val Loss: 11.351481, Acc: 1117.902\n",
      "1117.901953125\n",
      "Epoch: 1845 Train Loss: 0.927763, Acc: 453.227\n",
      "Val Loss: 11.506815, Acc: 1137.864\n",
      "1137.86396484375\n",
      "Epoch: 1846 Train Loss: 0.844888, Acc: 434.427\n",
      "Val Loss: 10.720903, Acc: 1054.716\n",
      "1054.71552734375\n",
      "Epoch: 1847 Train Loss: 0.688587, Acc: 403.989\n",
      "Val Loss: 10.048519, Acc: 967.499\n",
      "967.49873046875\n",
      "967.49873046875 967.49873046875\n",
      "Epoch: 1848 Train Loss: 0.567265, Acc: 370.068\n",
      "Val Loss: 10.275219, Acc: 1020.280\n",
      "1020.2796875\n",
      "Epoch: 1849 Train Loss: 0.684559, Acc: 394.145\n",
      "Val Loss: 11.811225, Acc: 1160.464\n",
      "1160.4636962890625\n",
      "Epoch: 1850 Train Loss: 0.673458, Acc: 397.203\n",
      "Val Loss: 10.354328, Acc: 1003.162\n",
      "1003.162060546875\n",
      "Epoch: 1851 Train Loss: 0.600431, Acc: 368.877\n",
      "Val Loss: 10.971169, Acc: 1055.881\n",
      "1055.88076171875\n",
      "Epoch: 1852 Train Loss: 0.700497, Acc: 390.006\n",
      "Val Loss: 10.653016, Acc: 1042.333\n",
      "1042.3333984375\n",
      "Epoch: 1853 Train Loss: 0.891054, Acc: 441.232\n",
      "Val Loss: 11.284321, Acc: 1087.131\n",
      "1087.13125\n",
      "Epoch: 1854 Train Loss: 0.788953, Acc: 420.390\n",
      "Val Loss: 11.948004, Acc: 1152.236\n",
      "1152.2359130859375\n",
      "Epoch: 1855 Train Loss: 0.867879, Acc: 453.741\n",
      "Val Loss: 10.343780, Acc: 1056.980\n",
      "1056.979736328125\n",
      "Epoch: 1856 Train Loss: 1.876118, Acc: 613.491\n",
      "Val Loss: 9.909053, Acc: 982.632\n",
      "982.63212890625\n",
      "Epoch: 1857 Train Loss: 2.482363, Acc: 709.690\n",
      "Val Loss: 14.862123, Acc: 1424.791\n",
      "1424.79111328125\n",
      "Epoch: 1858 Train Loss: 1.839389, Acc: 607.208\n",
      "Val Loss: 11.046182, Acc: 1107.515\n",
      "1107.514892578125\n",
      "Epoch: 1859 Train Loss: 1.120303, Acc: 504.269\n",
      "Val Loss: 10.477817, Acc: 1122.561\n",
      "1122.560791015625\n",
      "Epoch: 1860 Train Loss: 1.677572, Acc: 639.123\n",
      "Val Loss: 11.989703, Acc: 1216.721\n",
      "1216.72138671875\n",
      "Epoch: 1861 Train Loss: 1.280745, Acc: 553.134\n",
      "Val Loss: 11.186861, Acc: 1153.980\n",
      "1153.979833984375\n",
      "Epoch: 1862 Train Loss: 0.990878, Acc: 472.469\n",
      "Val Loss: 10.709964, Acc: 1108.925\n",
      "1108.925\n",
      "Epoch: 1863 Train Loss: 0.834514, Acc: 444.167\n",
      "Val Loss: 10.127267, Acc: 1023.091\n",
      "1023.09130859375\n",
      "Epoch: 1864 Train Loss: 1.086743, Acc: 483.702\n",
      "Val Loss: 10.724436, Acc: 1074.538\n",
      "1074.538232421875\n",
      "Epoch: 1865 Train Loss: 0.850467, Acc: 459.115\n",
      "Val Loss: 12.268248, Acc: 1180.694\n",
      "1180.693505859375\n",
      "Epoch: 1866 Train Loss: 0.963929, Acc: 477.940\n",
      "Val Loss: 12.006189, Acc: 1225.000\n",
      "1225.00029296875\n",
      "Epoch: 1867 Train Loss: 1.503525, Acc: 581.000\n",
      "Val Loss: 14.469333, Acc: 1432.192\n",
      "1432.1921875\n",
      "Epoch: 1868 Train Loss: 1.368090, Acc: 537.412\n",
      "Val Loss: 10.898804, Acc: 1129.139\n",
      "1129.1390625\n",
      "Epoch: 1869 Train Loss: 1.111469, Acc: 502.864\n",
      "Val Loss: 12.119987, Acc: 1229.524\n",
      "1229.5244140625\n",
      "Epoch: 1870 Train Loss: 1.006219, Acc: 486.992\n",
      "Val Loss: 11.124187, Acc: 1140.323\n",
      "1140.323193359375\n",
      "Epoch: 1871 Train Loss: 1.239208, Acc: 523.681\n",
      "Val Loss: 10.827033, Acc: 1045.322\n",
      "1045.322412109375\n",
      "Epoch: 1872 Train Loss: 1.266084, Acc: 521.409\n",
      "Val Loss: 11.050238, Acc: 1173.895\n",
      "1173.89453125\n",
      "Epoch: 1873 Train Loss: 2.262350, Acc: 674.838\n",
      "Val Loss: 12.722345, Acc: 1264.620\n",
      "1264.619970703125\n",
      "Epoch: 1874 Train Loss: 1.141084, Acc: 497.641\n",
      "Val Loss: 11.551512, Acc: 1152.720\n",
      "1152.720068359375\n",
      "Epoch: 1875 Train Loss: 0.857330, Acc: 443.680\n",
      "Val Loss: 10.908349, Acc: 1056.351\n",
      "1056.3507080078125\n",
      "Epoch: 1876 Train Loss: 0.899768, Acc: 466.101\n",
      "Val Loss: 10.372714, Acc: 1044.969\n",
      "1044.9689697265626\n",
      "Epoch: 1877 Train Loss: 1.217086, Acc: 515.167\n",
      "Val Loss: 11.119524, Acc: 1126.119\n",
      "1126.11875\n",
      "Epoch: 1878 Train Loss: 0.961336, Acc: 453.132\n",
      "Val Loss: 10.939338, Acc: 1073.807\n",
      "1073.806689453125\n",
      "Epoch: 1879 Train Loss: 0.903785, Acc: 460.590\n",
      "Val Loss: 10.501201, Acc: 1109.059\n",
      "1109.0589111328125\n",
      "Epoch: 1880 Train Loss: 1.035885, Acc: 474.391\n",
      "Val Loss: 9.749955, Acc: 983.509\n",
      "983.509130859375\n",
      "Epoch: 1881 Train Loss: 0.947180, Acc: 460.415\n",
      "Val Loss: 11.508699, Acc: 1125.912\n",
      "1125.911865234375\n",
      "Epoch: 1882 Train Loss: 0.712430, Acc: 408.700\n",
      "Val Loss: 10.195446, Acc: 1016.748\n",
      "1016.747607421875\n",
      "Epoch: 1883 Train Loss: 0.625958, Acc: 392.201\n",
      "Val Loss: 9.834854, Acc: 990.152\n",
      "990.1515869140625\n",
      "Epoch: 1884 Train Loss: 0.690538, Acc: 395.895\n",
      "Val Loss: 9.414638, Acc: 947.720\n",
      "947.7196044921875\n",
      "947.7196044921875 947.7196044921875\n",
      "Epoch: 1885 Train Loss: 1.010418, Acc: 462.952\n",
      "Val Loss: 12.542663, Acc: 1226.201\n",
      "1226.200927734375\n",
      "Epoch: 1886 Train Loss: 1.574555, Acc: 584.228\n",
      "Val Loss: 10.106178, Acc: 993.157\n",
      "993.156884765625\n",
      "Epoch: 1887 Train Loss: 1.101534, Acc: 475.843\n",
      "Val Loss: 10.242217, Acc: 1077.114\n",
      "1077.114306640625\n",
      "Epoch: 1888 Train Loss: 1.196892, Acc: 511.295\n",
      "Val Loss: 12.677648, Acc: 1262.396\n",
      "1262.395654296875\n",
      "Epoch: 1889 Train Loss: 1.172006, Acc: 513.198\n",
      "Val Loss: 10.688490, Acc: 1055.269\n",
      "1055.268994140625\n",
      "Epoch: 1890 Train Loss: 0.996290, Acc: 468.847\n",
      "Val Loss: 10.128200, Acc: 1005.672\n",
      "1005.672314453125\n",
      "Epoch: 1891 Train Loss: 1.197124, Acc: 495.257\n",
      "Val Loss: 10.575739, Acc: 1041.691\n",
      "1041.69140625\n",
      "Epoch: 1892 Train Loss: 0.671778, Acc: 393.936\n",
      "Val Loss: 10.365660, Acc: 1028.072\n",
      "1028.071923828125\n",
      "Epoch: 1893 Train Loss: 0.691911, Acc: 400.572\n",
      "Val Loss: 10.628115, Acc: 1034.189\n",
      "1034.1894287109376\n",
      "Epoch: 1894 Train Loss: 0.605701, Acc: 384.571\n",
      "Val Loss: 9.791500, Acc: 960.737\n",
      "960.737060546875\n",
      "Epoch: 1895 Train Loss: 0.732440, Acc: 402.111\n",
      "Val Loss: 10.316587, Acc: 988.810\n",
      "988.80966796875\n",
      "Epoch: 1896 Train Loss: 0.654772, Acc: 392.627\n",
      "Val Loss: 10.719062, Acc: 1024.167\n",
      "1024.1670654296875\n",
      "Epoch: 1897 Train Loss: 0.619094, Acc: 382.170\n",
      "Val Loss: 11.812454, Acc: 1128.130\n",
      "1128.1296875\n",
      "Epoch: 1898 Train Loss: 0.933320, Acc: 443.878\n",
      "Val Loss: 11.129928, Acc: 1119.636\n",
      "1119.635888671875\n",
      "Epoch: 1899 Train Loss: 1.283809, Acc: 519.608\n",
      "Val Loss: 10.718885, Acc: 1040.231\n",
      "1040.2310546875\n",
      "Epoch: 1900 Train Loss: 1.775751, Acc: 601.337\n",
      "Val Loss: 9.782219, Acc: 1008.783\n",
      "1008.782763671875\n",
      "Epoch: 1901 Train Loss: 2.182887, Acc: 651.171\n",
      "Val Loss: 14.799880, Acc: 1295.093\n",
      "1295.092724609375\n",
      "Epoch: 1902 Train Loss: 2.169530, Acc: 666.018\n",
      "Val Loss: 9.801296, Acc: 1101.910\n",
      "1101.910498046875\n",
      "Epoch: 1903 Train Loss: 2.426742, Acc: 691.756\n",
      "Val Loss: 15.051150, Acc: 1410.012\n",
      "1410.0123046875\n",
      "Epoch: 1904 Train Loss: 1.812755, Acc: 611.338\n",
      "Val Loss: 10.620690, Acc: 1128.167\n",
      "1128.166552734375\n",
      "Epoch: 1905 Train Loss: 1.332396, Acc: 560.219\n",
      "Val Loss: 11.520176, Acc: 1212.621\n",
      "1212.620947265625\n",
      "Epoch: 1906 Train Loss: 0.889088, Acc: 491.803\n",
      "Val Loss: 9.945847, Acc: 1055.912\n",
      "1055.91220703125\n",
      "Epoch: 1907 Train Loss: 0.884488, Acc: 478.054\n",
      "Val Loss: 12.166531, Acc: 1237.996\n",
      "1237.99638671875\n",
      "Epoch: 1908 Train Loss: 1.056925, Acc: 501.546\n",
      "Val Loss: 9.529542, Acc: 989.322\n",
      "989.3224853515625\n",
      "Epoch: 1909 Train Loss: 1.424483, Acc: 560.678\n",
      "Val Loss: 10.540493, Acc: 1053.130\n",
      "1053.129931640625\n",
      "Epoch: 1910 Train Loss: 0.943387, Acc: 466.990\n",
      "Val Loss: 11.276263, Acc: 1122.080\n",
      "1122.079931640625\n",
      "Epoch: 1911 Train Loss: 0.959916, Acc: 479.500\n",
      "Val Loss: 10.396268, Acc: 1046.705\n",
      "1046.70517578125\n",
      "Epoch: 1912 Train Loss: 0.774606, Acc: 429.218\n",
      "Val Loss: 10.313918, Acc: 1058.558\n",
      "1058.55751953125\n",
      "Epoch: 1913 Train Loss: 0.790269, Acc: 429.903\n",
      "Val Loss: 11.169563, Acc: 1123.380\n",
      "1123.380126953125\n",
      "Epoch: 1914 Train Loss: 0.794038, Acc: 422.484\n",
      "Val Loss: 10.508267, Acc: 1032.196\n",
      "1032.196142578125\n",
      "Epoch: 1915 Train Loss: 0.769050, Acc: 419.115\n",
      "Val Loss: 12.032686, Acc: 1189.648\n",
      "1189.6482421875\n",
      "Epoch: 1916 Train Loss: 1.000970, Acc: 476.165\n",
      "Val Loss: 9.767128, Acc: 1004.500\n",
      "1004.499609375\n",
      "Epoch: 1917 Train Loss: 0.907607, Acc: 445.362\n",
      "Val Loss: 9.925401, Acc: 969.292\n",
      "969.291650390625\n",
      "Epoch: 1918 Train Loss: 0.627048, Acc: 382.564\n",
      "Val Loss: 11.063165, Acc: 1106.963\n",
      "1106.962548828125\n",
      "Epoch: 1919 Train Loss: 0.762151, Acc: 420.637\n",
      "Val Loss: 10.199100, Acc: 1015.955\n",
      "1015.95458984375\n",
      "Epoch: 1920 Train Loss: 0.780257, Acc: 416.224\n",
      "Val Loss: 12.148387, Acc: 1151.209\n",
      "1151.208740234375\n",
      "Epoch: 1921 Train Loss: 1.119807, Acc: 471.493\n",
      "Val Loss: 10.597942, Acc: 1052.253\n",
      "1052.253271484375\n",
      "Epoch: 1922 Train Loss: 1.013976, Acc: 466.516\n",
      "Val Loss: 11.636098, Acc: 1085.623\n",
      "1085.623193359375\n",
      "Epoch: 1923 Train Loss: 0.959109, Acc: 430.266\n",
      "Val Loss: 10.825817, Acc: 1117.004\n",
      "1117.00390625\n",
      "Epoch: 1924 Train Loss: 1.082189, Acc: 465.180\n",
      "Val Loss: 10.025395, Acc: 987.915\n",
      "987.9152099609375\n",
      "Epoch: 1925 Train Loss: 1.271267, Acc: 508.243\n",
      "Val Loss: 12.335416, Acc: 1203.718\n",
      "1203.7184326171875\n",
      "Epoch: 1926 Train Loss: 1.009440, Acc: 466.214\n",
      "Val Loss: 10.159961, Acc: 992.542\n",
      "992.541748046875\n",
      "Epoch: 1927 Train Loss: 0.855853, Acc: 438.144\n",
      "Val Loss: 11.946070, Acc: 1188.253\n",
      "1188.2531494140626\n",
      "Epoch: 1928 Train Loss: 0.648931, Acc: 383.801\n",
      "Val Loss: 10.246183, Acc: 987.819\n",
      "987.81923828125\n",
      "Epoch: 1929 Train Loss: 0.638833, Acc: 383.031\n",
      "Val Loss: 9.710431, Acc: 958.870\n",
      "958.870166015625\n",
      "Epoch: 1930 Train Loss: 0.568331, Acc: 367.118\n",
      "Val Loss: 9.518686, Acc: 934.466\n",
      "934.466259765625\n",
      "934.466259765625 934.466259765625\n",
      "Epoch: 1931 Train Loss: 0.721065, Acc: 401.928\n",
      "Val Loss: 10.404543, Acc: 1000.611\n",
      "1000.610595703125\n",
      "Epoch: 1932 Train Loss: 0.627446, Acc: 385.086\n",
      "Val Loss: 9.939003, Acc: 993.763\n",
      "993.762548828125\n",
      "Epoch: 1933 Train Loss: 0.686523, Acc: 394.239\n",
      "Val Loss: 10.890231, Acc: 1052.303\n",
      "1052.30263671875\n",
      "Epoch: 1934 Train Loss: 0.657663, Acc: 391.550\n",
      "Val Loss: 11.016299, Acc: 1058.573\n",
      "1058.5730224609374\n",
      "Epoch: 1935 Train Loss: 0.734855, Acc: 400.308\n",
      "Val Loss: 10.592381, Acc: 1025.066\n",
      "1025.0658203125\n",
      "Epoch: 1936 Train Loss: 1.035571, Acc: 460.474\n",
      "Val Loss: 10.206260, Acc: 990.685\n",
      "990.68486328125\n",
      "Epoch: 1937 Train Loss: 0.838847, Acc: 434.167\n",
      "Val Loss: 10.724261, Acc: 1080.060\n",
      "1080.0600830078124\n",
      "Epoch: 1938 Train Loss: 0.728231, Acc: 406.221\n",
      "Val Loss: 10.893570, Acc: 1045.498\n",
      "1045.497607421875\n",
      "Epoch: 1939 Train Loss: 0.806038, Acc: 417.459\n",
      "Val Loss: 9.757003, Acc: 977.327\n",
      "977.327490234375\n",
      "Epoch: 1940 Train Loss: 1.157388, Acc: 479.686\n",
      "Val Loss: 10.876029, Acc: 1109.711\n",
      "1109.710546875\n",
      "Epoch: 1941 Train Loss: 1.178248, Acc: 496.883\n",
      "Val Loss: 10.874629, Acc: 1092.643\n",
      "1092.6430908203124\n",
      "Epoch: 1942 Train Loss: 0.932883, Acc: 455.911\n",
      "Val Loss: 10.538864, Acc: 1084.342\n",
      "1084.3416748046875\n",
      "Epoch: 1943 Train Loss: 0.982619, Acc: 463.729\n",
      "Val Loss: 10.833382, Acc: 1077.263\n",
      "1077.2625244140625\n",
      "Epoch: 1944 Train Loss: 1.276002, Acc: 512.949\n",
      "Val Loss: 14.507371, Acc: 1400.428\n",
      "1400.428173828125\n",
      "Epoch: 1945 Train Loss: 1.984988, Acc: 619.694\n",
      "Val Loss: 11.452256, Acc: 1140.556\n",
      "1140.55634765625\n",
      "Epoch: 1946 Train Loss: 0.959431, Acc: 453.669\n",
      "Val Loss: 10.976446, Acc: 1097.645\n",
      "1097.645263671875\n",
      "Epoch: 1947 Train Loss: 0.922357, Acc: 446.557\n",
      "Val Loss: 10.189540, Acc: 1012.595\n",
      "1012.59501953125\n",
      "Epoch: 1948 Train Loss: 0.787756, Acc: 422.886\n",
      "Val Loss: 11.829038, Acc: 1143.475\n",
      "1143.47509765625\n",
      "Epoch: 1949 Train Loss: 1.366513, Acc: 559.993\n",
      "Val Loss: 9.705387, Acc: 1086.266\n",
      "1086.265673828125\n",
      "Epoch: 1950 Train Loss: 1.411099, Acc: 566.499\n",
      "Val Loss: 10.944639, Acc: 1061.306\n",
      "1061.3064208984374\n",
      "Epoch: 1951 Train Loss: 1.138422, Acc: 507.901\n",
      "Val Loss: 11.326705, Acc: 1153.160\n",
      "1153.1599609375\n",
      "Epoch: 1952 Train Loss: 0.918632, Acc: 456.028\n",
      "Val Loss: 10.128211, Acc: 1066.994\n",
      "1066.994140625\n",
      "Epoch: 1953 Train Loss: 0.805368, Acc: 432.774\n",
      "Val Loss: 10.416005, Acc: 1017.353\n",
      "1017.35283203125\n",
      "Epoch: 1954 Train Loss: 0.651382, Acc: 401.608\n",
      "Val Loss: 9.896023, Acc: 993.185\n",
      "993.184814453125\n",
      "Epoch: 1955 Train Loss: 0.795326, Acc: 442.612\n",
      "Val Loss: 9.771363, Acc: 972.537\n",
      "972.536767578125\n",
      "Epoch: 1956 Train Loss: 0.686589, Acc: 409.694\n",
      "Val Loss: 10.868149, Acc: 1065.620\n",
      "1065.619580078125\n",
      "Epoch: 1957 Train Loss: 0.794196, Acc: 423.991\n",
      "Val Loss: 10.437324, Acc: 1042.640\n",
      "1042.639892578125\n",
      "Epoch: 1958 Train Loss: 0.721765, Acc: 406.162\n",
      "Val Loss: 11.905884, Acc: 1179.361\n",
      "1179.3611328125\n",
      "Epoch: 1959 Train Loss: 0.834876, Acc: 442.194\n",
      "Val Loss: 9.789953, Acc: 973.869\n",
      "973.8694580078125\n",
      "Epoch: 1960 Train Loss: 1.099274, Acc: 475.247\n",
      "Val Loss: 10.172264, Acc: 1025.485\n",
      "1025.48515625\n",
      "Epoch: 1961 Train Loss: 0.785951, Acc: 424.780\n",
      "Val Loss: 10.474870, Acc: 1056.449\n",
      "1056.449462890625\n",
      "Epoch: 1962 Train Loss: 0.833480, Acc: 437.851\n",
      "Val Loss: 11.173766, Acc: 1109.139\n",
      "1109.13916015625\n",
      "Epoch: 1963 Train Loss: 0.754776, Acc: 407.835\n",
      "Val Loss: 10.529633, Acc: 1034.448\n",
      "1034.448193359375\n",
      "Epoch: 1964 Train Loss: 0.723018, Acc: 421.074\n",
      "Val Loss: 11.803232, Acc: 1169.188\n",
      "1169.1880859375\n",
      "Epoch: 1965 Train Loss: 0.696397, Acc: 406.091\n",
      "Val Loss: 10.235446, Acc: 1027.226\n",
      "1027.2255859375\n",
      "Epoch: 1966 Train Loss: 0.649148, Acc: 385.607\n",
      "Val Loss: 9.615500, Acc: 956.752\n",
      "956.75234375\n",
      "Epoch: 1967 Train Loss: 0.597365, Acc: 376.513\n",
      "Val Loss: 11.362322, Acc: 1091.740\n",
      "1091.740087890625\n",
      "Epoch: 1968 Train Loss: 0.936419, Acc: 452.530\n",
      "Val Loss: 11.866148, Acc: 1164.453\n",
      "1164.4529296875\n",
      "Epoch: 1969 Train Loss: 1.239911, Acc: 509.080\n",
      "Val Loss: 11.341708, Acc: 1083.124\n",
      "1083.12412109375\n",
      "Epoch: 1970 Train Loss: 1.084000, Acc: 461.599\n",
      "Val Loss: 9.635782, Acc: 999.622\n",
      "999.622265625\n",
      "Epoch: 1971 Train Loss: 0.889807, Acc: 438.087\n",
      "Val Loss: 11.552295, Acc: 1067.983\n",
      "1067.983251953125\n",
      "Epoch: 1972 Train Loss: 0.816288, Acc: 420.899\n",
      "Val Loss: 11.181774, Acc: 1107.292\n",
      "1107.29208984375\n",
      "Epoch: 1973 Train Loss: 0.953211, Acc: 456.790\n",
      "Val Loss: 12.452134, Acc: 1215.819\n",
      "1215.81884765625\n",
      "Epoch: 1974 Train Loss: 1.489762, Acc: 537.395\n",
      "Val Loss: 10.885311, Acc: 1108.279\n",
      "1108.279052734375\n",
      "Epoch: 1975 Train Loss: 1.338365, Acc: 512.697\n",
      "Val Loss: 12.093826, Acc: 1211.023\n",
      "1211.0226318359375\n",
      "Epoch: 1976 Train Loss: 0.938852, Acc: 433.143\n",
      "Val Loss: 9.780243, Acc: 1013.136\n",
      "1013.136376953125\n",
      "Epoch: 1977 Train Loss: 0.855894, Acc: 430.433\n",
      "Val Loss: 10.784630, Acc: 1036.250\n",
      "1036.249609375\n",
      "Epoch: 1978 Train Loss: 1.251756, Acc: 502.720\n",
      "Val Loss: 12.030873, Acc: 1256.642\n",
      "1256.642138671875\n",
      "Epoch: 1979 Train Loss: 1.072888, Acc: 499.791\n",
      "Val Loss: 10.173631, Acc: 1017.596\n",
      "1017.596142578125\n",
      "Epoch: 1980 Train Loss: 0.937303, Acc: 445.620\n",
      "Val Loss: 9.929158, Acc: 1007.822\n",
      "1007.82197265625\n",
      "Epoch: 1981 Train Loss: 0.616322, Acc: 375.574\n",
      "Val Loss: 10.501346, Acc: 1024.402\n",
      "1024.40205078125\n",
      "Epoch: 1982 Train Loss: 0.626222, Acc: 382.544\n",
      "Val Loss: 9.913306, Acc: 979.901\n",
      "979.9008544921875\n",
      "Epoch: 1983 Train Loss: 0.571946, Acc: 359.976\n",
      "Val Loss: 10.143622, Acc: 989.153\n",
      "989.152587890625\n",
      "Epoch: 1984 Train Loss: 0.512779, Acc: 338.406\n",
      "Val Loss: 10.117392, Acc: 985.285\n",
      "985.284521484375\n",
      "Epoch: 1985 Train Loss: 0.524379, Acc: 343.948\n",
      "Val Loss: 9.422170, Acc: 931.660\n",
      "931.6595703125\n",
      "931.6595703125 931.6595703125\n",
      "Epoch: 1986 Train Loss: 0.594525, Acc: 372.577\n",
      "Val Loss: 10.293942, Acc: 1008.902\n",
      "1008.9015625\n",
      "Epoch: 1987 Train Loss: 0.722484, Acc: 404.082\n",
      "Val Loss: 11.600597, Acc: 1099.132\n",
      "1099.13173828125\n",
      "Epoch: 1988 Train Loss: 0.740826, Acc: 404.287\n",
      "Val Loss: 9.593429, Acc: 936.351\n",
      "936.35126953125\n",
      "Epoch: 1989 Train Loss: 0.868926, Acc: 431.779\n",
      "Val Loss: 9.923166, Acc: 981.244\n",
      "981.2436279296875\n",
      "Epoch: 1990 Train Loss: 0.633896, Acc: 385.038\n",
      "Val Loss: 9.484217, Acc: 958.738\n",
      "958.73837890625\n",
      "Epoch: 1991 Train Loss: 0.701355, Acc: 389.481\n",
      "Val Loss: 9.487469, Acc: 952.311\n",
      "952.31123046875\n",
      "Epoch: 1992 Train Loss: 0.729713, Acc: 392.699\n",
      "Val Loss: 9.920580, Acc: 985.068\n",
      "985.0680419921875\n",
      "Epoch: 1993 Train Loss: 0.667861, Acc: 387.889\n",
      "Val Loss: 10.842116, Acc: 1058.366\n",
      "1058.3662841796875\n",
      "Epoch: 1994 Train Loss: 0.745109, Acc: 400.618\n",
      "Val Loss: 10.007105, Acc: 972.504\n",
      "972.504150390625\n",
      "Epoch: 1995 Train Loss: 0.808288, Acc: 418.217\n",
      "Val Loss: 10.263240, Acc: 1015.008\n",
      "1015.00771484375\n",
      "Epoch: 1996 Train Loss: 0.782798, Acc: 406.138\n",
      "Val Loss: 9.802928, Acc: 992.171\n",
      "992.171240234375\n",
      "Epoch: 1997 Train Loss: 0.746492, Acc: 403.135\n",
      "Val Loss: 10.161292, Acc: 986.434\n",
      "986.43369140625\n",
      "Epoch: 1998 Train Loss: 0.705140, Acc: 388.580\n",
      "Val Loss: 10.909783, Acc: 1057.828\n",
      "1057.82822265625\n",
      "Epoch: 1999 Train Loss: 0.735724, Acc: 402.338\n",
      "Val Loss: 9.757285, Acc: 959.941\n",
      "959.9410400390625\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('./output', exist_ok=True)\n",
    "best = 10000000\n",
    "lose = []\n",
    "for epoch in range(2000):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    for batch, (batch_x, batch_y) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():\n",
    "            batch_x, batch_y = Variable(batch_x.cuda()), Variable(batch_y.cuda())\n",
    "        out = model(batch_x)  #\n",
    "\n",
    "        loss = loss_func(out, batch_y)\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_corret = abs(out - batch_y).sum()\n",
    "        train_acc += train_corret.item()\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        with open('结果存放.txt','a') as file_handle:   # .txt可以不自己新建,代码会自动新建\n",
    "            file_handle.write(str(out))     # 写入\n",
    "            file_handle.write(str(batch_y))     # 写入\n",
    "            file_handle.write('\\n')\n",
    "    lose.append(train_loss / (math.ceil(len(train_data) / 7)))\n",
    "    print(f'Epoch: {epoch}', 'Train Loss: %.6f, Acc: %.3f' % (train_loss / (math.ceil(len(train_data) / 7)),\n",
    "                                                              train_acc / (len(train_data))))\n",
    "    \n",
    "    #     evaluation\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_acc = 0\n",
    "    for batch_x, batch_y in val_loader:\n",
    "        if torch.cuda.is_available():\n",
    "            batch_x, batch_y = Variable(batch_x.cuda()), Variable(batch_y.cuda())\n",
    "\n",
    "        out = model(batch_x)\n",
    "        loss = loss_func(out, batch_y)\n",
    "        eval_loss += loss.item()\n",
    "        num_correct = abs(out - batch_y).sum()\n",
    "        eval_acc += num_correct.item()\n",
    "    print('Val Loss: %.6f, Acc: %.3f' % (eval_loss / (math.ceil(len(val_data) / 7)),\n",
    "                                         eval_acc / (len(val_data))))\n",
    "    print(eval_acc / (len(val_data)))\n",
    "    if (eval_acc / (len(val_data))) < best:\n",
    "        best = eval_acc / (len(val_data))\n",
    "        print(best, eval_acc / (len(val_data)))\n",
    "        torch.save(model.state_dict(), 'output/15x25/UNetid1.pth')\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(18, 10))\n",
    "plt.plot(lose[3:],color=\"darkorange\",linestyle=\"solid\",linewidth=3,mec='darkorange',mfc='w',markersize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: 3.656451, Acc: 1514.785\n",
      "torch.Size([10, 1, 60, 100])\n",
      "torch.Size([10, 1, 60, 100])\n"
     ]
    }
   ],
   "source": [
    "model = un.UNet()\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "model.load_state_dict(torch.load('output/15x25/UNetid1.pth'))\n",
    "model.eval()\n",
    "plot_data = []\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "eval_loss = 0\n",
    "eval_acc = 0\n",
    "for batch_x, batch_y in test_loader:\n",
    "    if torch.cuda.is_available():\n",
    "        batch_x, batch_y = Variable(batch_x.cuda()), Variable(batch_y.cuda())\n",
    "            \n",
    "    out = model(batch_x)\n",
    "\n",
    "    plot_data = out\n",
    "    loss = loss_func(out, batch_y)\n",
    "    eval_loss += loss.item()\n",
    "    num_correct = abs(out - batch_y).sum()\n",
    "    eval_acc += num_correct.item()\n",
    "print('Val Loss: %.6f, Acc: %.3f' % (eval_loss / (math.ceil(len(val_data) / 7)),\n",
    "                                     eval_acc / (len(val_data))))\n",
    "plot_data1 = batch_y.cpu()\n",
    "plot_data = plot_data.cpu()\n",
    "print(plot_data.shape)\n",
    "print(plot_data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picture0\n",
      "True1: 203.2962188720703\n",
      "Estimate1: 197.27295112609863\n",
      "True2: 202.44772338867188\n",
      "Estimate2: 195.7489423751831\n",
      "MAE:  0.05521930886558645\n",
      "RMSE:  0.4765438858252476\n",
      "NE:  0.025288300998406864\n",
      "\n",
      "Picture1\n",
      "True1: 202.63963317871094\n",
      "Estimate1: 209.51700019836426\n",
      "True2: 201.79576110839844\n",
      "Estimate2: 199.16143798828125\n",
      "MAE:  0.04977283274491007\n",
      "RMSE:  0.44402624674283997\n",
      "NE:  0.0072178578380192315\n",
      "\n",
      "Picture2\n",
      "True1: 203.2021026611328\n",
      "Estimate1: 196.19178199768066\n",
      "True2: 202.29220581054688\n",
      "Estimate2: 193.77457237243652\n",
      "MAE:  0.05612039966989929\n",
      "RMSE:  0.5727441125856337\n",
      "NE:  0.03881072139443718\n",
      "\n",
      "Picture3\n",
      "True1: 202.73800659179688\n",
      "Estimate1: 221.34924221038818\n",
      "True2: 201.27931213378906\n",
      "Estimate2: 146.53077125549316\n",
      "MAE:  0.21765409611669986\n",
      "RMSE:  2.9871904659577013\n",
      "NE:  0.08790164331418518\n",
      "\n",
      "Picture4\n",
      "True1: 204.0208740234375\n",
      "Estimate1: 212.6014289855957\n",
      "True2: 202.6131591796875\n",
      "Estimate2: 186.6091890335083\n",
      "MAE:  0.08934175598617486\n",
      "RMSE:  0.9593692903430457\n",
      "NE:  0.010810771990470522\n",
      "\n",
      "Picture5\n",
      "True1: 203.4015655517578\n",
      "Estimate1: 254.4170207977295\n",
      "True2: 202.7084503173828\n",
      "Estimate2: 204.68620586395264\n",
      "MAE:  0.16591634584940038\n",
      "RMSE:  2.6379048192291434\n",
      "NE:  0.1288014169495867\n",
      "\n",
      "Picture6\n",
      "True1: 203.00228881835938\n",
      "Estimate1: 261.79149532318115\n",
      "True2: 202.30770874023438\n",
      "Estimate2: 195.04077529907227\n",
      "MAE:  0.19390311809047125\n",
      "RMSE:  3.0604049191097857\n",
      "NE:  0.12012085088521979\n",
      "\n",
      "Picture7\n",
      "True1: 203.7343292236328\n",
      "Estimate1: 205.2871789932251\n",
      "True2: 201.2583770751953\n",
      "Estimate2: 172.79367446899414\n",
      "MAE:  0.09477570645829353\n",
      "RMSE:  1.4738410080693352\n",
      "NE:  0.06978683445706303\n",
      "\n",
      "Picture8\n",
      "True1: 203.74798583984375\n",
      "Estimate1: 227.03545093536377\n",
      "True2: 203.36819458007812\n",
      "Estimate2: 198.66014003753662\n",
      "MAE:  0.09481252116211301\n",
      "RMSE:  1.2283534405180303\n",
      "NE:  0.049429290886606976\n",
      "\n",
      "Picture9\n",
      "True1: 203.9412078857422\n",
      "Estimate1: 214.48759269714355\n",
      "True2: 202.22828674316406\n",
      "Estimate2: 202.92627716064453\n",
      "MAE:  0.0486440092571526\n",
      "RMSE:  0.5489534896631557\n",
      "NE:  0.030262818035372792\n",
      "\n",
      "该程序运行时间：0小时10分钟54秒\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "plot_data = plot_data.detach().numpy()\n",
    "plot_data1 = plot_data1.detach().numpy()\n",
    "\n",
    "# print(plot_data.shape)\n",
    "\n",
    "plot_data = plot_data.reshape(len(plot_data),15*4,25*4)\n",
    "plot_data1 = plot_data1.reshape(len(plot_data),15*4,25*4)\n",
    "\n",
    "# print(plot_data.shape)\n",
    "\n",
    "Plot_data = np.ones([len(plot_data),15,25])\n",
    "Plot_data1 = np.ones([len(plot_data),15,25])\n",
    "\n",
    "ken = np.ones([4, 4])/(4*4)\n",
    "\n",
    "for i in range(len(plot_data)):\n",
    "    Plot_data[i] = Conv2(plot_data[i], ken, 4, 4, 0)\n",
    "    Plot_data1[i] = Conv2(plot_data1[i], ken, 4, 4, 0)\n",
    "\n",
    "c = abs(Plot_data1-Plot_data)\n",
    "# print(Plot_data.shape)\n",
    "\n",
    "for j in range(len(Plot_data1)):\n",
    "\n",
    "    print('Picture' + str(j))\n",
    "    print('True1: '+ str(np.max(Plot_data1[j])))\n",
    "    print('Estimate1: '+ str(np.max(Plot_data[j])))\n",
    "    print('True2: '+ str(np.sort(Plot_data1[j].reshape(1,-1))[0][-2]))\n",
    "    print('Estimate2: ' + str(np.sort(Plot_data[j].reshape(1,-1))[0][-2]))\n",
    "    print('MAE:  '+str(mean_absolute_error(Plot_data1[j],Plot_data[j])))#Y_real为实际值，Y_pre为预测值\n",
    "#     print('MSE:  '+str(mean_squared_error(plot_data1[j],plot_data[j])))\n",
    "    print('RMSE:  '+str(np.sqrt(mean_squared_error(Plot_data1[j],Plot_data[j]))))\n",
    "    a = np.cumsum(Plot_data[j])\n",
    "    b = np.cumsum(Plot_data1[j])\n",
    "\n",
    "    print('NE:  '+str(abs(a[-1]-b[-1])/b[-1]))\n",
    "#     print('ME:  '+str(abs(a[-1]-b[-1])/(13*8)))\n",
    "#     print('R2:  '+str(r2_score(plot_data1[j],plot_data[j])))\n",
    "    print()\n",
    "end_time = time.time()\n",
    "\n",
    "run_time = round(end_time-begin_time)\n",
    "# 计算时分秒\n",
    "hour = run_time//3600\n",
    "minute = (run_time-3600*hour)//60\n",
    "second = run_time-3600*hour-60*minute\n",
    "# 输出\n",
    "print (f'该程序运行时间：{hour}小时{minute}分钟{second}秒')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x1800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.27295112609863\n",
      "195.7489423751831\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14, 18))\n",
    "# plot_data = plot_data*(y_train_max - y_train_min)+y_train_min\n",
    "# plot_data = plot_data.cpu()\n",
    "# plot_data = plot_data.detach().numpy()\n",
    "\n",
    "Plot_data = Plot_data.reshape(len(Plot_data),1,15,25)\n",
    "# plot_data = plot_data.reshape(10,1,18,14)\n",
    "# plot_data = plot_data.reshape(len(plot_data),1,8,13)\n",
    "\n",
    "# print(Plot_data.shape)\n",
    "# print(len(plot_data[1]))\n",
    "for j in range(len(Plot_data[1])):\n",
    "    plt.imshow(Plot_data[0][j], cmap='Purples')\n",
    "#     plt.axis('off')\n",
    "plt.colorbar(fraction=0.03, pad=0.05)\n",
    "plt.show()\n",
    "# plot_data = torch.from_numpy(plot_data)\n",
    "\n",
    "print(np.max(Plot_data[0]))\n",
    "# print(plot_data[3].shape)\n",
    "# plot_data[3].reshape(1,-1)\n",
    "# print(plot_data[3].reshape(1,-1).shape)\n",
    "# print(plot_data[3].reshape(1,-1))\n",
    "print(np.sort(Plot_data[0].reshape(1,-1))[0][-2])\n",
    "# print(plot_data[4][0][5][3])\n",
    "# print(torch.max(plot_data[0],0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x1800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203.2962188720703\n",
      "202.44772338867188\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14, 18))\n",
    "# plot_data1 = plot_data1*(y_train_max - y_train_min)+y_train_min\n",
    "\n",
    "# plot_data1 = plot_data1.detach().numpy()\n",
    "\n",
    "Plot_data1 = Plot_data1.reshape(len(Plot_data),1,15,25)\n",
    "# plot_data1 = plot_data1.reshape(10,1,18,14)\n",
    "# plot_data1 = plot_data1.reshape(len(plot_data1),1,8,13)\n",
    "\n",
    "for j in range(len(Plot_data1[1])):\n",
    "    plt.imshow(Plot_data1[0][j], cmap='Purples')\n",
    "#     plt.axis('off')\n",
    "plt.colorbar(fraction=0.03, pad=0.05)\n",
    "plt.show()\n",
    "# plot_data1 = torch.from_numpy(plot_data1)\n",
    "\n",
    "print(np.max(Plot_data1[0]))\n",
    "# print(np.sort(plot_data1[1])[-2])\n",
    "print(np.sort(Plot_data1[0].reshape(1,-1))[0][-2])\n",
    "\n",
    "# print(torch.max()[1])\n",
    "# print(len(plot_data1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1400x1800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.547276496887207\n"
     ]
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(14, 18))\n",
    "\n",
    "# c = c.detach().numpy()\n",
    "\n",
    "c = c.reshape(len(plot_data),1,15,25)\n",
    "# c = c.reshape(10,1,18,14)\n",
    "# c = c.reshape(len(plot_data),1,8,13)\n",
    "\n",
    "# print(c.shape)\n",
    "# print(len(plot_data[1]))\n",
    "for j in range(len(c[0])):\n",
    "    plt.imshow(c[0][j], cmap='OrRd', vmin = 0, vmax = 20)\n",
    "#     plt.axis('off')\n",
    "plt.colorbar(fraction=0.03, pad=0.05)\n",
    "plt.show()\n",
    "# c = torch.from_numpy(c)\n",
    "\n",
    "print(np.max(c[0]))\n",
    "# print(np.sort(c[3])[-2])\n",
    "# print(plot_data[4][0][5][3])\n",
    "# print(torch.max(plot_data[0],0)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def Conv2(img, kernel, n, stride,p):\n",
    "#     #img：输入图片；kernel：卷积核值；n：卷积核大小为n*n；stride:步长。\n",
    "#     #return：feature map\n",
    "#     h, w = img.shape\n",
    "#     res_h = ((h+2*p-n)//stride)+1 #卷积边长计算公式：((n+2*p-k)/stride)+1\n",
    "#     res_w = ((w+2*p -n)//stride)+1\n",
    "#     res = np.zeros([res_h, res_w])\n",
    "#     for i in range(res_h):\n",
    "#         for j in range(res_w):\n",
    "#             temp = img[i*stride:i*stride+n , j*stride:j*stride+n]\n",
    "#             temp = np.multiply(kernel, temp)\n",
    "#             res[i][j] = temp.sum()\n",
    "#     return res\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     A = np.ones([10, 15*4, 25*4])\n",
    "#     B = np.ones([10, 15, 25])\n",
    "#     ken = np.ones([4, 4])/(4*4)\n",
    "#     for i in range(len(A)):\n",
    "#         B[i] = Conv2(A[i], ken, 4, 4, 0)\n",
    "# #     print(\"result：\",Conv2(A, ken, 32, 32, 0))\n",
    "# #     print(B)\n",
    "#     print(B.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mid = np.ones([1, 1, 4, 4])\n",
    "# # mid.dtype = 'float32'\n",
    "# print(mid.shape)\n",
    "# A = np.ones([100, 1, 15, 25])\n",
    "# A = np.kron(A, mid)\n",
    "# print(A.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch171",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
